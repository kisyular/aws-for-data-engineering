{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2a6e24",
   "metadata": {},
   "source": [
    "# AWS S3 Operations - Comprehensive Guide\n",
    "\n",
    "This notebook provides a complete walkthrough of AWS S3 operations using boto3 SDK.\n",
    "\n",
    "## Prerequisites - Manual AWS Setup\n",
    "\n",
    "Before running this notebook, the following setup was completed manually in AWS Console:\n",
    "\n",
    "### Step 1: S3 Bucket Creation\n",
    "- Logged into AWS account\n",
    "- Created S3 bucket named **\"real-learn-s3\"** in **us-east-1** region\n",
    "- Configuration:\n",
    "  - Public access: **BLOCKED** (security best practice)\n",
    "  - Versioning: **ENABLED** (maintains file history)\n",
    "- Uploaded initial test file: **bookings.csv**\n",
    "\n",
    "### Step 2: IAM User Configuration\n",
    "- Created IAM user with **programmatic access** (no console access needed)\n",
    "- Attached policy: **AmazonS3FullAccess** (full S3 permissions)\n",
    "- Generated access credentials:\n",
    "  - Access Key ID\n",
    "  - Secret Access Key\n",
    "- Saved credentials securely in `.env` file (never commit to git)\n",
    "\n",
    "### Step 3: Environment File\n",
    "Created `.env` file with the following variables:\n",
    "```\n",
    "AWS_BUCKET_NAME=real-learn-s3\n",
    "AWS_ACCESS_KEY=your-access-key-id\n",
    "AWS_SECRET_KEY=your-secret-access-key\n",
    "AWS_REGION=us-east-1\n",
    "```\n",
    "\n",
    "### Step 4: Python Environment\n",
    "- Installed boto3 library for AWS SDK\n",
    "- Installed python-dotenv for secure credential loading\n",
    "- Created initial Python script to verify S3 connection and list objects\n",
    "\n",
    "Now we're ready to explore S3 operations programmatically.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **S3 Fundamentals** - Understanding buckets and objects\n",
    "2. **Bucket Operations** - List, create, and manage buckets\n",
    "3. **Object Operations** - Upload, download, list, and delete files\n",
    "4. **Advanced Features** - Metadata, presigned URLs, progress tracking\n",
    "5. **Error Handling** - Robust exception management\n",
    "6. **Best Practices** - Security and performance optimization\n",
    "\n",
    "## S3 Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  AWS S3 STRUCTURE                                       │\n",
    "│                                                         │\n",
    "│  Your AWS Account                                       │\n",
    "│  └─► Buckets (Globally unique names)                    │\n",
    "│      ├─► Bucket 1: my-app-data-2024                     │\n",
    "│      │   ├─► Object: file1.txt                          │\n",
    "│      │   ├─► Object: images/photo.jpg                   │\n",
    "│      │   └─► Object: data/2024/records.csv              │\n",
    "│      │                                                  │\n",
    "│      └─► Bucket 2: backups-prod                         │\n",
    "│          ├─► Object: backup-2024-01.tar.gz              │\n",
    "│          └─► Object: logs/app.log                       │\n",
    "│                                                         │\n",
    "│  Key Concepts:                                          │\n",
    "│  • Bucket = Container (like a root folder)              │\n",
    "│  • Object = File with metadata                          │\n",
    "│  • Key = Full path/name (e.g., \"folder/file.txt\")       │\n",
    "│  • Region = Geographic location                         │\n",
    "│  • Storage Class = Cost/access tier                     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced8b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db96ea8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Loading AWS credentials securely from environment variables using `.env` file.\n",
    "\n",
    "**Security Best Practice:** Never hardcode credentials in your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4885676",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = os.getenv(\"AWS_BUCKET_NAME\")    \n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deefcef",
   "metadata": {},
   "source": [
    "## 2. AWS Credentials Configuration\n",
    "\n",
    "Required environment variables in your `.env` file:\n",
    "```\n",
    "AWS_BUCKET_NAME=your-bucket-name\n",
    "AWS_ACCESS_KEY=your-access-key-id\n",
    "AWS_SECRET_KEY=your-secret-access-key\n",
    "AWS_REGION=us-east-1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449f59b",
   "metadata": {},
   "source": [
    "## 3. Initialize S3 Client\n",
    "\n",
    "boto3 provides two interfaces:\n",
    "- **Client**: Low-level service access (more control)\n",
    "- **Resource**: Higher-level object-oriented interface (easier to use)\n",
    "\n",
    "We'll use the **client** interface for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4b9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an s3 client\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION, \n",
    "                         aws_access_key_id=ACCESS_KEY, \n",
    "                         aws_secret_access_key=SECRET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024553e",
   "metadata": {},
   "source": [
    "## 4. Bucket Operations\n",
    "\n",
    "### Operation Flow\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│  BUCKET LIFECYCLE                       │\n",
    "│                                         │\n",
    "│  1. CREATE → Bucket exists in S3        │\n",
    "│  2. LIST → View all your buckets        │\n",
    "│  3. CONFIGURE → Set permissions/policies│\n",
    "│  4. USE → Upload/download objects       │\n",
    "│  5. DELETE → Remove bucket (if empty)   │\n",
    "└─────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e393ac2",
   "metadata": {},
   "source": [
    "## 5. Create Bucket\n",
    "\n",
    "Create a new S3 bucket in your account.\n",
    "\n",
    "**Important Rules:**\n",
    "- Bucket names must be globally unique across ALL AWS accounts\n",
    "- Only lowercase letters, numbers, hyphens, and dots allowed\n",
    "- Must be 3-63 characters long\n",
    "- Cannot start or end with a hyphen\n",
    "- Region matters for latency and compliance\n",
    "\n",
    "**Regions:**\n",
    "- `us-east-1` is the default and doesn't require LocationConstraint\n",
    "- Other regions need explicit LocationConstraint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d35e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Bucket 'real-learn-s3-test' created successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in a specified region\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): Name for the bucket (must be globally unique)\n",
    "        region (str): AWS region (if None, uses default from client)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if bucket created, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region is None or region == 'us-east-1':\n",
    "            # us-east-1 doesn't require LocationConstraint\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            # Other regions require LocationConstraint\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"SUCCESS: Bucket '{bucket_name}' created successfully\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'BucketAlreadyExists':\n",
    "            print(f\"ERROR: Bucket '{bucket_name}' already exists (owned by someone else)\")\n",
    "        elif error_code == 'BucketAlreadyOwnedByYou':\n",
    "            print(f\"INFO: Bucket '{bucket_name}' already exists and is owned by you\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create bucket - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "create_bucket(f\"{BUCKET_NAME}-test\", region=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c12e2",
   "metadata": {},
   "source": [
    "## 6. Upload File to S3\n",
    "\n",
    "Upload files from your local filesystem to an S3 bucket.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `file_name`: Path to local file\n",
    "- `bucket`: Target S3 bucket name\n",
    "- `object_name`: Key (path/name) in S3 (if None, uses file_name)\n",
    "\n",
    "**Use Cases:**\n",
    "- Backup files to cloud storage\n",
    "- Store application data\n",
    "- Host static website content\n",
    "- Archive logs and reports\n",
    "\n",
    "**File Types:** Any file type supported (images, videos, documents, code, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21b57c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/hosts.csv' uploaded to 'real-learn-s3/data/hosts.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    # If S3 object_name not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_file('data/hosts.csv', BUCKET_NAME, 'data/hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e03631",
   "metadata": {},
   "source": [
    "## 7. Download File from S3\n",
    "\n",
    "Download objects from S3 to your local filesystem.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `bucket`: Source S3 bucket name\n",
    "- `object_name`: Key (path/name) in S3\n",
    "- `file_name`: Destination path on local filesystem\n",
    "\n",
    "**Important Notes:**\n",
    "- Creates directories automatically if they don't exist\n",
    "- Overwrites existing local files without warning\n",
    "- Preserves file content but not S3 metadata\n",
    "\n",
    "**Common Pattern:**\n",
    "```python\n",
    "# Download backup\n",
    "download_file('my-bucket', 'backups/db-2024.sql', './local-backups/db.sql')\n",
    "\n",
    "# Download with same name\n",
    "download_file('my-bucket', 'report.pdf', 'report.pdf')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3458c734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'real-learn-s3/raw/bookings.csv' downloaded to './downloads/bookings.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_file(bucket, object_name, file_name):\n",
    "    \"\"\"\n",
    "    Download a file from an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to download\n",
    "        file_name (str): Local file path to save\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if download successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "        \n",
    "        s3_client.download_file(bucket, object_name, file_name)\n",
    "        print(f\"SUCCESS: '{bucket}/{object_name}' downloaded to '{file_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to download file - {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "download_file(BUCKET_NAME, 'raw/bookings.csv', './downloads/bookings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5d749",
   "metadata": {},
   "source": [
    "## 8. Upload String Content to S3\n",
    "\n",
    "Upload string/text data directly to S3 without creating a local file first.\n",
    "\n",
    "**Use Cases:**\n",
    "- Store JSON data from API responses\n",
    "- Save generated text, logs, or reports\n",
    "- Write configuration files\n",
    "- Store processed data without local disk I/O\n",
    "\n",
    "**Advantages:**\n",
    "- No temporary files needed\n",
    "- Faster for small text content\n",
    "- Memory-efficient for string data\n",
    "- Direct encoding control (UTF-8 by default)\n",
    "\n",
    "**Example Scenarios:**\n",
    "```python\n",
    "# Save JSON data\n",
    "upload_string('{\"status\": \"ok\"}', 'my-bucket', 'config.json')\n",
    "\n",
    "# Save log entry\n",
    "upload_string('2024-01-28: System started', 'logs-bucket', 'app.log')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d992ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: String content uploaded to 'real-learn-s3/test/hello.txt'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_string(content, bucket, object_name):\n",
    "    \"\"\"\n",
    "    Upload string content directly to S3\n",
    "    \n",
    "    Args:\n",
    "        content (str): String content to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=object_name,\n",
    "            Body=content.encode('utf-8')\n",
    "        )\n",
    "        print(f\"SUCCESS: String content uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload string - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_string('Hello from S3!', BUCKET_NAME, 'test/hello.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac7dd7",
   "metadata": {},
   "source": [
    "## 9. Read Object Content from S3\n",
    "\n",
    "Read S3 object content directly into memory without downloading to a file.\n",
    "\n",
    "**Use Cases:**\n",
    "- Read configuration files\n",
    "- Load small datasets for processing\n",
    "- Retrieve API responses or JSON data\n",
    "- Stream log files for analysis\n",
    "\n",
    "**Important Notes:**\n",
    "- Best for small to medium files (avoid for GB-sized files)\n",
    "- Returns content as bytes (decode to string if needed)\n",
    "- Entire file loaded into memory\n",
    "- More efficient than download + read for small files\n",
    "\n",
    "**Performance Tips:**\n",
    "- For large files: Use download_file() or streaming\n",
    "- For text files: Decode with proper encoding (UTF-8 default)\n",
    "- For binary files: Use raw bytes without decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c473d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Read 14 characters from 'real-learn-s3/test/hello.txt'\n",
      "Content: Hello from S3!\n"
     ]
    }
   ],
   "source": [
    "def read_object(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Read S3 object content directly into memory\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to read\n",
    "    \n",
    "    Returns:\n",
    "        str: File content as string, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=object_name)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        print(f\"SUCCESS: Read {len(content)} characters from '{bucket}/{object_name}'\")\n",
    "        return content\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'NoSuchKey':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to read object - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "content = read_object(BUCKET_NAME, 'test/hello.txt')\n",
    "if content:\n",
    "    print(f\"Content: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b4215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
