{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2a6e24",
   "metadata": {},
   "source": [
    "# AWS S3 Operations - Comprehensive Guide\n",
    "\n",
    "This notebook provides a complete walkthrough of AWS S3 operations using boto3 SDK.\n",
    "\n",
    "## Prerequisites - Manual AWS Setup\n",
    "\n",
    "Before running this notebook, the following setup was completed manually in AWS Console:\n",
    "\n",
    "### Step 1: S3 Bucket Creation\n",
    "- Logged into AWS account\n",
    "- Created S3 bucket named **\"real-learn-s3\"** in **us-east-1** region\n",
    "- Configuration:\n",
    "  - Public access: **BLOCKED** (security best practice)\n",
    "  - Versioning: **ENABLED** (maintains file history)\n",
    "- Uploaded initial test file: **bookings.csv**\n",
    "\n",
    "### Step 2: IAM User Configuration\n",
    "- Created IAM user with **programmatic access** (no console access needed)\n",
    "- Attached policy: **AmazonS3FullAccess** (full S3 permissions)\n",
    "- Generated access credentials:\n",
    "  - Access Key ID\n",
    "  - Secret Access Key\n",
    "- Saved credentials securely in `.env` file (never commit to git)\n",
    "\n",
    "### Step 3: Environment File\n",
    "Created `.env` file with the following variables:\n",
    "```\n",
    "AWS_BUCKET_NAME=real-learn-s3\n",
    "AWS_ACCESS_KEY=your-access-key-id\n",
    "AWS_SECRET_KEY=your-secret-access-key\n",
    "AWS_REGION=us-east-1\n",
    "```\n",
    "\n",
    "### Step 4: Python Environment\n",
    "- Installed boto3 library for AWS SDK\n",
    "- Installed python-dotenv for secure credential loading\n",
    "- Created initial Python script to verify S3 connection and list objects\n",
    "\n",
    "Now we're ready to explore S3 operations programmatically.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **S3 Fundamentals** - Understanding buckets and objects\n",
    "2. **Bucket Operations** - List, create, and manage buckets\n",
    "3. **Object Operations** - Upload, download, list, and delete files\n",
    "4. **Advanced Features** - Metadata, presigned URLs, progress tracking\n",
    "5. **Error Handling** - Robust exception management\n",
    "6. **Best Practices** - Security and performance optimization\n",
    "\n",
    "## S3 Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  AWS S3 STRUCTURE                                       │\n",
    "│                                                         │\n",
    "│  Your AWS Account                                       │\n",
    "│  └─► Buckets (Globally unique names)                    │\n",
    "│      ├─► Bucket 1: my-app-data-2024                     │\n",
    "│      │   ├─► Object: file1.txt                          │\n",
    "│      │   ├─► Object: images/photo.jpg                   │\n",
    "│      │   └─► Object: data/2024/records.csv              │\n",
    "│      │                                                  │\n",
    "│      └─► Bucket 2: backups-prod                         │\n",
    "│          ├─► Object: backup-2024-01.tar.gz              │\n",
    "│          └─► Object: logs/app.log                       │\n",
    "│                                                         │\n",
    "│  Key Concepts:                                          │\n",
    "│  • Bucket = Container (like a root folder)              │\n",
    "│  • Object = File with metadata                          │\n",
    "│  • Key = Full path/name (e.g., \"folder/file.txt\")       │\n",
    "│  • Region = Geographic location                         │\n",
    "│  • Storage Class = Cost/access tier                     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced8b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db96ea8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Loading AWS credentials securely from environment variables using `.env` file.\n",
    "\n",
    "**Security Best Practice:** Never hardcode credentials in your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4885676",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = os.getenv(\"AWS_BUCKET_NAME\")    \n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deefcef",
   "metadata": {},
   "source": [
    "## 2. AWS Credentials Configuration\n",
    "\n",
    "Required environment variables in your `.env` file:\n",
    "```\n",
    "AWS_BUCKET_NAME=your-bucket-name\n",
    "AWS_ACCESS_KEY=your-access-key-id\n",
    "AWS_SECRET_KEY=your-secret-access-key\n",
    "AWS_REGION=us-east-1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449f59b",
   "metadata": {},
   "source": [
    "## 3. Initialize S3 Client\n",
    "\n",
    "boto3 provides two interfaces:\n",
    "- **Client**: Low-level service access (more control)\n",
    "- **Resource**: Higher-level object-oriented interface (easier to use)\n",
    "\n",
    "We'll use the **client** interface for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4b9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an s3 client\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION, \n",
    "                         aws_access_key_id=ACCESS_KEY, \n",
    "                         aws_secret_access_key=SECRET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024553e",
   "metadata": {},
   "source": [
    "## 4. Bucket Operations\n",
    "\n",
    "### Operation Flow\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│  BUCKET LIFECYCLE                       │\n",
    "│                                         │\n",
    "│  1. CREATE → Bucket exists in S3        │\n",
    "│  2. LIST → View all your buckets        │\n",
    "│  3. CONFIGURE → Set permissions/policies│\n",
    "│  4. USE → Upload/download objects       │\n",
    "│  5. DELETE → Remove bucket (if empty)   │\n",
    "└─────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e393ac2",
   "metadata": {},
   "source": [
    "## 5. Create Bucket\n",
    "\n",
    "Create a new S3 bucket in your account.\n",
    "\n",
    "**Important Rules:**\n",
    "- Bucket names must be globally unique across ALL AWS accounts\n",
    "- Only lowercase letters, numbers, hyphens, and dots allowed\n",
    "- Must be 3-63 characters long\n",
    "- Cannot start or end with a hyphen\n",
    "- Region matters for latency and compliance\n",
    "\n",
    "**Regions:**\n",
    "- `us-east-1` is the default and doesn't require LocationConstraint\n",
    "- Other regions need explicit LocationConstraint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d35e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Bucket 'real-learn-s3-test' created successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in a specified region\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): Name for the bucket (must be globally unique)\n",
    "        region (str): AWS region (if None, uses default from client)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if bucket created, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region is None or region == 'us-east-1':\n",
    "            # us-east-1 doesn't require LocationConstraint\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            # Other regions require LocationConstraint\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"SUCCESS: Bucket '{bucket_name}' created successfully\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'BucketAlreadyExists':\n",
    "            print(f\"ERROR: Bucket '{bucket_name}' already exists (owned by someone else)\")\n",
    "        elif error_code == 'BucketAlreadyOwnedByYou':\n",
    "            print(f\"INFO: Bucket '{bucket_name}' already exists and is owned by you\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create bucket - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "create_bucket(f\"{BUCKET_NAME}-test\", region=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c12e2",
   "metadata": {},
   "source": [
    "## 6. Upload File to S3\n",
    "\n",
    "Upload files from your local filesystem to an S3 bucket.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `file_name`: Path to local file\n",
    "- `bucket`: Target S3 bucket name\n",
    "- `object_name`: Key (path/name) in S3 (if None, uses file_name)\n",
    "\n",
    "**Use Cases:**\n",
    "- Backup files to cloud storage\n",
    "- Store application data\n",
    "- Host static website content\n",
    "- Archive logs and reports\n",
    "\n",
    "**File Types:** Any file type supported (images, videos, documents, code, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21b57c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/hosts.csv' uploaded to 'real-learn-s3/data/hosts.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    # If S3 object_name not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_file('data/hosts.csv', BUCKET_NAME, 'data/hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e03631",
   "metadata": {},
   "source": [
    "## 7. Download File from S3\n",
    "\n",
    "Download objects from S3 to your local filesystem.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `bucket`: Source S3 bucket name\n",
    "- `object_name`: Key (path/name) in S3\n",
    "- `file_name`: Destination path on local filesystem\n",
    "\n",
    "**Important Notes:**\n",
    "- Creates directories automatically if they don't exist\n",
    "- Overwrites existing local files without warning\n",
    "- Preserves file content but not S3 metadata\n",
    "\n",
    "**Common Pattern:**\n",
    "```python\n",
    "# Download backup\n",
    "download_file('my-bucket', 'backups/db-2024.sql', './local-backups/db.sql')\n",
    "\n",
    "# Download with same name\n",
    "download_file('my-bucket', 'report.pdf', 'report.pdf')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3458c734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'real-learn-s3/raw/bookings.csv' downloaded to './downloads/bookings.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_file(bucket, object_name, file_name):\n",
    "    \"\"\"\n",
    "    Download a file from an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to download\n",
    "        file_name (str): Local file path to save\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if download successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "        \n",
    "        s3_client.download_file(bucket, object_name, file_name)\n",
    "        print(f\"SUCCESS: '{bucket}/{object_name}' downloaded to '{file_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to download file - {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "download_file(BUCKET_NAME, 'raw/bookings.csv', './downloads/bookings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5d749",
   "metadata": {},
   "source": [
    "## 8. Upload String Content to S3\n",
    "\n",
    "Upload string/text data directly to S3 without creating a local file first.\n",
    "\n",
    "**Use Cases:**\n",
    "- Store JSON data from API responses\n",
    "- Save generated text, logs, or reports\n",
    "- Write configuration files\n",
    "- Store processed data without local disk I/O\n",
    "\n",
    "**Advantages:**\n",
    "- No temporary files needed\n",
    "- Faster for small text content\n",
    "- Memory-efficient for string data\n",
    "- Direct encoding control (UTF-8 by default)\n",
    "\n",
    "**Example Scenarios:**\n",
    "```python\n",
    "# Save JSON data\n",
    "upload_string('{\"status\": \"ok\"}', 'my-bucket', 'config.json')\n",
    "\n",
    "# Save log entry\n",
    "upload_string('2024-01-28: System started', 'logs-bucket', 'app.log')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d992ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: String content uploaded to 'real-learn-s3/test/hello.txt'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_string(content, bucket, object_name):\n",
    "    \"\"\"\n",
    "    Upload string content directly to S3\n",
    "    \n",
    "    Args:\n",
    "        content (str): String content to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=object_name,\n",
    "            Body=content.encode('utf-8')\n",
    "        )\n",
    "        print(f\"SUCCESS: String content uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload string - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_string('Hello from S3!', BUCKET_NAME, 'test/hello.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac7dd7",
   "metadata": {},
   "source": [
    "## 9. Read Object Content from S3\n",
    "\n",
    "Read S3 object content directly into memory without downloading to a file.\n",
    "\n",
    "**Use Cases:**\n",
    "- Read configuration files\n",
    "- Load small datasets for processing\n",
    "- Retrieve API responses or JSON data\n",
    "- Stream log files for analysis\n",
    "\n",
    "**Important Notes:**\n",
    "- Best for small to medium files (avoid for GB-sized files)\n",
    "- Returns content as bytes (decode to string if needed)\n",
    "- Entire file loaded into memory\n",
    "- More efficient than download + read for small files\n",
    "\n",
    "**Performance Tips:**\n",
    "- For large files: Use download_file() or streaming\n",
    "- For text files: Decode with proper encoding (UTF-8 default)\n",
    "- For binary files: Use raw bytes without decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c473d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Read 14 characters from 'real-learn-s3/test/hello.txt'\n",
      "Content: Hello from S3!\n"
     ]
    }
   ],
   "source": [
    "def read_object(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Read S3 object content directly into memory\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to read\n",
    "    \n",
    "    Returns:\n",
    "        str: File content as string, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=object_name)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        print(f\"SUCCESS: Read {len(content)} characters from '{bucket}/{object_name}'\")\n",
    "        return content\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'NoSuchKey':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to read object - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "content = read_object(BUCKET_NAME, 'test/hello.txt')\n",
    "if content:\n",
    "    print(f\"Content: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58617b5a",
   "metadata": {},
   "source": [
    "## 10. List Objects with Details\n",
    "\n",
    "List all objects in a bucket with comprehensive metadata.\n",
    "\n",
    "**Retrieved Information:**\n",
    "- Object Key (name/path)\n",
    "- Size (bytes, KB, MB)\n",
    "- Last Modified date\n",
    "- Storage Class (STANDARD, GLACIER, etc.)\n",
    "- ETag (MD5 hash for data integrity)\n",
    "\n",
    "**Use Cases:**\n",
    "- Audit bucket contents\n",
    "- Monitor storage usage\n",
    "- Track file modifications\n",
    "- Verify data integrity\n",
    "- Generate inventory reports\n",
    "\n",
    "**Performance Notes:**\n",
    "- Returns up to 1000 objects per request\n",
    "- For buckets with more objects, use pagination (not shown here for simplicity)\n",
    "- Can filter by prefix to list specific folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8a1795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 'real-learn-s3/':\n",
      "Key: data/hosts.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-29 04:55:22\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Key: raw/\n",
      "Size: 0 B (0 bytes)\n",
      "Last Modified: 2026-01-29 03:49:08\n",
      "Storage Class: STANDARD\n",
      "ETag: \"d41d8cd98f00b204e9800998ecf8427e\"\n",
      "Key: raw/bookings.csv\n",
      "Size: 501.35 KB (513,378 bytes)\n",
      "Last Modified: 2026-01-29 03:49:39\n",
      "Storage Class: STANDARD\n",
      "ETag: \"203775ebda6b0e99de614895de78159f\"\n",
      "Key: test/hello.txt\n",
      "Size: 14 B (14 bytes)\n",
      "Last Modified: 2026-01-29 04:53:38\n",
      "Storage Class: STANDARD\n",
      "ETag: \"e19169950b1b59f05e3412e2f3975a3b\"\n",
      "Total: 4 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'data/hosts.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 4, 55, 22, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'raw/',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 3, 49, 8, tzinfo=tzutc()),\n",
       "  'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "  'ChecksumAlgorithm': ['CRC64NVME'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 0,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'raw/bookings.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 3, 49, 39, tzinfo=tzutc()),\n",
       "  'ETag': '\"203775ebda6b0e99de614895de78159f\"',\n",
       "  'ChecksumAlgorithm': ['CRC64NVME'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 513378,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'test/hello.txt',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 4, 53, 38, tzinfo=tzutc()),\n",
       "  'ETag': '\"e19169950b1b59f05e3412e2f3975a3b\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 14,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def list_objects_detailed(bucket, prefix=''):\n",
    "    \"\"\"\n",
    "    List all objects in a bucket with detailed metadata\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        prefix (str): Filter objects by prefix (folder path)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of object metadata dictionaries, or empty list if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(f\"No objects found in bucket '{bucket}' with prefix '{prefix}'\")\n",
    "            return []\n",
    "        \n",
    "        objects = []\n",
    "        print(f\"Objects in '{bucket}/{prefix}':\")\n",
    "        \n",
    "        for obj in response['Contents']:\n",
    "            # Convert size to human-readable format\n",
    "            size_bytes = obj['Size']\n",
    "            if size_bytes < 1024:\n",
    "                size_str = f\"{size_bytes} B\"\n",
    "            elif size_bytes < 1024**2:\n",
    "                size_str = f\"{size_bytes/1024:.2f} KB\"\n",
    "            else:\n",
    "                size_str = f\"{size_bytes/(1024**2):.2f} MB\"\n",
    "            \n",
    "            # Format last modified date\n",
    "            last_modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            print(f\"Key: {obj['Key']}\")\n",
    "            print(f\"Size: {size_str} ({size_bytes:,} bytes)\")\n",
    "            print(f\"Last Modified: {last_modified}\")\n",
    "            print(f\"Storage Class: {obj.get('StorageClass', 'STANDARD')}\")\n",
    "            print(f\"ETag: {obj['ETag']}\")\n",
    "            \n",
    "            objects.append(obj)\n",
    "        \n",
    "        print(f\"Total: {len(objects)} object(s)\")\n",
    "        return objects\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to list objects - {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "list_objects_detailed(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bdb20de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 'real-learn-s3/data/':\n",
      "Key: data/hosts.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-29 04:55:22\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Total: 1 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'data/hosts.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 4, 55, 22, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_objects_detailed(BUCKET_NAME, prefix='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1cc82b",
   "metadata": {},
   "source": [
    "## 11. Delete Object from S3\n",
    "\n",
    "Delete a specific object (file) from an S3 bucket.\n",
    "\n",
    "**Important Warnings:**\n",
    "- Deletion is PERMANENT (unless versioning is enabled)\n",
    "- No confirmation prompt - deletes immediately\n",
    "- Cannot be undone for non-versioned buckets\n",
    "- Returns success even if object doesn't exist\n",
    "\n",
    "**Best Practices:**\n",
    "- Always verify object key before deletion\n",
    "- Enable versioning for important buckets\n",
    "- Use lifecycle policies for automated cleanup\n",
    "- Consider archiving to Glacier before deletion\n",
    "\n",
    "**Safety Tips:**\n",
    "```python\n",
    "# List objects first to verify\n",
    "list_objects_detailed(bucket, prefix='folder/')\n",
    "\n",
    "# Then delete specific object\n",
    "delete_object(bucket, 'folder/file.txt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60ec8532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Object 'test/hello.txt' deleted from bucket 'real-learn-s3'\n",
      "No objects found in bucket 'real-learn-s3' with prefix 'test/'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_object(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Delete an object from S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key) to delete\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if deletion successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.delete_object(Bucket=bucket, Key=object_name)\n",
    "        print(f\"SUCCESS: Object '{object_name}' deleted from bucket '{bucket}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to delete object - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test with caution):\n",
    "delete_object(BUCKET_NAME, 'test/hello.txt')\n",
    "\n",
    "# list the object to confirm deletion\n",
    "list_objects_detailed(BUCKET_NAME, prefix='test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f6431",
   "metadata": {},
   "source": [
    "## 12. Object Metadata Operations\n",
    "\n",
    "S3 allows you to attach custom metadata to objects for organization and tracking.\n",
    "\n",
    "**What is Metadata?**\n",
    "- Key-value pairs attached to S3 objects\n",
    "- Two types: System metadata (AWS-managed) and User metadata (custom)\n",
    "- User metadata keys must start with `x-amz-meta-` prefix\n",
    "- Useful for categorization, tracking, and application logic\n",
    "\n",
    "**System Metadata (AWS-managed):**\n",
    "- Content-Type (MIME type)\n",
    "- Content-Length (file size)\n",
    "- Last-Modified (timestamp)\n",
    "- ETag (version identifier)\n",
    "\n",
    "**User Metadata (Custom):**\n",
    "- Any custom key-value pairs\n",
    "- Examples: author, version, department, project-id\n",
    "- Maximum 2KB total metadata size\n",
    "- Cannot be updated after upload (must re-upload object)\n",
    "\n",
    "**Use Cases:**\n",
    "- Tag files by department, project, or owner\n",
    "- Store application-specific data\n",
    "- Track file versions or processing status\n",
    "- Add searchable attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0c41d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'real-learn-s3/data/hosts.csv':\n",
      "SYSTEM METADATA:\n",
      "Content-Type: binary/octet-stream\n",
      "Content-Length: 13,083 bytes\n",
      "Last-Modified: 2026-01-29 04:55:22+00:00\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Storage-Class: STANDARD\n",
      "USER METADATA: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ZV2PMBDRZGJ73968',\n",
       "  'HostId': 'xojZDBKmBzDoaBSzyt3cWHPeltKTyujr36RlAVPyg/iYbR9rUClO9ePYu3TlNVSLgFCd8u3t3as=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'xojZDBKmBzDoaBSzyt3cWHPeltKTyujr36RlAVPyg/iYbR9rUClO9ePYu3TlNVSLgFCd8u3t3as=',\n",
       "   'x-amz-request-id': 'ZV2PMBDRZGJ73968',\n",
       "   'date': 'Thu, 29 Jan 2026 05:07:34 GMT',\n",
       "   'last-modified': 'Thu, 29 Jan 2026 04:55:22 GMT',\n",
       "   'etag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'content-length': '13083',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2026, 1, 29, 4, 55, 22, tzinfo=tzutc()),\n",
       " 'ContentLength': 13083,\n",
       " 'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_object_metadata(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Retrieve metadata for an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata dictionary, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=object_name)\n",
    "        \n",
    "        print(f\"Metadata for '{bucket}/{object_name}':\")\n",
    "        \n",
    "        # System metadata\n",
    "        print(\"SYSTEM METADATA:\")\n",
    "        print(f\"Content-Type: {response.get('ContentType', 'N/A')}\")\n",
    "        print(f\"Content-Length: {response.get('ContentLength', 0):,} bytes\")\n",
    "        print(f\"Last-Modified: {response.get('LastModified', 'N/A')}\")\n",
    "        print(f\"ETag: {response.get('ETag', 'N/A')}\")\n",
    "        print(f\"Storage-Class: {response.get('StorageClass', 'STANDARD')}\")\n",
    "        \n",
    "        # User metadata (custom)\n",
    "        user_metadata = response.get('Metadata', {})\n",
    "        if user_metadata:\n",
    "            print(\"USER METADATA (Custom):\")\n",
    "            for key, value in user_metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(\"USER METADATA: None\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to get metadata - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "get_object_metadata(BUCKET_NAME, 'data/hosts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "356c7127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/hosts.csv' uploaded to 'real-learn-s3/data/hosts-with-meta.csv'\n",
      "With metadata:\n",
      "  author: Data Team\n",
      "  department: Analytics\n",
      "  version: 1.0\n",
      "  description: CSV data file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_with_metadata(file_name, bucket, object_name=None, metadata=None):\n",
    "    \"\"\"\n",
    "    Upload a file to S3 with custom metadata\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "        metadata (dict): Custom metadata key-value pairs\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \n",
    "    Example metadata:\n",
    "        {\n",
    "            'author': 'John Doe',\n",
    "            'department': 'Engineering',\n",
    "            'version': '1.0',\n",
    "            'project': 'data-analysis'\n",
    "        }\n",
    "    \"\"\"\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "    \n",
    "    try:\n",
    "        # Upload with metadata\n",
    "        s3_client.upload_file(\n",
    "            file_name, \n",
    "            bucket, \n",
    "            object_name,\n",
    "            ExtraArgs={'Metadata': metadata}\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        if metadata:\n",
    "            print(\"With metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "metadata = {\n",
    "    'author': 'Data Team',\n",
    "    'department': 'Analytics',\n",
    "    'version': '1.0',\n",
    "    'description': 'CSV data file'\n",
    "}\n",
    "upload_with_metadata('data/hosts.csv', BUCKET_NAME, 'data/hosts-with-meta.csv', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84ea27b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'real-learn-s3/data/hosts-with-meta.csv':\n",
      "SYSTEM METADATA:\n",
      "Content-Type: binary/octet-stream\n",
      "Content-Length: 13,083 bytes\n",
      "Last-Modified: 2026-01-29 05:09:11+00:00\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Storage-Class: STANDARD\n",
      "USER METADATA (Custom):\n",
      "  department: Analytics\n",
      "  version: 1.0\n",
      "  author: Data Team\n",
      "  description: CSV data file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ACT4WT2Q699CPCX9',\n",
       "  'HostId': 'm7ipBRizxIEiUXphyWuFF29RvAwP3u0brcHML64c+CawxoqZqoKJJOzQit/cArDadZh1j6qORAMA5UDKA6sq1FD0Tepy7xvMcmnwJmIYJ44=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'm7ipBRizxIEiUXphyWuFF29RvAwP3u0brcHML64c+CawxoqZqoKJJOzQit/cArDadZh1j6qORAMA5UDKA6sq1FD0Tepy7xvMcmnwJmIYJ44=',\n",
       "   'x-amz-request-id': 'ACT4WT2Q699CPCX9',\n",
       "   'date': 'Thu, 29 Jan 2026 05:09:16 GMT',\n",
       "   'last-modified': 'Thu, 29 Jan 2026 05:09:11 GMT',\n",
       "   'etag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'x-amz-meta-department': 'Analytics',\n",
       "   'x-amz-meta-version': '1.0',\n",
       "   'x-amz-meta-author': 'Data Team',\n",
       "   'x-amz-meta-description': 'CSV data file',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'content-length': '13083',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2026, 1, 29, 5, 9, 11, tzinfo=tzutc()),\n",
       " 'ContentLength': 13083,\n",
       " 'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {'department': 'Analytics',\n",
       "  'version': '1.0',\n",
       "  'author': 'Data Team',\n",
       "  'description': 'CSV data file'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Verify metadata was saved\n",
    "get_object_metadata(BUCKET_NAME, 'data/hosts-with-meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360187d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
