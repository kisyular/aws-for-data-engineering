{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2a6e24",
   "metadata": {},
   "source": [
    "# AWS S3 Operations - Comprehensive Guide\n",
    "\n",
    "This notebook provides a complete walkthrough of AWS S3 operations using boto3 SDK.\n",
    "\n",
    "## Prerequisites - Manual AWS Setup\n",
    "\n",
    "Before running this notebook, the following setup was completed manually in AWS Console:\n",
    "\n",
    "### Step 1: S3 Bucket Creation\n",
    "- Logged into AWS account\n",
    "- Created S3 bucket named **\"real-learn-s3\"** in **us-east-1** region\n",
    "- Configuration:\n",
    "  - Public access: **BLOCKED** (security best practice)\n",
    "  - Versioning: **ENABLED** (maintains file history)\n",
    "- Uploaded initial test file: **bookings.csv**\n",
    "\n",
    "### Step 2: IAM User Configuration\n",
    "- Created IAM user with **programmatic access** (no console access needed)\n",
    "- Attached policy: **AmazonS3FullAccess** (full S3 permissions)\n",
    "- Generated access credentials:\n",
    "  - Access Key ID\n",
    "  - Secret Access Key\n",
    "- Saved credentials securely in `.env` file (never commit to git)\n",
    "\n",
    "### Step 3: Environment File\n",
    "Created `.env` file with the following variables:\n",
    "```\n",
    "AWS_BUCKET_NAME=real-learn-s3\n",
    "AWS_ACCESS_KEY=your-access-key-id\n",
    "AWS_SECRET_KEY=your-secret-access-key\n",
    "AWS_REGION=us-east-1\n",
    "```\n",
    "\n",
    "### Step 4: Python Environment\n",
    "- Installed boto3 library for AWS SDK\n",
    "- Installed python-dotenv for secure credential loading\n",
    "- Created initial Python script to verify S3 connection and list objects\n",
    "\n",
    "Now we're ready to explore S3 operations programmatically.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **S3 Fundamentals** - Understanding buckets and objects\n",
    "2. **Bucket Operations** - List, create, and manage buckets\n",
    "3. **Object Operations** - Upload, download, list, and delete files\n",
    "4. **Advanced Features** - Metadata, presigned URLs, progress tracking\n",
    "5. **Error Handling** - Robust exception management\n",
    "6. **Best Practices** - Security and performance optimization\n",
    "\n",
    "## S3 Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  AWS S3 STRUCTURE                                       │\n",
    "│                                                         │\n",
    "│  Your AWS Account                                       │\n",
    "│  └─► Buckets (Globally unique names)                    │\n",
    "│      ├─► Bucket 1: my-app-data-2024                     │\n",
    "│      │   ├─► Object: file1.txt                          │\n",
    "│      │   ├─► Object: images/photo.jpg                   │\n",
    "│      │   └─► Object: data/2024/records.csv              │\n",
    "│      │                                                  │\n",
    "│      └─► Bucket 2: backups-prod                         │\n",
    "│          ├─► Object: backup-2024-01.tar.gz              │\n",
    "│          └─► Object: logs/app.log                       │\n",
    "│                                                         │\n",
    "│  Key Concepts:                                          │\n",
    "│  • Bucket = Container (like a root folder)              │\n",
    "│  • Object = File with metadata                          │\n",
    "│  • Key = Full path/name (e.g., \"folder/file.txt\")       │\n",
    "│  • Region = Geographic location                         │\n",
    "│  • Storage Class = Cost/access tier                     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ced8b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db96ea8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Loading AWS credentials securely from environment variables using `.env` file.\n",
    "\n",
    "**Security Best Practice:** Never hardcode credentials in your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4885676",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = os.getenv(\"AWS_BUCKET_NAME\")    \n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deefcef",
   "metadata": {},
   "source": [
    "## 2. AWS Credentials Configuration\n",
    "\n",
    "Required environment variables in your `.env` file:\n",
    "```\n",
    "AWS_BUCKET_NAME=your-bucket-name\n",
    "AWS_ACCESS_KEY=your-access-key-id\n",
    "AWS_SECRET_KEY=your-secret-access-key\n",
    "AWS_REGION=us-east-1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449f59b",
   "metadata": {},
   "source": [
    "## 3. Initialize S3 Client\n",
    "\n",
    "boto3 provides two interfaces:\n",
    "- **Client**: Low-level service access (more control)\n",
    "- **Resource**: Higher-level object-oriented interface (easier to use)\n",
    "\n",
    "We'll use the **client** interface for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec4b9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an s3 client\n",
    "from botocore.config import Config\n",
    "\n",
    "# Configure S3 client with Signature Version 4 and regional endpoint\n",
    "# Regional endpoint is required for presigned URLs to work correctly\n",
    "\n",
    "s3_client = boto3.client('s3', \n",
    "                         endpoint_url=f'https://s3.{AWS_REGION}.amazonaws.com',\n",
    "                         config=Config(signature_version='s3v4'),\n",
    "                         region_name=AWS_REGION,\n",
    "                         aws_secret_access_key=SECRET_KEY,\n",
    "                         aws_access_key_id=ACCESS_KEY,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024553e",
   "metadata": {},
   "source": [
    "## 4. Bucket Operations\n",
    "\n",
    "### Operation Flow\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│  BUCKET LIFECYCLE                       │\n",
    "│                                         │\n",
    "│  1. CREATE → Bucket exists in S3        │\n",
    "│  2. LIST → View all your buckets        │\n",
    "│  3. CONFIGURE → Set permissions/policies│\n",
    "│  4. USE → Upload/download objects       │\n",
    "│  5. DELETE → Remove bucket (if empty)   │\n",
    "└─────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e393ac2",
   "metadata": {},
   "source": [
    "## 5. Create Bucket\n",
    "\n",
    "Create a new S3 bucket in your account.\n",
    "\n",
    "**Important Rules:**\n",
    "- Bucket names must be globally unique across ALL AWS accounts\n",
    "- Only lowercase letters, numbers, hyphens, and dots allowed\n",
    "- Must be 3-63 characters long\n",
    "- Cannot start or end with a hyphen\n",
    "- Region matters for latency and compliance\n",
    "\n",
    "**Regions:**\n",
    "- `us-east-1` is the default and doesn't require LocationConstraint\n",
    "- Other regions need explicit LocationConstraint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d35e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Bucket 'real-learn-s3-test' already exists and is owned by you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in a specified region\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): Name for the bucket (must be globally unique)\n",
    "        region (str): AWS region (if None, uses default from client)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if bucket created, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region is None or region == 'us-east-1':\n",
    "            # us-east-1 doesn't require LocationConstraint\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            # Other regions require LocationConstraint\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"SUCCESS: Bucket '{bucket_name}' created successfully\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'BucketAlreadyExists':\n",
    "            print(f\"ERROR: Bucket '{bucket_name}' already exists (owned by someone else)\")\n",
    "        elif error_code == 'BucketAlreadyOwnedByYou':\n",
    "            print(f\"INFO: Bucket '{bucket_name}' already exists and is owned by you\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create bucket - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "create_bucket(f\"{BUCKET_NAME}-test\", region=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c12e2",
   "metadata": {},
   "source": [
    "## 6. Upload File to S3\n",
    "\n",
    "Upload files from your local filesystem to an S3 bucket.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `file_name`: Path to local file\n",
    "- `bucket`: Target S3 bucket name\n",
    "- `object_name`: Key (path/name) in S3 (if None, uses file_name)\n",
    "\n",
    "**Use Cases:**\n",
    "- Backup files to cloud storage\n",
    "- Store application data\n",
    "- Host static website content\n",
    "- Archive logs and reports\n",
    "\n",
    "**File Types:** Any file type supported (images, videos, documents, code, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21b57c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/hosts.csv' uploaded to 'real-learn-s3/data/hosts.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    # If S3 object_name not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_file('data/hosts.csv', BUCKET_NAME, 'data/hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e03631",
   "metadata": {},
   "source": [
    "## 7. Download File from S3\n",
    "\n",
    "Download objects from S3 to your local filesystem.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `bucket`: Source S3 bucket name\n",
    "- `object_name`: Key (path/name) in S3\n",
    "- `file_name`: Destination path on local filesystem\n",
    "\n",
    "**Important Notes:**\n",
    "- Creates directories automatically if they don't exist\n",
    "- Overwrites existing local files without warning\n",
    "- Preserves file content but not S3 metadata\n",
    "\n",
    "**Common Pattern:**\n",
    "```python\n",
    "# Download backup\n",
    "download_file('my-bucket', 'backups/db-2024.sql', './local-backups/db.sql')\n",
    "\n",
    "# Download with same name\n",
    "download_file('my-bucket', 'report.pdf', 'report.pdf')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3458c734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'real-learn-s3/raw/bookings.csv' downloaded to './downloads/bookings.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_file(bucket, object_name, file_name):\n",
    "    \"\"\"\n",
    "    Download a file from an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to download\n",
    "        file_name (str): Local file path to save\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if download successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "        \n",
    "        s3_client.download_file(bucket, object_name, file_name)\n",
    "        print(f\"SUCCESS: '{bucket}/{object_name}' downloaded to '{file_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to download file - {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "download_file(BUCKET_NAME, 'raw/bookings.csv', './downloads/bookings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5d749",
   "metadata": {},
   "source": [
    "## 8. Upload String Content to S3\n",
    "\n",
    "Upload string/text data directly to S3 without creating a local file first.\n",
    "\n",
    "**Use Cases:**\n",
    "- Store JSON data from API responses\n",
    "- Save generated text, logs, or reports\n",
    "- Write configuration files\n",
    "- Store processed data without local disk I/O\n",
    "\n",
    "**Advantages:**\n",
    "- No temporary files needed\n",
    "- Faster for small text content\n",
    "- Memory-efficient for string data\n",
    "- Direct encoding control (UTF-8 by default)\n",
    "\n",
    "**Example Scenarios:**\n",
    "```python\n",
    "# Save JSON data\n",
    "upload_string('{\"status\": \"ok\"}', 'my-bucket', 'config.json')\n",
    "\n",
    "# Save log entry\n",
    "upload_string('2024-01-28: System started', 'logs-bucket', 'app.log')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d992ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: String content uploaded to 'real-learn-s3/test/hello.txt'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_string(content, bucket, object_name):\n",
    "    \"\"\"\n",
    "    Upload string content directly to S3\n",
    "    \n",
    "    Args:\n",
    "        content (str): String content to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=object_name,\n",
    "            Body=content.encode('utf-8')\n",
    "        )\n",
    "        print(f\"SUCCESS: String content uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload string - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_string('Hello from S3!', BUCKET_NAME, 'test/hello.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac7dd7",
   "metadata": {},
   "source": [
    "## 9. Read Object Content from S3\n",
    "\n",
    "Read S3 object content directly into memory without downloading to a file.\n",
    "\n",
    "**Use Cases:**\n",
    "- Read configuration files\n",
    "- Load small datasets for processing\n",
    "- Retrieve API responses or JSON data\n",
    "- Stream log files for analysis\n",
    "\n",
    "**Important Notes:**\n",
    "- Best for small to medium files (avoid for GB-sized files)\n",
    "- Returns content as bytes (decode to string if needed)\n",
    "- Entire file loaded into memory\n",
    "- More efficient than download + read for small files\n",
    "\n",
    "**Performance Tips:**\n",
    "- For large files: Use download_file() or streaming\n",
    "- For text files: Decode with proper encoding (UTF-8 default)\n",
    "- For binary files: Use raw bytes without decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c473d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Read 14 characters from 'real-learn-s3/test/hello.txt'\n",
      "Content: Hello from S3!\n"
     ]
    }
   ],
   "source": [
    "def read_object(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Read S3 object content directly into memory\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to read\n",
    "    \n",
    "    Returns:\n",
    "        str: File content as string, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=object_name)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        print(f\"SUCCESS: Read {len(content)} characters from '{bucket}/{object_name}'\")\n",
    "        return content\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'NoSuchKey':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to read object - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "content = read_object(BUCKET_NAME, 'test/hello.txt')\n",
    "if content:\n",
    "    print(f\"Content: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58617b5a",
   "metadata": {},
   "source": [
    "## 10. List Objects with Details\n",
    "\n",
    "List all objects in a bucket with comprehensive metadata.\n",
    "\n",
    "**Retrieved Information:**\n",
    "- Object Key (name/path)\n",
    "- Size (bytes, KB, MB)\n",
    "- Last Modified date\n",
    "- Storage Class (STANDARD, GLACIER, etc.)\n",
    "- ETag (MD5 hash for data integrity)\n",
    "\n",
    "**Use Cases:**\n",
    "- Audit bucket contents\n",
    "- Monitor storage usage\n",
    "- Track file modifications\n",
    "- Verify data integrity\n",
    "- Generate inventory reports\n",
    "\n",
    "**Performance Notes:**\n",
    "- Returns up to 1000 objects per request\n",
    "- For buckets with more objects, use pagination (not shown here for simplicity)\n",
    "- Can filter by prefix to list specific folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8a1795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 'real-learn-s3/':\n",
      "Key: data/hosts-with-meta.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-29 05:56:48\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Key: data/hosts.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-29 06:02:40\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Key: raw/\n",
      "Size: 0 B (0 bytes)\n",
      "Last Modified: 2026-01-29 03:49:08\n",
      "Storage Class: STANDARD\n",
      "ETag: \"d41d8cd98f00b204e9800998ecf8427e\"\n",
      "Key: raw/bookings.csv\n",
      "Size: 501.35 KB (513,378 bytes)\n",
      "Last Modified: 2026-01-29 03:49:39\n",
      "Storage Class: STANDARD\n",
      "ETag: \"203775ebda6b0e99de614895de78159f\"\n",
      "Key: test/hello.txt\n",
      "Size: 14 B (14 bytes)\n",
      "Last Modified: 2026-01-29 06:02:41\n",
      "Storage Class: STANDARD\n",
      "ETag: \"e19169950b1b59f05e3412e2f3975a3b\"\n",
      "Total: 5 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'data/hosts-with-meta.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 5, 56, 48, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'data/hosts.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 6, 2, 40, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'raw/',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 3, 49, 8, tzinfo=tzutc()),\n",
       "  'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "  'ChecksumAlgorithm': ['CRC64NVME'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 0,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'raw/bookings.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 3, 49, 39, tzinfo=tzutc()),\n",
       "  'ETag': '\"203775ebda6b0e99de614895de78159f\"',\n",
       "  'ChecksumAlgorithm': ['CRC64NVME'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 513378,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'test/hello.txt',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 6, 2, 41, tzinfo=tzutc()),\n",
       "  'ETag': '\"e19169950b1b59f05e3412e2f3975a3b\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 14,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def list_objects_detailed(bucket, prefix=''):\n",
    "    \"\"\"\n",
    "    List all objects in a bucket with detailed metadata\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        prefix (str): Filter objects by prefix (folder path)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of object metadata dictionaries, or empty list if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(f\"No objects found in bucket '{bucket}' with prefix '{prefix}'\")\n",
    "            return []\n",
    "        \n",
    "        objects = []\n",
    "        print(f\"Objects in '{bucket}/{prefix}':\")\n",
    "        \n",
    "        for obj in response['Contents']:\n",
    "            # Convert size to human-readable format\n",
    "            size_bytes = obj['Size']\n",
    "            if size_bytes < 1024:\n",
    "                size_str = f\"{size_bytes} B\"\n",
    "            elif size_bytes < 1024**2:\n",
    "                size_str = f\"{size_bytes/1024:.2f} KB\"\n",
    "            else:\n",
    "                size_str = f\"{size_bytes/(1024**2):.2f} MB\"\n",
    "            \n",
    "            # Format last modified date\n",
    "            last_modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            print(f\"Key: {obj['Key']}\")\n",
    "            print(f\"Size: {size_str} ({size_bytes:,} bytes)\")\n",
    "            print(f\"Last Modified: {last_modified}\")\n",
    "            print(f\"Storage Class: {obj.get('StorageClass', 'STANDARD')}\")\n",
    "            print(f\"ETag: {obj['ETag']}\")\n",
    "            \n",
    "            objects.append(obj)\n",
    "        \n",
    "        print(f\"Total: {len(objects)} object(s)\")\n",
    "        return objects\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to list objects - {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "list_objects_detailed(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bdb20de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 'real-learn-s3/data/':\n",
      "Key: data/hosts-with-meta.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-29 05:56:48\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Key: data/hosts.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-29 06:02:40\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Total: 2 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'data/hosts-with-meta.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 5, 56, 48, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'data/hosts.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 29, 6, 2, 40, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_objects_detailed(BUCKET_NAME, prefix='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1cc82b",
   "metadata": {},
   "source": [
    "## 11. Delete Object from S3\n",
    "\n",
    "Delete a specific object (file) from an S3 bucket.\n",
    "\n",
    "**Important Warnings:**\n",
    "- Deletion is PERMANENT (unless versioning is enabled)\n",
    "- No confirmation prompt - deletes immediately\n",
    "- Cannot be undone for non-versioned buckets\n",
    "- Returns success even if object doesn't exist\n",
    "\n",
    "**Best Practices:**\n",
    "- Always verify object key before deletion\n",
    "- Enable versioning for important buckets\n",
    "- Use lifecycle policies for automated cleanup\n",
    "- Consider archiving to Glacier before deletion\n",
    "\n",
    "**Safety Tips:**\n",
    "```python\n",
    "# List objects first to verify\n",
    "list_objects_detailed(bucket, prefix='folder/')\n",
    "\n",
    "# Then delete specific object\n",
    "delete_object(bucket, 'folder/file.txt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60ec8532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Object 'test/hello.txt' deleted from bucket 'real-learn-s3'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No objects found in bucket 'real-learn-s3' with prefix 'test/'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_object(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Delete an object from S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key) to delete\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if deletion successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.delete_object(Bucket=bucket, Key=object_name)\n",
    "        print(f\"SUCCESS: Object '{object_name}' deleted from bucket '{bucket}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to delete object - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test with caution):\n",
    "delete_object(BUCKET_NAME, 'test/hello.txt')\n",
    "\n",
    "# list the object to confirm deletion\n",
    "list_objects_detailed(BUCKET_NAME, prefix='test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f6431",
   "metadata": {},
   "source": [
    "## 12. Object Metadata Operations\n",
    "\n",
    "S3 allows you to attach custom metadata to objects for organization and tracking.\n",
    "\n",
    "**What is Metadata?**\n",
    "- Key-value pairs attached to S3 objects\n",
    "- Two types: System metadata (AWS-managed) and User metadata (custom)\n",
    "- User metadata keys must start with `x-amz-meta-` prefix\n",
    "- Useful for categorization, tracking, and application logic\n",
    "\n",
    "**System Metadata (AWS-managed):**\n",
    "- Content-Type (MIME type)\n",
    "- Content-Length (file size)\n",
    "- Last-Modified (timestamp)\n",
    "- ETag (version identifier)\n",
    "\n",
    "**User Metadata (Custom):**\n",
    "- Any custom key-value pairs\n",
    "- Examples: author, version, department, project-id\n",
    "- Maximum 2KB total metadata size\n",
    "- Cannot be updated after upload (must re-upload object)\n",
    "\n",
    "**Use Cases:**\n",
    "- Tag files by department, project, or owner\n",
    "- Store application-specific data\n",
    "- Track file versions or processing status\n",
    "- Add searchable attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0c41d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'real-learn-s3/data/hosts.csv':\n",
      "SYSTEM METADATA:\n",
      "Content-Type: binary/octet-stream\n",
      "Content-Length: 13,083 bytes\n",
      "Last-Modified: 2026-01-29 06:02:40+00:00\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Storage-Class: STANDARD\n",
      "USER METADATA: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'YAPXGBHT668TXHYK',\n",
       "  'HostId': '5Df/VbMimaH+k0twYiW3ciArqmCZeEdlJA92EUXDlTrbmGYs8By2ZTwXcJnBh/LeaSJYCga7o8E=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '5Df/VbMimaH+k0twYiW3ciArqmCZeEdlJA92EUXDlTrbmGYs8By2ZTwXcJnBh/LeaSJYCga7o8E=',\n",
       "   'x-amz-request-id': 'YAPXGBHT668TXHYK',\n",
       "   'date': 'Thu, 29 Jan 2026 06:02:41 GMT',\n",
       "   'last-modified': 'Thu, 29 Jan 2026 06:02:40 GMT',\n",
       "   'etag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'content-length': '13083',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2026, 1, 29, 6, 2, 40, tzinfo=tzutc()),\n",
       " 'ContentLength': 13083,\n",
       " 'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_object_metadata(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Retrieve metadata for an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata dictionary, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=object_name)\n",
    "        \n",
    "        print(f\"Metadata for '{bucket}/{object_name}':\")\n",
    "        \n",
    "        # System metadata\n",
    "        print(\"SYSTEM METADATA:\")\n",
    "        print(f\"Content-Type: {response.get('ContentType', 'N/A')}\")\n",
    "        print(f\"Content-Length: {response.get('ContentLength', 0):,} bytes\")\n",
    "        print(f\"Last-Modified: {response.get('LastModified', 'N/A')}\")\n",
    "        print(f\"ETag: {response.get('ETag', 'N/A')}\")\n",
    "        print(f\"Storage-Class: {response.get('StorageClass', 'STANDARD')}\")\n",
    "        \n",
    "        # User metadata (custom)\n",
    "        user_metadata = response.get('Metadata', {})\n",
    "        if user_metadata:\n",
    "            print(\"USER METADATA (Custom):\")\n",
    "            for key, value in user_metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(\"USER METADATA: None\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to get metadata - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "get_object_metadata(BUCKET_NAME, 'data/hosts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "356c7127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/hosts.csv' uploaded to 'real-learn-s3/data/hosts-with-meta.csv'\n",
      "With metadata:\n",
      "  author: Data Team\n",
      "  department: Analytics\n",
      "  version: 1.0\n",
      "  description: CSV data file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_with_metadata(file_name, bucket, object_name=None, metadata=None):\n",
    "    \"\"\"\n",
    "    Upload a file to S3 with custom metadata\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "        metadata (dict): Custom metadata key-value pairs\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \n",
    "    Example metadata:\n",
    "        {\n",
    "            'author': 'John Doe',\n",
    "            'department': 'Engineering',\n",
    "            'version': '1.0',\n",
    "            'project': 'data-analysis'\n",
    "        }\n",
    "    \"\"\"\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "    \n",
    "    try:\n",
    "        # Upload with metadata\n",
    "        s3_client.upload_file(\n",
    "            file_name, \n",
    "            bucket, \n",
    "            object_name,\n",
    "            ExtraArgs={'Metadata': metadata}\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        if metadata:\n",
    "            print(\"With metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "metadata = {\n",
    "    'author': 'Data Team',\n",
    "    'department': 'Analytics',\n",
    "    'version': '1.0',\n",
    "    'description': 'CSV data file'\n",
    "}\n",
    "upload_with_metadata('data/hosts.csv', BUCKET_NAME, 'data/hosts-with-meta.csv', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84ea27b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'real-learn-s3/data/hosts-with-meta.csv':\n",
      "SYSTEM METADATA:\n",
      "Content-Type: binary/octet-stream\n",
      "Content-Length: 13,083 bytes\n",
      "Last-Modified: 2026-01-29 06:02:41+00:00\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Storage-Class: STANDARD\n",
      "USER METADATA (Custom):\n",
      "  department: Analytics\n",
      "  version: 1.0\n",
      "  author: Data Team\n",
      "  description: CSV data file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'YAPTCYMZN7420R0T',\n",
       "  'HostId': 'LT1zUH6REHmFVXEiMlCtpl4ScRDkk/IO5CskhA0B/AQE5tayMw+cJWoHpdvJsHsa43hcXd2WhHs=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'LT1zUH6REHmFVXEiMlCtpl4ScRDkk/IO5CskhA0B/AQE5tayMw+cJWoHpdvJsHsa43hcXd2WhHs=',\n",
       "   'x-amz-request-id': 'YAPTCYMZN7420R0T',\n",
       "   'date': 'Thu, 29 Jan 2026 06:02:41 GMT',\n",
       "   'last-modified': 'Thu, 29 Jan 2026 06:02:41 GMT',\n",
       "   'etag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'x-amz-meta-department': 'Analytics',\n",
       "   'x-amz-meta-version': '1.0',\n",
       "   'x-amz-meta-author': 'Data Team',\n",
       "   'x-amz-meta-description': 'CSV data file',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'content-length': '13083',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2026, 1, 29, 6, 2, 41, tzinfo=tzutc()),\n",
       " 'ContentLength': 13083,\n",
       " 'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {'department': 'Analytics',\n",
       "  'version': '1.0',\n",
       "  'author': 'Data Team',\n",
       "  'description': 'CSV data file'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Verify metadata was saved\n",
    "get_object_metadata(BUCKET_NAME, 'data/hosts-with-meta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce0356",
   "metadata": {},
   "source": [
    "## 13. Presigned URLs\n",
    "\n",
    "Generate temporary, secure URLs that grant time-limited access to S3 objects without requiring AWS credentials.\n",
    "\n",
    "**What are Presigned URLs?**\n",
    "- Temporary URLs with embedded authentication\n",
    "- Allow public access to private S3 objects for a limited time\n",
    "- No AWS credentials needed by the recipient\n",
    "- Expires after specified duration (default 1 hour, max 7 days)\n",
    "\n",
    "**How They Work:**\n",
    "```\n",
    "1. Your app generates presigned URL with your credentials\n",
    "2. URL includes signature and expiration time\n",
    "3. Share URL with user (email, web page, etc.)\n",
    "4. User accesses S3 directly using the URL\n",
    "5. URL expires after set time period\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- Share private files temporarily (reports, images, videos)\n",
    "- Allow users to download files without authentication\n",
    "- Enable temporary file uploads from web forms\n",
    "- Distribute time-limited content (tickets, invoices)\n",
    "- Securely share large files without email attachments\n",
    "\n",
    "**Security Benefits:**\n",
    "- No credential sharing required\n",
    "- Automatic expiration prevents long-term access\n",
    "- Can revoke by deleting original object\n",
    "- Audit trail in CloudTrail logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2805d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Presigned URL generated for 'real-learn-s3/data/hosts.csv'\n",
      "Expires in: 3600 seconds (1.0 hours)\n",
      "\n",
      "URL (valid for 1.0 hours):\n",
      "https://s3.us-east-2.amazonaws.com/real-learn-s3/data/hosts.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAWPVGU3OO6BQJAVE3%2F20260129%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20260129T060241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=34b2f2b03a62ad3d171556fdb530ae4c94cfe538df7939ec323ff64ab1e19f71\n",
      "\n",
      "Anyone with this URL can download the file until it expires.\n"
     ]
    }
   ],
   "source": [
    "def generate_presigned_download_url(bucket, object_name, expiration=3600):\n",
    "    \"\"\"\n",
    "    Generate a presigned URL for downloading an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "        expiration (int): URL expiration time in seconds (default 3600 = 1 hour)\n",
    "    \n",
    "    Returns:\n",
    "        str: Presigned URL, or None if error\n",
    "    \n",
    "    Common expiration times:\n",
    "        - 3600 = 1 hour (default)\n",
    "        - 7200 = 2 hours\n",
    "        - 86400 = 24 hours\n",
    "        - 604800 = 7 days (maximum)\n",
    "    \n",
    "    Note: Uses AWS Signature Version 4 (required by S3)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = s3_client.generate_presigned_url(\n",
    "            'get_object',\n",
    "            Params={\n",
    "                'Bucket': bucket,\n",
    "                'Key': object_name\n",
    "            },\n",
    "            ExpiresIn=expiration\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Presigned URL generated for '{bucket}/{object_name}'\")\n",
    "        print(f\"Expires in: {expiration} seconds ({expiration/3600:.1f} hours)\")\n",
    "        print(f\"\\nURL (valid for {expiration/3600:.1f} hours):\")\n",
    "        print(url)\n",
    "        print(\"\\nAnyone with this URL can download the file until it expires.\")\n",
    "        \n",
    "        return url\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to generate presigned URL - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "url = generate_presigned_download_url(BUCKET_NAME, 'data/hosts.csv', expiration=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8294f35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 13083 bytes\n"
     ]
    }
   ],
   "source": [
    "# # Test the URL (uncomment to download using the URL)\n",
    "import requests\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(f\"Successfully downloaded {len(response.content)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccef0f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Presigned upload URL generated for 'real-learn-s3/uploads/user-file.txt'\n",
      "Expires in: 3600 seconds (1.0 hours)\n",
      "URL (valid for 1.0 hours):\n",
      "https://s3.us-east-2.amazonaws.com/real-learn-s3/uploads/user-file.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAWPVGU3OO6BQJAVE3%2F20260129%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20260129T060241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4784f860ac1afb85460b93871c632ecb1257e1ef8cc56804fb70db593b0208a5\n",
      "Anyone with this URL can upload a file to this location until it expires.\n",
      "Upload using HTTP PUT method.\n"
     ]
    }
   ],
   "source": [
    "def generate_presigned_upload_url(bucket, object_name, expiration=3600):\n",
    "    \"\"\"\n",
    "    Generate a presigned URL for uploading to S3\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key) where file will be uploaded\n",
    "        expiration (int): URL expiration time in seconds (default 3600 = 1 hour)\n",
    "    \n",
    "    Returns:\n",
    "        str: Presigned URL, or None if error\n",
    "    \n",
    "    Use case:\n",
    "        Allow users to upload files directly to S3 without AWS credentials\n",
    "        Common in web forms, mobile apps, and file sharing applications\n",
    "    \n",
    "    Note: Uses AWS Signature Version 4 (required by S3)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = s3_client.generate_presigned_url(\n",
    "            'put_object',\n",
    "            Params={\n",
    "                'Bucket': bucket,\n",
    "                'Key': object_name\n",
    "            },\n",
    "            ExpiresIn=expiration\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Presigned upload URL generated for '{bucket}/{object_name}'\")\n",
    "        print(f\"Expires in: {expiration} seconds ({expiration/3600:.1f} hours)\")\n",
    "        print(f\"URL (valid for {expiration/3600:.1f} hours):\")\n",
    "        print(url)\n",
    "        print(\"Anyone with this URL can upload a file to this location until it expires.\")\n",
    "        print(\"Upload using HTTP PUT method.\")\n",
    "        \n",
    "        return url\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to generate presigned upload URL - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "\n",
    "upload_url = generate_presigned_upload_url(BUCKET_NAME, 'uploads/user-file.txt', expiration=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de6e7f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully via presigned URL!\n",
      "Metadata for 'real-learn-s3/uploads/user-file.txt':\n",
      "SYSTEM METADATA:\n",
      "Content-Type: binary/octet-stream\n",
      "Content-Length: 47 bytes\n",
      "Last-Modified: 2026-01-29 06:02:42+00:00\n",
      "ETag: \"603b54cde85cdaad11167f9df423d785\"\n",
      "Storage-Class: STANDARD\n",
      "USER METADATA: None\n"
     ]
    }
   ],
   "source": [
    "# # Test the upload URL (uncomment to upload using the URL)\n",
    "import requests\n",
    "test_data = b\"This is test content uploaded via presigned URL\"\n",
    "response = requests.put(upload_url, data=test_data)\n",
    "if response.status_code == 200:\n",
    "    print(\"File uploaded successfully via presigned URL!\")\n",
    "    # Verify the upload\n",
    "    get_object_metadata(BUCKET_NAME, 'uploads/user-file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817eaf92",
   "metadata": {},
   "source": [
    "## 14. Progress Tracking for Large Files\n",
    "\n",
    "Monitor upload and download progress for large files to provide user feedback.\n",
    "\n",
    "**Why Progress Tracking?**\n",
    "- Essential for large file transfers (videos, backups, datasets)\n",
    "- Prevents user frustration by showing operation status\n",
    "- Helps detect stalled or slow transfers\n",
    "- Enables progress bars in applications\n",
    "\n",
    "**How It Works:**\n",
    "- Uses callback functions that execute during transfer\n",
    "- Callback receives bytes transferred so far\n",
    "- Can calculate percentage, speed, and ETA\n",
    "- Works with upload_file() and download_file() operations\n",
    "\n",
    "**Use Cases:**\n",
    "- Web applications with file upload forms\n",
    "- Backup systems with large data transfers\n",
    "- Data migration tools\n",
    "- Video/media upload platforms\n",
    "- Batch file processing systems\n",
    "\n",
    "**Implementation:**\n",
    "boto3 provides a `Callback` parameter that accepts a function called periodically during transfer with the number of bytes transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01c27ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    \"\"\"\n",
    "    Track upload/download progress with visual feedback\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, filesize):\n",
    "        self.filename = filename\n",
    "        self.filesize = filesize\n",
    "        self.bytes_transferred = 0\n",
    "        self.start_time = None\n",
    "        \n",
    "    def __call__(self, bytes_amount):\n",
    "        \"\"\"\n",
    "        Called by boto3 during transfer\n",
    "        \n",
    "        Args:\n",
    "            bytes_amount (int): Bytes transferred in this chunk\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        self.bytes_transferred += bytes_amount\n",
    "        \n",
    "        # Calculate progress percentage\n",
    "        percentage = (self.bytes_transferred / self.filesize) * 100\n",
    "        \n",
    "        # Calculate transfer speed\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if elapsed_time > 0:\n",
    "            speed_mbps = (self.bytes_transferred / (1024 * 1024)) / elapsed_time\n",
    "        else:\n",
    "            speed_mbps = 0\n",
    "        \n",
    "        # Calculate ETA\n",
    "        if speed_mbps > 0:\n",
    "            remaining_mb = (self.filesize - self.bytes_transferred) / (1024 * 1024)\n",
    "            eta_seconds = remaining_mb / speed_mbps\n",
    "            eta_str = f\"{int(eta_seconds)}s\"\n",
    "        else:\n",
    "            eta_str = \"calculating...\"\n",
    "        \n",
    "        # Display progress\n",
    "        print(f\"\\r{self.filename}: {percentage:.1f}% | \"\n",
    "              f\"{self.bytes_transferred/(1024*1024):.2f}/{self.filesize/(1024*1024):.2f} MB | \"\n",
    "              f\"Speed: {speed_mbps:.2f} MB/s | ETA: {eta_str}\", end='')\n",
    "        \n",
    "        # Print newline when complete\n",
    "        if self.bytes_transferred >= self.filesize:\n",
    "            print()  # New line after completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6537955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hosts.csv: 100.0% | 0.01/0.01 MB | Speed: 4025.54 MB/s | ETA: 0s\n",
      "SUCCESS: 'data/hosts.csv' uploaded to 'real-learn-s3/data/hosts-progress-test.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_file_with_progress(file_name, bucket, object_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to S3 with progress tracking\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \n",
    "    Example output:\n",
    "        hosts.csv: 45.2% | 5.43/12.00 MB | Speed: 2.31 MB/s | ETA: 3s\n",
    "    \"\"\"\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    try:\n",
    "        # Get file size\n",
    "        filesize = os.path.getsize(file_name)\n",
    "        \n",
    "        # Create progress tracker\n",
    "        progress = ProgressTracker(os.path.basename(file_name), filesize)\n",
    "        \n",
    "        # Upload with progress callback\n",
    "        s3_client.upload_file(\n",
    "            file_name, \n",
    "            bucket, \n",
    "            object_name,\n",
    "            Callback=progress\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test with a larger file):\n",
    "upload_file_with_progress('data/hosts.csv', BUCKET_NAME, 'data/hosts-progress-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12fe432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hosts-progress-test.csv: 100.0% | 0.01/0.01 MB | Speed: 13083.00 MB/s | ETA: 0s\n",
      "SUCCESS: 'real-learn-s3/data/hosts.csv' downloaded to './downloads/hosts-progress-test.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_file_with_progress(bucket, object_name, file_name):\n",
    "    \"\"\"\n",
    "    Download a file from S3 with progress tracking\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to download\n",
    "        file_name (str): Local file path to save\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if download successful, False otherwise\n",
    "    \n",
    "    Example output:\n",
    "        hosts.csv: 78.5% | 9.42/12.00 MB | Speed: 3.45 MB/s | ETA: 1s\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "        \n",
    "        # Get file size from S3\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=object_name)\n",
    "        filesize = response['ContentLength']\n",
    "        \n",
    "        # Create progress tracker\n",
    "        progress = ProgressTracker(os.path.basename(file_name), filesize)\n",
    "        \n",
    "        # Download with progress callback\n",
    "        s3_client.download_file(\n",
    "            bucket, \n",
    "            object_name, \n",
    "            file_name,\n",
    "            Callback=progress\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: '{bucket}/{object_name}' downloaded to '{file_name}'\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to download file - {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "download_file_with_progress(BUCKET_NAME, 'data/hosts.csv', './downloads/hosts-progress-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03757472",
   "metadata": {},
   "source": [
    "### Progress Tracking Demo\n",
    "\n",
    "Test the progress tracking with a file upload and download.\n",
    "\n",
    "**Note:** Progress updates work best with larger files (several MB or more). For very small files, the transfer may complete too quickly to see detailed progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f7502f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing upload with progress tracking:\n",
      "hosts.csv: 100.0% | 0.01/0.01 MB | Speed: 5814.67 MB/s | ETA: 0s\n",
      "SUCCESS: 'data/hosts.csv' uploaded to 'real-learn-s3/data/hosts-progress-demo.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test upload with progress tracking\n",
    "print(\"Testing upload with progress tracking:\")\n",
    "upload_file_with_progress('data/hosts.csv', BUCKET_NAME, 'data/hosts-progress-demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ef0e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing download with progress tracking:\n",
      "hosts-progress-demo.csv: 100.0% | 0.01/0.01 MB | Speed: 4025.54 MB/s | ETA: 0s\n",
      "SUCCESS: 'real-learn-s3/data/hosts.csv' downloaded to './downloads/hosts-progress-demo.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test download with progress tracking\n",
    "print(\"Testing download with progress tracking:\")\n",
    "download_file_with_progress(BUCKET_NAME, 'data/hosts.csv', './downloads/hosts-progress-demo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fd772",
   "metadata": {},
   "source": [
    "## Summary - What We've Learned\n",
    "\n",
    "This notebook covered comprehensive S3 operations in a progressive, hands-on approach:\n",
    "\n",
    "### Phase 1: Basic Operations\n",
    "- **create_bucket()** - Create S3 buckets with region awareness\n",
    "- **upload_file()** - Upload files from local filesystem\n",
    "- **download_file()** - Download objects to local storage\n",
    "\n",
    "### Phase 2: Content Operations\n",
    "- **upload_string()** - Direct text upload without local files\n",
    "- **read_object()** - Read content directly into memory\n",
    "\n",
    "### Phase 3: Management Operations\n",
    "- **list_objects_detailed()** - Comprehensive metadata listing\n",
    "- **delete_object()** - Safe object deletion\n",
    "\n",
    "### Phase 4: Advanced Features\n",
    "**Part 1 - Metadata:**\n",
    "- **get_object_metadata()** - Retrieve system and user metadata\n",
    "- **upload_with_metadata()** - Attach custom key-value pairs\n",
    "\n",
    "**Part 2 - Presigned URLs:**\n",
    "- **generate_presigned_download_url()** - Temporary download URLs\n",
    "- **generate_presigned_upload_url()** - Temporary upload URLs\n",
    "\n",
    "**Part 3 - Progress Tracking:**\n",
    "- **ProgressTracker** class - Real-time transfer monitoring\n",
    "- **upload_file_with_progress()** - Upload with progress feedback\n",
    "- **download_file_with_progress()** - Download with progress feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1196c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
