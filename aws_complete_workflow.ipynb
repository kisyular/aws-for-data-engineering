{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb1445a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "699b71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "GLUE_ROLE_ARN = os.getenv(\"GLUE_ROLE_ARN\")\n",
    "LAMBDA_ROLE_ARN = os.getenv(\"LAMBDA_ROLE_ARN\")\n",
    "S3_BUCKET_COMPLETE_PIPELINE = os.getenv(\"S3_BUCKET_COMPLETE_PIPELINE\")\n",
    "S3_UPLOAD_FOLDER = \"raw/\"\n",
    "S3_PROCESSED_FOLDER = \"processed/\"\n",
    "S3_GLUE_OUTPUT_FOLDER = \"glue-output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca8f71",
   "metadata": {},
   "source": [
    "# COMPLETE DATA LAKE WORKFLOW\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                        COMPLETE DATA LAKE PIPELINE                                |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                                                                                   |\n",
    "|  1. INGEST                                                                        |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Upload to S3      |  <-- Manual, Lambda, Kinesis Firehose                   |\n",
    "|     | (raw/ folder)     |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  2. CATALOG                                                                       |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Glue Crawler      |  --> Discovers schema, creates tables                   |\n",
    "|     | (Data Catalog)    |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  3. TRANSFORM                                                                     |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Glue ETL Job      |  --> Clean, transform, convert to Parquet               |\n",
    "|     | (processed/)      |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  4. CATALOG PROCESSED                                                             |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Another Crawler   |  --> Update catalog with processed tables               |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  5. ANALYZE                                                                       |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Athena SQL        |  --> Fast, serverless analytics                         |\n",
    "|     | Queries           |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  6. VISUALIZE (Optional)                                                          |\n",
    "|     +-------------------+                                                         |\n",
    "|     | QuickSight        |  --> Dashboards, reports                                |\n",
    "|     +-------------------+                                                         |\n",
    "|                                                                                   |\n",
    "|  All serverless, managed, pay-per-use!                                            |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e327485",
   "metadata": {},
   "source": [
    "# PHASE 1: DATA INGESTION\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 1: INGEST DATA TO S3                                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   Local Files ──────┐                                                       │\n",
    "│                     │                                                       │\n",
    "│   APIs/Streams ─────┼────►  S3 Bucket  ────►  raw/ folder                   │\n",
    "│                     │      (Landing Zone)                                   │\n",
    "│   Lambda Events ────┘                                                       │\n",
    "│                                                                             │\n",
    "│   Key Operations:                                                           │\n",
    "│   • Create S3 bucket (landing zone)                                         │\n",
    "│   • Upload raw data files                                                   │\n",
    "│   • Verify uploads & read content                                           │\n",
    "│   • Generate presigned URLs for sharing                                     │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 1.1 Initialize S3 Client\n",
    "\n",
    "Configure the S3 client with **Signature Version 4** (required for presigned URLs) and regional endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f414ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an s3 client\n",
    "from botocore.config import Config\n",
    "\n",
    "# Configure S3 client with Signature Version 4 and regional endpoint\n",
    "# Regional endpoint is required for presigned URLs to work correctly\n",
    "\n",
    "s3_client = boto3.client('s3', \n",
    "                         endpoint_url=f'https://s3.{AWS_REGION}.amazonaws.com',\n",
    "                         config=Config(signature_version='s3v4'),\n",
    "                         region_name=AWS_REGION,\n",
    "                         aws_secret_access_key=SECRET_KEY,\n",
    "                         aws_access_key_id=ACCESS_KEY,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05809552",
   "metadata": {},
   "source": [
    "## 1.2 Create S3 Bucket (Landing Zone)\n",
    "\n",
    "The bucket serves as the **landing zone** for all raw data. All incoming files go to the `raw/` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "259c235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Bucket 's3-complete-pipeline' already exists and is owned by you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in a specified region\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): Name for the bucket (must be globally unique)\n",
    "        region (str): AWS region (if None, uses default from client)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if bucket created, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region is None or region == 'us-east-1':\n",
    "            # us-east-1 doesn't require LocationConstraint\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            # Other regions require LocationConstraint\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"SUCCESS: Bucket '{bucket_name}' created successfully\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'BucketAlreadyExists':\n",
    "            print(f\"ERROR: Bucket '{bucket_name}' already exists (owned by someone else)\")\n",
    "        elif error_code == 'BucketAlreadyOwnedByYou':\n",
    "            print(f\"INFO: Bucket '{bucket_name}' already exists and is owned by you\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create bucket - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "create_bucket(S3_BUCKET_COMPLETE_PIPELINE, region=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8bbb1",
   "metadata": {},
   "source": [
    "## 1.3 Upload Raw Data\n",
    "\n",
    "Upload files to the `raw/` folder. This is where Glue Crawlers will discover and catalog the data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff822e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/hosts.csv' uploaded to 's3-complete-pipeline/raw/hosts.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload a file to s3\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    # If S3 object_name not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_file('data/hosts.csv', S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699d30b",
   "metadata": {},
   "source": [
    "## 1.4 Verify Upload (Read Content)\n",
    "\n",
    "Read the uploaded file directly from S3 to confirm the data landed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40690886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Read 13083 characters from 's3-complete-pipeline/raw/hosts.csv'\n",
      "Content:\n",
      "host_id,host_name,host_since,is_superhost,response_rate,created_at\n",
      "1,Timothy Parker,2018-03-20,False,99,2025-12-26 14:15:54.011160\n",
      "2,Hannah Evans,2024-01-01,False,95,2025-12-26 14:15:54.011160\n",
      "3,Crystal Green,2016-08-06,False,74,2025-12-26 14:15:54.011160\n",
      "4,Kevin Johnson,2020-02-25,False,100,2025-12-26 14:15:54.011160\n",
      "5,Monica Johnson,2024-11-11,False,77,2025-12-26 14:15:54.011160\n",
      "6,Nancy Turner,2016-11-03,False,96,2025-12-26 14:15:54.011160\n",
      "7,Gerald Hunt,2022-01-21,True,99,2025-12-26 14:\n"
     ]
    }
   ],
   "source": [
    "def read_object(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Read S3 object content directly into memory\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to read\n",
    "    \n",
    "    Returns:\n",
    "        str: File content as string, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=object_name)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        print(f\"SUCCESS: Read {len(content)} characters from '{bucket}/{object_name}'\")\n",
    "        return content\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'NoSuchKey':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to read object - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "content = read_object(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')\n",
    "if content:\n",
    "    print(\"Content:\")\n",
    "    print(content[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d883c",
   "metadata": {},
   "source": [
    "## 1.5 List Objects in Bucket\n",
    "\n",
    "View all objects in the `raw/` folder with detailed metadata (size, last modified, storage class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5fca925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 's3-complete-pipeline/raw/':\n",
      "Key: raw/hosts.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-31 03:02:32\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Total: 1 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'raw/hosts.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 31, 3, 2, 32, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def list_objects_detailed(bucket, prefix=''):\n",
    "    \"\"\"\n",
    "    List all objects in a bucket with detailed metadata\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        prefix (str): Filter objects by prefix (folder path)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of object metadata dictionaries, or empty list if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(f\"No objects found in bucket '{bucket}' with prefix '{prefix}'\")\n",
    "            return []\n",
    "        \n",
    "        objects = []\n",
    "        print(f\"Objects in '{bucket}/{prefix}':\")\n",
    "        \n",
    "        for obj in response['Contents']:\n",
    "            # Convert size to human-readable format\n",
    "            size_bytes = obj['Size']\n",
    "            if size_bytes < 1024:\n",
    "                size_str = f\"{size_bytes} B\"\n",
    "            elif size_bytes < 1024**2:\n",
    "                size_str = f\"{size_bytes/1024:.2f} KB\"\n",
    "            else:\n",
    "                size_str = f\"{size_bytes/(1024**2):.2f} MB\"\n",
    "            \n",
    "            # Format last modified date\n",
    "            last_modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            print(f\"Key: {obj['Key']}\")\n",
    "            print(f\"Size: {size_str} ({size_bytes:,} bytes)\")\n",
    "            print(f\"Last Modified: {last_modified}\")\n",
    "            print(f\"Storage Class: {obj.get('StorageClass', 'STANDARD')}\")\n",
    "            print(f\"ETag: {obj['ETag']}\")\n",
    "            \n",
    "            objects.append(obj)\n",
    "        \n",
    "        print(f\"Total: {len(objects)} object(s)\")\n",
    "        return objects\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to list objects - {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "list_objects_detailed(S3_BUCKET_COMPLETE_PIPELINE, prefix=f'{S3_UPLOAD_FOLDER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1895a2",
   "metadata": {},
   "source": [
    "## 1.6 Get Object Metadata\n",
    "\n",
    "Retrieve detailed metadata for a specific object (content type, encryption, custom tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b907126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 's3-complete-pipeline/raw/hosts.csv':\n",
      "SYSTEM METADATA:\n",
      "Content-Type: binary/octet-stream\n",
      "Content-Length: 13,083 bytes\n",
      "Last-Modified: 2026-01-31 03:02:32+00:00\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Storage-Class: STANDARD\n",
      "USER METADATA: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'W1QD945F3JFZGWF0',\n",
       "  'HostId': 'JDY8bqiBgPQQdc9mhRexUjWGk+L4itRZrVyrD+77LMFXx7seuBMPzgEeN6dAuj4WDzNYLB4zhws=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'JDY8bqiBgPQQdc9mhRexUjWGk+L4itRZrVyrD+77LMFXx7seuBMPzgEeN6dAuj4WDzNYLB4zhws=',\n",
       "   'x-amz-request-id': 'W1QD945F3JFZGWF0',\n",
       "   'date': 'Sat, 31 Jan 2026 03:03:04 GMT',\n",
       "   'last-modified': 'Sat, 31 Jan 2026 03:02:32 GMT',\n",
       "   'etag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'content-length': '13083',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2026, 1, 31, 3, 2, 32, tzinfo=tzutc()),\n",
       " 'ContentLength': 13083,\n",
       " 'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_object_metadata(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Retrieve metadata for an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata dictionary, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=object_name)\n",
    "        \n",
    "        print(f\"Metadata for '{bucket}/{object_name}':\")\n",
    "        \n",
    "        # System metadata\n",
    "        print(\"SYSTEM METADATA:\")\n",
    "        print(f\"Content-Type: {response.get('ContentType', 'N/A')}\")\n",
    "        print(f\"Content-Length: {response.get('ContentLength', 0):,} bytes\")\n",
    "        print(f\"Last-Modified: {response.get('LastModified', 'N/A')}\")\n",
    "        print(f\"ETag: {response.get('ETag', 'N/A')}\")\n",
    "        print(f\"Storage-Class: {response.get('StorageClass', 'STANDARD')}\")\n",
    "        \n",
    "        # User metadata (custom)\n",
    "        user_metadata = response.get('Metadata', {})\n",
    "        if user_metadata:\n",
    "            print(\"USER METADATA (Custom):\")\n",
    "            for key, value in user_metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(\"USER METADATA: None\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to get metadata - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "get_object_metadata(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89bf53",
   "metadata": {},
   "source": [
    "## 1.7 Generate Presigned URL (Share Data)\n",
    "\n",
    "Create time-limited URLs for secure sharing without exposing AWS credentials.\n",
    "\n",
    "| Expiration | Use Case |\n",
    "|------------|----------|\n",
    "| 600s (10 min) | Quick one-time downloads |\n",
    "| 3600s (1 hour) | Team collaboration |\n",
    "| 86400s (24 hours) | External sharing |\n",
    "| 604800s (7 days) | Maximum allowed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9acbab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Presigned URL generated for 's3-complete-pipeline/raw/hosts.csv'\n",
      "Expires in: 600 seconds (0.2 hours)\n",
      "\n",
      "URL (valid for 0.2 hours):\n",
      "https://s3.us-east-2.amazonaws.com/s3-complete-pipeline/raw/hosts.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAWPVGU3OO6BQJAVE3%2F20260131%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20260131T030321Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Signature=32c0c87db9fa04e5c387a2c5e06e1ff6d693ac6fa3b0b96553197a457e9e52ae\n",
      "\n",
      "Anyone with this URL can download the file until it expires.\n"
     ]
    }
   ],
   "source": [
    "def generate_presigned_download_url(bucket, object_name, expiration=3600):\n",
    "    \"\"\"\n",
    "    Generate a presigned URL for downloading an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "        expiration (int): URL expiration time in seconds (default 3600 = 1 hour)\n",
    "    \n",
    "    Returns:\n",
    "        str: Presigned URL, or None if error\n",
    "    \n",
    "    Common expiration times:\n",
    "        - 3600 = 1 hour (default)\n",
    "        - 7200 = 2 hours\n",
    "        - 86400 = 24 hours\n",
    "        - 604800 = 7 days (maximum)\n",
    "    \n",
    "    Note: Uses AWS Signature Version 4 (required by S3)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = s3_client.generate_presigned_url(\n",
    "            'get_object',\n",
    "            Params={\n",
    "                'Bucket': bucket,\n",
    "                'Key': object_name\n",
    "            },\n",
    "            ExpiresIn=expiration\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Presigned URL generated for '{bucket}/{object_name}'\")\n",
    "        print(f\"Expires in: {expiration} seconds ({expiration/3600:.1f} hours)\")\n",
    "        print(f\"\\nURL (valid for {expiration/3600:.1f} hours):\")\n",
    "        print(url)\n",
    "        print(\"\\nAnyone with this URL can download the file until it expires.\")\n",
    "        \n",
    "        return url\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to generate presigned URL - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "url = generate_presigned_download_url(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv', expiration=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06a066",
   "metadata": {},
   "source": [
    "# PHASE 2: DATA CATALOG\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 2: CATALOG DATA WITH GLUE CRAWLER                                    │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   S3 (raw/)  ────►  Glue Crawler  ────►  Data Catalog                       │\n",
    "│                     (auto-schema)        (database + tables)                │\n",
    "│                                                                             │\n",
    "│   Key Operations:                                                           │\n",
    "│   • Create Glue database (metadata container)                               │\n",
    "│   • Create & run crawler (discovers schema from S3)                         │\n",
    "│   • List tables (verify catalog entries)                                    │\n",
    "│   • Query-ready tables for Athena/ETL                                       │\n",
    "│                                                                             │\n",
    "│   Crawler detects:                                                          │\n",
    "│   • File format (CSV, JSON, Parquet, etc.)                                  │\n",
    "│   • Column names and data types                                             │\n",
    "│   • Partition structure                                                     │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 2.1 Initialize Glue Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc1ecb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Glue Client\n",
    "glue_client = boto3.client(\n",
    "    'glue',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "# Helper function for security\n",
    "def redact_account_id(arn):\n",
    "    \"\"\"Redact AWS account ID from ARN\"\"\"\n",
    "    return re.sub(r':\\d{12}:', ':************:', str(arn))\n",
    "\n",
    "print(\"Glue client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mp7xulz552i",
   "metadata": {},
   "source": [
    "## 2.2 Create Glue Database\n",
    "\n",
    "The database is a **logical container** for tables in the Data Catalog. Tables discovered by crawlers are stored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d46e359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Glue database 'aws_full_pipeline_db'...\n",
      "Database 'aws_full_pipeline_db' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_database(database_name, description=''):\n",
    "    \"\"\"\n",
    "    Create a database in the AWS Glue Data Catalog\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Name (lowercase, no spaces)\n",
    "        description (str): Optional description\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if created successfully\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating Glue database '{database_name}'...\")\n",
    "        \n",
    "        glue_client.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': database_name,\n",
    "                'Description': description\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Database '{database_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Database '{database_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test\n",
    "create_glue_database('aws_full_pipeline_db', 'AWS Glue database for complete pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jo41tmysnlf",
   "metadata": {},
   "source": [
    "## 2.3 List Databases\n",
    "\n",
    "View all databases in the Glue Data Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75698ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing Glue databases...\n",
      "------------------------------------------------------------\n",
      "Found 4 database(s):\n",
      "\n",
      "Database: aws_full_pipeline_db\n",
      "  Description: AWS Glue database for complete pipeline\n",
      "\n",
      "Database: data_engineering_db\n",
      "  Description: Learning database\n",
      "\n",
      "Database: default\n",
      "  Description: Default Hive database\n",
      "\n",
      "Database: glue_db\n",
      "  Description: A glue databse\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aws_full_pipeline_db', 'data_engineering_db', 'default', 'glue_db']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_glue_databases():\n",
    "    \"\"\"\n",
    "    List all databases in the Glue Data Catalog\n",
    "    \n",
    "    Returns:\n",
    "        list: Database names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Listing Glue databases...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_databases()\n",
    "        databases = response.get('DatabaseList', [])\n",
    "        \n",
    "        if not databases:\n",
    "            print(\"No databases found\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(databases)} database(s):\\n\")\n",
    "        \n",
    "        for db in databases:\n",
    "            print(f\"Database: {db['Name']}\")\n",
    "            print(f\"  Description: {db.get('Description', 'N/A')}\")\n",
    "            print()\n",
    "        \n",
    "        return [db['Name'] for db in databases]\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "list_glue_databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8p4ifj8i9ne",
   "metadata": {},
   "source": [
    "## 2.4 Create Crawler\n",
    "\n",
    "Crawlers automatically discover schema from S3 data and populate the Data Catalog with table definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "103666ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating crawler 'full_pipeline_crawler'...\n",
      "Target: aws_full_pipeline_db\n",
      "Path: s3://s3-complete-pipeline/raw/\n",
      "Crawler 'full_pipeline_crawler' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_crawler(crawler_name, database_name, s3_path, description=''):\n",
    "    \"\"\"\n",
    "    Create a Glue crawler\n",
    "    \n",
    "    Args:\n",
    "        crawler_name (str): Crawler name\n",
    "        database_name (str): Target database\n",
    "        s3_path (str): S3 path to crawl (e.g., 's3://bucket/path/')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating crawler '{crawler_name}'...\")\n",
    "        print(f\"Target: {database_name}\")\n",
    "        print(f\"Path: {s3_path}\")\n",
    "        \n",
    "        glue_client.create_crawler(\n",
    "            Name=crawler_name,\n",
    "            Role=GLUE_ROLE_ARN,\n",
    "            DatabaseName=database_name,\n",
    "            Description=description,\n",
    "            Targets={'S3Targets': [{'Path': s3_path}]},\n",
    "            SchemaChangePolicy={\n",
    "                'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                'DeleteBehavior': 'LOG'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Crawler '{crawler_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Crawler '{crawler_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example\n",
    "create_glue_crawler('full_pipeline_crawler', 'aws_full_pipeline_db', f's3://{S3_BUCKET_COMPLETE_PIPELINE}/raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lf6gpj0778b",
   "metadata": {},
   "source": [
    "## 2.5 Run Crawler\n",
    "\n",
    "Execute the crawler to scan S3 and update the Data Catalog. Use `wait=True` to block until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f75c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawler 'full_pipeline_crawler'...\n",
      "Crawler started\n",
      "Waiting for completion...\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "Completed: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_crawler(crawler_name, wait=False):\n",
    "    \"\"\"\n",
    "    Start a Glue crawler\n",
    "    \n",
    "    Args:\n",
    "        crawler_name (str): Crawler name\n",
    "        wait (bool): Wait for completion\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting crawler '{crawler_name}'...\")\n",
    "        glue_client.start_crawler(Name=crawler_name)\n",
    "        print(f\"Crawler started\")\n",
    "        \n",
    "        if wait:\n",
    "            print(\"Waiting for completion...\")\n",
    "            while True:\n",
    "                response = glue_client.get_crawler(Name=crawler_name)\n",
    "                state = response['Crawler']['State']\n",
    "                \n",
    "                if state == 'READY':\n",
    "                    # Wait for stats to update, then re-fetch\n",
    "                    time.sleep(2)\n",
    "                    response = glue_client.get_crawler(Name=crawler_name)\n",
    "                    last = response['Crawler'].get('LastCrawl', {})\n",
    "                    print(f\"Completed: {last.get('Status', 'Unknown')}\")\n",
    "                    break\n",
    "                print(f\"  State: {state}\")\n",
    "                time.sleep(10)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'CrawlerRunningException':\n",
    "            print(\"Crawler already running\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example\n",
    "run_crawler('full_pipeline_crawler', wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n7hs4i3onib",
   "metadata": {},
   "source": [
    "## 2.6 List Tables (Verify Catalog)\n",
    "\n",
    "**Critical step** - Verify the crawler created tables in the Data Catalog. These tables are now queryable by Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a057cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in database 'aws_full_pipeline_db':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Table: raw\n",
      "  Location: s3://s3-complete-pipeline/raw/\n",
      "  Format: csv\n",
      "  Columns (6):\n",
      "    - host_id: bigint\n",
      "    - host_name: string\n",
      "    - host_since: string\n",
      "    - is_superhost: boolean\n",
      "    - response_rate: bigint\n",
      "    ... and 1 more\n",
      "\n",
      "Total: 1 table(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'raw',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'host_id', 'Type': 'bigint'},\n",
       "    {'Name': 'host_name', 'Type': 'string'},\n",
       "    {'Name': 'host_since', 'Type': 'string'},\n",
       "    {'Name': 'is_superhost', 'Type': 'boolean'},\n",
       "    {'Name': 'response_rate', 'Type': 'bigint'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '13083',\n",
       "    'objectCount': '1',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '207',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '63',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': 'ca8177af-3451-41f6-8098-5213f431aa33',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '13083',\n",
       "   'objectCount': '1',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '207',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '63',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': 'ca8177af-3451-41f6-8098-5213f431aa33',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_tables(database_name):\n",
    "    \"\"\"\n",
    "    List all tables in a Glue database\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Database name\n",
    "    \n",
    "    Returns:\n",
    "        list: Table information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Tables in database '{database_name}':\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_tables(DatabaseName=database_name)\n",
    "        tables = response.get('TableList', [])\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables found\")\n",
    "            return []\n",
    "        \n",
    "        for table in tables:\n",
    "            print(f\"\\nTable: {table['Name']}\")\n",
    "            print(f\"  Location: {table.get('StorageDescriptor', {}).get('Location', 'N/A')}\")\n",
    "            print(f\"  Format: {table.get('Parameters', {}).get('classification', 'N/A')}\")\n",
    "            \n",
    "            # Show columns\n",
    "            columns = table.get('StorageDescriptor', {}).get('Columns', [])\n",
    "            if columns:\n",
    "                print(f\"  Columns ({len(columns)}):\")\n",
    "                for col in columns[:5]:  # Show first 5\n",
    "                    print(f\"    - {col['Name']}: {col['Type']}\")\n",
    "                if len(columns) > 5:\n",
    "                    print(f\"    ... and {len(columns) - 5} more\")\n",
    "        \n",
    "        print(f\"\\nTotal: {len(tables)} table(s)\")\n",
    "        return tables\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# Verify crawler created tables\n",
    "list_tables('aws_full_pipeline_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x0us7nb3rfd",
   "metadata": {},
   "source": [
    "## 2.7 List Crawlers\n",
    "\n",
    "View all crawlers in your account with their current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "s04fu1uel7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Crawlers:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Crawler: db_s3_crawler\n",
      "  State: READY\n",
      "  Database: glue_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: full_pipeline_crawler\n",
      "  State: READY\n",
      "  Database: aws_full_pipeline_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: my-crawler\n",
      "  State: READY\n",
      "  Database: data_engineering_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Total: 3 crawler(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'db_s3_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://real-learn-s3/processed/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'glue_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'DEPRECATE_IN_DATABASE'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 0, 15, 47, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 0, 15, 47, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:db_s3_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: ad4a76dd-3d4f-4471-9a73-0b8c40264d90; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'db_s3_crawler',\n",
       "   'MessagePrefix': '59e7981a-85f4-4423-b698-0f8c29c94bf5',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 0, 20, 59, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'Configuration': '{\"Version\":1.0,\"CreatePartitionIndex\":true}',\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'full_pipeline_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://s3-complete-pipeline/raw/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 22, 10, 54, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 22, 10, 54, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:full_pipeline_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: 80061fe3-4b76-42c1-b1b9-7d72942a18c3; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'full_pipeline_crawler',\n",
       "   'MessagePrefix': '29614048-7a38-42a5-b52e-7ce2ca4c2ec2',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 22, 17, 38, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'my-crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://real-learn-s3/raw/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'data_engineering_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 1, 25, 15, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 1, 25, 15, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:my-crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: 5c03d4eb-525f-4f08-b119-6309c565b76b; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'my-crawler',\n",
       "   'MessagePrefix': '6933cefa-b8c2-45d9-b00b-9a5f54546f3a',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 1, 36, 25, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_crawlers():\n",
    "    \"\"\"\n",
    "    List all Glue crawlers\n",
    "    \n",
    "    Returns:\n",
    "        list: Crawler names and states\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Glue Crawlers:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_crawlers()\n",
    "        crawlers = response.get('Crawlers', [])\n",
    "        \n",
    "        if not crawlers:\n",
    "            print(\"No crawlers found\")\n",
    "            return []\n",
    "        \n",
    "        for crawler in crawlers:\n",
    "            print(f\"\\nCrawler: {crawler['Name']}\")\n",
    "            print(f\"  State: {crawler['State']}\")\n",
    "            print(f\"  Database: {crawler.get('DatabaseName', 'N/A')}\")\n",
    "            \n",
    "            # Last crawl info\n",
    "            last = crawler.get('LastCrawl', {})\n",
    "            if last:\n",
    "                print(f\"  Last Run: {last.get('Status', 'N/A')}\")\n",
    "                print(f\"  Tables Created: {last.get('TablesCreated', 0)}\")\n",
    "                print(f\"  Tables Updated: {last.get('TablesUpdated', 0)}\")\n",
    "        \n",
    "        print(f\"\\nTotal: {len(crawlers)} crawler(s)\")\n",
    "        return crawlers\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# List all crawlers\n",
    "list_crawlers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w1segdt9fn",
   "metadata": {},
   "source": [
    "## 2.8 Cleanup Functions\n",
    "\n",
    "Delete crawlers and databases when no longer needed. **Use with caution!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7tpay1ca8mn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_crawler(crawler_name):\n",
    "    \"\"\"\n",
    "    Delete a Glue crawler\n",
    "    \n",
    "    Args:\n",
    "        crawler_name (str): Crawler name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting crawler '{crawler_name}'...\")\n",
    "        glue_client.delete_crawler(Name=crawler_name)\n",
    "        print(f\"SUCCESS: Crawler '{crawler_name}' deleted\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityNotFoundException':\n",
    "            print(f\"Crawler '{crawler_name}' not found\")\n",
    "        else:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def delete_database(database_name):\n",
    "    \"\"\"\n",
    "    Delete a Glue database and all its tables\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Database name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting database '{database_name}'...\")\n",
    "        glue_client.delete_database(Name=database_name)\n",
    "        print(f\"SUCCESS: Database '{database_name}' deleted\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityNotFoundException':\n",
    "            print(f\"Database '{database_name}' not found\")\n",
    "        else:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example (uncomment to use):\n",
    "# delete_crawler('full_pipeline_crawler')\n",
    "# delete_database('aws_full_pipeline_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4c360",
   "metadata": {},
   "source": [
    "# PHASE 3: DATA TRANSFORMATION (ETL)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 3: TRANSFORM DATA WITH GLUE ETL JOB                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   S3 (raw/)  ────►  Glue ETL Job  ────►  S3 (processed/)                    │\n",
    "│   (CSV)             (Spark/Python)       (Parquet)                          │\n",
    "│                                                                             │\n",
    "│   Key Operations:                                                           │\n",
    "│   • Upload ETL script to S3                                                 │\n",
    "│   • Create Glue job pointing to script                                      │\n",
    "│   • Run job with parameters                                                 │\n",
    "│   • Monitor job execution                                                   │\n",
    "│                                                                             │\n",
    "│   Transformations in aws_glue_etl.py:                                       │\n",
    "│   • Null value checking & rejection                                         │\n",
    "│   • Duplicate detection & handling                                          │\n",
    "│   • Calculated columns:                                                     │\n",
    "│     - total_booking_amount = nights_booked * booking_amount                 │\n",
    "│     - additional_cost = cleaning_fee + service_fee                          │\n",
    "│     - total_cost = total_booking_amount + additional_cost                   │\n",
    "│   • Output as Parquet (compressed, columnar)                                │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 3.1 Upload Raw Data (bookings.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "621396fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/bookings.csv' uploaded to 's3-complete-pipeline/raw/bookings.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_file('data/bookings.csv', S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}bookings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6h7i7ilhnx",
   "metadata": {},
   "source": [
    "## 3.2 Upload ETL Script to S3\n",
    "\n",
    "Glue jobs require the Python script to be stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbc8f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'scripts/aws_glue_etl.py' uploaded to 's3-complete-pipeline/scripts/aws_glue_etl.py'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload the ETL script to S3\n",
    "upload_file('scripts/aws_glue_etl.py', S3_BUCKET_COMPLETE_PIPELINE, 'scripts/aws_glue_etl.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9335b66",
   "metadata": {},
   "source": [
    "## 3.3 Create Glue Job\n",
    "\n",
    "Create an ETL job that points to the script in S3. Job types:\n",
    "\n",
    "| Type | Use Case | Workers |\n",
    "|------|----------|---------|\n",
    "| `glueetl` | Spark-based, large datasets | 2+ DPUs |\n",
    "| `pythonshell` | Simple Python, small data | 0.0625-1 DPU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a18ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Glue job 'bookings-etl-job'...\n",
      "  Script: s3://s3-complete-pipeline/scripts/aws_glue_etl.py\n",
      "  Type: glueetl\n",
      "SUCCESS: Job 'bookings-etl-job' created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_job(job_name, script_location, description='', job_type='glueetl', \n",
    "                    worker_type='G.1X', num_workers=2, timeout=60, max_retries=0):\n",
    "    \"\"\"\n",
    "    Create a Glue ETL job\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "        script_location (str): S3 path to ETL script (s3://bucket/path/script.py)\n",
    "        description (str): Job description\n",
    "        job_type (str): 'glueetl' (Spark) or 'pythonshell'\n",
    "        worker_type (str): 'G.1X', 'G.2X', 'G.025X' (for pythonshell)\n",
    "        num_workers (int): Number of workers (min 2 for glueetl)\n",
    "        timeout (int): Job timeout in minutes\n",
    "        max_retries (int): Number of retries on failure\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if created successfully\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating Glue job '{job_name}'...\")\n",
    "        print(f\"  Script: {script_location}\")\n",
    "        print(f\"  Type: {job_type}\")\n",
    "        \n",
    "        job_config = {\n",
    "            'Name': job_name,\n",
    "            'Description': description,\n",
    "            'Role': GLUE_ROLE_ARN,\n",
    "            'Command': {\n",
    "                'Name': job_type,\n",
    "                'ScriptLocation': script_location,\n",
    "                'PythonVersion': '3'\n",
    "            },\n",
    "            'DefaultArguments': {\n",
    "                '--job-language': 'python',\n",
    "                '--enable-metrics': 'true',\n",
    "                '--enable-continuous-cloudwatch-log': 'true'\n",
    "            },\n",
    "            'Timeout': timeout,\n",
    "            'MaxRetries': max_retries,\n",
    "            'GlueVersion': '4.0'\n",
    "        }\n",
    "        \n",
    "        # Add worker config for glueetl jobs\n",
    "        if job_type == 'glueetl':\n",
    "            job_config['WorkerType'] = worker_type\n",
    "            job_config['NumberOfWorkers'] = num_workers\n",
    "        \n",
    "        glue_client.create_job(**job_config)\n",
    "        \n",
    "        print(f\"SUCCESS: Job '{job_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Job '{job_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Create the bookings ETL job\n",
    "script_path = f's3://{S3_BUCKET_COMPLETE_PIPELINE}/scripts/aws_glue_etl.py'\n",
    "create_glue_job(\n",
    "    job_name='bookings-etl-job',\n",
    "    script_location=script_path,\n",
    "    description='Transform bookings data: null check, dedup, calculate totals'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y40wylhjxam",
   "metadata": {},
   "source": [
    "## 3.4 Run Glue Job\n",
    "\n",
    "Execute the job with parameters. The job reads from `raw/bookings.csv` and writes to `processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1df09c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 'bookings-etl-job'...\n",
      "  Arguments: {'--S3_BUCKET': 's3-complete-pipeline', '--SOURCE_PREFIX': 'raw', '--TARGET_PREFIX': 'processed', '--DUPLICATE_HANDLING': 'keep_first'}\n",
      "Job started. Run ID: jr_96843101dee5f3eecd5d11029c418eea5ebb7b886849f9024d76c2ca5a312d2b\n",
      "Waiting for completion...\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "Job SUCCEEDED\n",
      "Duration: 74 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jr_96843101dee5f3eecd5d11029c418eea5ebb7b886849f9024d76c2ca5a312d2b'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_glue_job(job_name, arguments=None, wait=False):\n",
    "    \"\"\"\n",
    "    Start a Glue job run\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "        arguments (dict): Job arguments (e.g., {'--S3_BUCKET': 'my-bucket'})\n",
    "        wait (bool): Wait for completion\n",
    "    \n",
    "    Returns:\n",
    "        str: Job run ID, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting job '{job_name}'...\")\n",
    "        \n",
    "        run_config = {'JobName': job_name}\n",
    "        if arguments:\n",
    "            run_config['Arguments'] = arguments\n",
    "            print(f\"  Arguments: {arguments}\")\n",
    "        \n",
    "        response = glue_client.start_job_run(**run_config)\n",
    "        run_id = response['JobRunId']\n",
    "        \n",
    "        print(f\"Job started. Run ID: {run_id}\")\n",
    "        \n",
    "        if wait:\n",
    "            print(\"Waiting for completion...\")\n",
    "            while True:\n",
    "                status_response = glue_client.get_job_run(JobName=job_name, RunId=run_id)\n",
    "                state = status_response['JobRun']['JobRunState']\n",
    "                \n",
    "                if state in ['SUCCEEDED', 'FAILED', 'STOPPED', 'TIMEOUT']:\n",
    "                    print(f\"Job {state}\")\n",
    "                    if state == 'FAILED':\n",
    "                        error = status_response['JobRun'].get('ErrorMessage', 'Unknown')\n",
    "                        print(f\"Error: {error}\")\n",
    "                    elif state == 'SUCCEEDED':\n",
    "                        duration = status_response['JobRun'].get('ExecutionTime', 0)\n",
    "                        print(f\"Duration: {duration} seconds\")\n",
    "                    break\n",
    "                    \n",
    "                print(f\"  State: {state}\")\n",
    "                time.sleep(30)\n",
    "        \n",
    "        return run_id\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Run the bookings ETL job\n",
    "run_glue_job(\n",
    "    job_name='bookings-etl-job',\n",
    "    arguments={\n",
    "        '--S3_BUCKET': S3_BUCKET_COMPLETE_PIPELINE,\n",
    "        '--SOURCE_PREFIX': 'raw',\n",
    "        '--TARGET_PREFIX': 'processed',\n",
    "        '--DUPLICATE_HANDLING': 'keep_first'\n",
    "    },\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fkb3ohfljw",
   "metadata": {},
   "source": [
    "## 3.5 Get Job Run Status\n",
    "\n",
    "Check the status of a specific job run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ykodtehg1qo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Run Status for 'bookings-etl-job':\n",
      "--------------------------------------------------\n",
      "  Run ID: jr_96843101dee5f3eecd5d11029c418eea5ebb7b886849f9024d76c2ca5a312d2b\n",
      "  State: SUCCEEDED\n",
      "  Started: 2026-01-30 23:06:44.458000-05:00\n",
      "  Completed: 2026-01-30 23:08:05.023000-05:00\n",
      "  Duration: 74 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Id': 'jr_96843101dee5f3eecd5d11029c418eea5ebb7b886849f9024d76c2ca5a312d2b',\n",
       " 'Attempt': 0,\n",
       " 'JobName': 'bookings-etl-job',\n",
       " 'JobMode': 'SCRIPT',\n",
       " 'JobRunQueuingEnabled': False,\n",
       " 'StartedOn': datetime.datetime(2026, 1, 30, 23, 6, 44, 458000, tzinfo=tzlocal()),\n",
       " 'LastModifiedOn': datetime.datetime(2026, 1, 30, 23, 8, 5, 23000, tzinfo=tzlocal()),\n",
       " 'CompletedOn': datetime.datetime(2026, 1, 30, 23, 8, 5, 23000, tzinfo=tzlocal()),\n",
       " 'JobRunState': 'SUCCEEDED',\n",
       " 'Arguments': {'--TARGET_PREFIX': 'processed',\n",
       "  '--DUPLICATE_HANDLING': 'keep_first',\n",
       "  '--S3_BUCKET': 's3-complete-pipeline',\n",
       "  '--SOURCE_PREFIX': 'raw'},\n",
       " 'PredecessorRuns': [],\n",
       " 'AllocatedCapacity': 2,\n",
       " 'ExecutionTime': 74,\n",
       " 'Timeout': 60,\n",
       " 'MaxCapacity': 2.0,\n",
       " 'WorkerType': 'G.1X',\n",
       " 'NumberOfWorkers': 2,\n",
       " 'LogGroupName': '/aws-glue/jobs',\n",
       " 'GlueVersion': '4.0'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_job_run_status(job_name, run_id=None):\n",
    "    \"\"\"\n",
    "    Get status of a job run (latest if run_id not specified)\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "        run_id (str): Specific run ID (optional)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Job run details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if run_id:\n",
    "            response = glue_client.get_job_run(JobName=job_name, RunId=run_id)\n",
    "            runs = [response['JobRun']]\n",
    "        else:\n",
    "            response = glue_client.get_job_runs(JobName=job_name, MaxResults=1)\n",
    "            runs = response.get('JobRuns', [])\n",
    "        \n",
    "        if not runs:\n",
    "            print(f\"No runs found for job '{job_name}'\")\n",
    "            return None\n",
    "        \n",
    "        run = runs[0]\n",
    "        print(f\"Job Run Status for '{job_name}':\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"  Run ID: {run['Id']}\")\n",
    "        print(f\"  State: {run['JobRunState']}\")\n",
    "        print(f\"  Started: {run.get('StartedOn', 'N/A')}\")\n",
    "        print(f\"  Completed: {run.get('CompletedOn', 'N/A')}\")\n",
    "        print(f\"  Duration: {run.get('ExecutionTime', 0)} seconds\")\n",
    "        \n",
    "        if run['JobRunState'] == 'FAILED':\n",
    "            print(f\"  Error: {run.get('ErrorMessage', 'Unknown')}\")\n",
    "        \n",
    "        return run\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Check latest run status\n",
    "get_job_run_status('bookings-etl-job')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t7gt90dnpfm",
   "metadata": {},
   "source": [
    "## 3.6 List Jobs\n",
    "\n",
    "View all Glue jobs in the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eatai18key",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Jobs:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Job: aws-glue-pipeline\n",
      "  Description: \n",
      "  Type: glueetl\n",
      "  Glue Version: 5.0\n",
      "  Workers: 10\n",
      "  Timeout: 480 min\n",
      "\n",
      "Job: bookings-etl-job\n",
      "  Description: Transform bookings data: null check, dedup, calculate totals\n",
      "  Type: glueetl\n",
      "  Glue Version: 4.0\n",
      "  Workers: 2\n",
      "  Timeout: 60 min\n",
      "\n",
      "Job: my-etl-job\n",
      "  Description: Test ETL job\n",
      "  Type: pythonshell\n",
      "  Glue Version: 3.0\n",
      "  Workers: N/A\n",
      "  Timeout: 60 min\n",
      "\n",
      "Total: 3 job(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'aws-glue-pipeline',\n",
       "  'JobMode': 'VISUAL',\n",
       "  'JobRunQueuingEnabled': False,\n",
       "  'Description': '',\n",
       "  'Role': 'arn:aws:iam::445952351133:role/glue-access-s3',\n",
       "  'CreatedOn': datetime.datetime(2026, 1, 30, 0, 28, 24, 730000, tzinfo=tzlocal()),\n",
       "  'LastModifiedOn': datetime.datetime(2026, 1, 30, 0, 43, 37, 889000, tzinfo=tzlocal()),\n",
       "  'ExecutionProperty': {'MaxConcurrentRuns': 1},\n",
       "  'Command': {'Name': 'glueetl',\n",
       "   'ScriptLocation': 's3://aws-glue-assets-445952351133-us-east-2/scripts/aws-glue-pipeline.py',\n",
       "   'PythonVersion': '3'},\n",
       "  'DefaultArguments': {'--enable-metrics': 'true',\n",
       "   '--enable-spark-ui': 'true',\n",
       "   '--extra-py-files': 's3://aws-glue-studio-transforms-251189692203-prod-us-east-2/gs_common.py,s3://aws-glue-studio-transforms-251189692203-prod-us-east-2/gs_derived.py',\n",
       "   '--spark-event-logs-path': 's3://aws-glue-assets-445952351133-us-east-2/sparkHistoryLogs/',\n",
       "   '--enable-job-insights': 'true',\n",
       "   '--enable-observability-metrics': 'true',\n",
       "   '--conf': 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog',\n",
       "   '--enable-glue-datacatalog': 'true',\n",
       "   '--job-bookmark-option': 'job-bookmark-disable',\n",
       "   '--datalake-formats': 'delta',\n",
       "   '--job-language': 'python',\n",
       "   '--TempDir': 's3://aws-glue-assets-445952351133-us-east-2/temporary/'},\n",
       "  'MaxRetries': 0,\n",
       "  'AllocatedCapacity': 10,\n",
       "  'Timeout': 480,\n",
       "  'MaxCapacity': 10.0,\n",
       "  'WorkerType': 'G.1X',\n",
       "  'NumberOfWorkers': 10,\n",
       "  'GlueVersion': '5.0',\n",
       "  'ExecutionClass': 'STANDARD'},\n",
       " {'Name': 'bookings-etl-job',\n",
       "  'JobMode': 'SCRIPT',\n",
       "  'JobRunQueuingEnabled': False,\n",
       "  'Description': 'Transform bookings data: null check, dedup, calculate totals',\n",
       "  'Role': 'arn:aws:iam::445952351133:role/glue-access-s3',\n",
       "  'CreatedOn': datetime.datetime(2026, 1, 30, 22, 55, 43, 94000, tzinfo=tzlocal()),\n",
       "  'LastModifiedOn': datetime.datetime(2026, 1, 30, 22, 55, 43, 94000, tzinfo=tzlocal()),\n",
       "  'ExecutionProperty': {'MaxConcurrentRuns': 1},\n",
       "  'Command': {'Name': 'glueetl',\n",
       "   'ScriptLocation': 's3://s3-complete-pipeline/scripts/aws_glue_etl.py',\n",
       "   'PythonVersion': '3'},\n",
       "  'DefaultArguments': {'--enable-metrics': 'true',\n",
       "   '--job-language': 'python',\n",
       "   '--enable-continuous-cloudwatch-log': 'true'},\n",
       "  'MaxRetries': 0,\n",
       "  'AllocatedCapacity': 2,\n",
       "  'Timeout': 60,\n",
       "  'MaxCapacity': 2.0,\n",
       "  'WorkerType': 'G.1X',\n",
       "  'NumberOfWorkers': 2,\n",
       "  'GlueVersion': '4.0'},\n",
       " {'Name': 'my-etl-job',\n",
       "  'JobMode': 'SCRIPT',\n",
       "  'JobRunQueuingEnabled': False,\n",
       "  'Description': 'Test ETL job',\n",
       "  'Role': 'arn:aws:iam::445952351133:role/glue-access-s3',\n",
       "  'CreatedOn': datetime.datetime(2026, 1, 30, 1, 44, 15, 286000, tzinfo=tzlocal()),\n",
       "  'LastModifiedOn': datetime.datetime(2026, 1, 30, 1, 44, 15, 286000, tzinfo=tzlocal()),\n",
       "  'ExecutionProperty': {'MaxConcurrentRuns': 1},\n",
       "  'Command': {'Name': 'pythonshell',\n",
       "   'ScriptLocation': 's3://real-learn-s3/scripts/test_job.py',\n",
       "   'PythonVersion': '3.9'},\n",
       "  'DefaultArguments': {'--TempDir': 's3://real-learn-s3/glue-temp/',\n",
       "   '--job-language': 'python'},\n",
       "  'MaxRetries': 0,\n",
       "  'AllocatedCapacity': 0,\n",
       "  'Timeout': 60,\n",
       "  'MaxCapacity': 0.0625,\n",
       "  'GlueVersion': '3.0'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_jobs():\n",
    "    \"\"\"\n",
    "    List all Glue jobs\n",
    "    \n",
    "    Returns:\n",
    "        list: Job details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Glue Jobs:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_jobs()\n",
    "        jobs = response.get('Jobs', [])\n",
    "        \n",
    "        if not jobs:\n",
    "            print(\"No jobs found\")\n",
    "            return []\n",
    "        \n",
    "        for job in jobs:\n",
    "            print(f\"\\nJob: {job['Name']}\")\n",
    "            print(f\"  Description: {job.get('Description', 'N/A')}\")\n",
    "            print(f\"  Type: {job['Command']['Name']}\")\n",
    "            print(f\"  Glue Version: {job.get('GlueVersion', 'N/A')}\")\n",
    "            print(f\"  Workers: {job.get('NumberOfWorkers', 'N/A')}\")\n",
    "            print(f\"  Timeout: {job.get('Timeout', 'N/A')} min\")\n",
    "        \n",
    "        print(f\"\\nTotal: {len(jobs)} job(s)\")\n",
    "        return jobs\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# List all jobs\n",
    "list_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neevf3ydxg",
   "metadata": {},
   "source": [
    "## 3.7 Verify Output\n",
    "\n",
    "Check the processed folder to confirm Parquet files were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f13gv07tniq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 's3-complete-pipeline/processed/':\n",
      "Key: processed/bookings/part-00000-42d4a9f1-0e64-465d-8bdb-129f63975069-c000.snappy.parquet\n",
      "Size: 262.24 KB (268,529 bytes)\n",
      "Last Modified: 2026-01-31 04:07:54\n",
      "Storage Class: STANDARD\n",
      "ETag: \"99d0150fe63b8500442e47b32d495d0e-1\"\n",
      "Total: 1 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'processed/bookings/part-00000-42d4a9f1-0e64-465d-8bdb-129f63975069-c000.snappy.parquet',\n",
       "  'LastModified': datetime.datetime(2026, 1, 31, 4, 7, 54, tzinfo=tzutc()),\n",
       "  'ETag': '\"99d0150fe63b8500442e47b32d495d0e-1\"',\n",
       "  'ChecksumAlgorithm': ['CRC64NVME'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 268529,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify processed data was created\n",
    "list_objects_detailed(S3_BUCKET_COMPLETE_PIPELINE, prefix='processed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5nex43tuyxo",
   "metadata": {},
   "source": [
    "## 3.8 Cleanup Functions\n",
    "\n",
    "Delete jobs when no longer needed. **Use with caution!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sd0r7yuktgo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting job 'bookings-etl-job'...\n",
      "SUCCESS: Job 'bookings-etl-job' deleted\n",
      "Deleting job 'my-etl-job'...\n",
      "SUCCESS: Job 'my-etl-job' deleted\n",
      "Deleting job 'aws-glue-pipeline'...\n",
      "SUCCESS: Job 'aws-glue-pipeline' deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_job(job_name):\n",
    "    \"\"\"\n",
    "    Delete a Glue job\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting job '{job_name}'...\")\n",
    "        glue_client.delete_job(JobName=job_name)\n",
    "        print(f\"SUCCESS: Job '{job_name}' deleted\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityNotFoundException':\n",
    "            print(f\"Job '{job_name}' not found\")\n",
    "        else:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example (uncomment to use):\n",
    "delete_job('bookings-etl-job')\n",
    "delete_job('my-etl-job')\n",
    "delete_job('aws-glue-pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e419f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
