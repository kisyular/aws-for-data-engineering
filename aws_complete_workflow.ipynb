{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb1445a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699b71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "GLUE_ROLE_ARN = os.getenv(\"GLUE_ROLE_ARN\")\n",
    "LAMBDA_ROLE_ARN = os.getenv(\"LAMBDA_ROLE_ARN\")\n",
    "S3_BUCKET_COMPLETE_PIPELINE = os.getenv(\"S3_BUCKET_COMPLETE_PIPELINE\")\n",
    "S3_UPLOAD_FOLDER = \"raw/\"\n",
    "S3_PROCESSED_FOLDER = \"processed/\"\n",
    "S3_GLUE_OUTPUT_FOLDER = \"glue-output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca8f71",
   "metadata": {},
   "source": [
    "# COMPLETE DATA LAKE WORKFLOW\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                        COMPLETE DATA LAKE PIPELINE                                |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                                                                                   |\n",
    "|  1. INGEST                                                                        |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Upload to S3      |  <-- Manual, Lambda, Kinesis Firehose                   |\n",
    "|     | (raw/ folder)     |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  2. CATALOG                                                                       |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Glue Crawler      |  --> Discovers schema, creates tables                   |\n",
    "|     | (Data Catalog)    |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  3. TRANSFORM                                                                     |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Glue ETL Job      |  --> Clean, transform, convert to Parquet               |\n",
    "|     | (processed/)      |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  4. CATALOG PROCESSED                                                             |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Another Crawler   |  --> Update catalog with processed tables               |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  5. ANALYZE                                                                       |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Athena SQL        |  --> Fast, serverless analytics                         |\n",
    "|     | Queries           |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  6. VISUALIZE (Optional)                                                          |\n",
    "|     +-------------------+                                                         |\n",
    "|     | QuickSight        |  --> Dashboards, reports                                |\n",
    "|     +-------------------+                                                         |\n",
    "|                                                                                   |\n",
    "|  All serverless, managed, pay-per-use!                                            |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e327485",
   "metadata": {},
   "source": [
    "# PHASE 1: DATA INGESTION\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 1: INGEST DATA TO S3                                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   Local Files ──────┐                                                       │\n",
    "│                     │                                                       │\n",
    "│   APIs/Streams ─────┼────►  S3 Bucket  ────►  raw/ folder                   │\n",
    "│                     │      (Landing Zone)                                   │\n",
    "│   Lambda Events ────┘                                                       │\n",
    "│                                                                             │\n",
    "│   Key Operations:                                                           │\n",
    "│   • Create S3 bucket (landing zone)                                         │\n",
    "│   • Upload raw data files                                                   │\n",
    "│   • Verify uploads & read content                                           │\n",
    "│   • Generate presigned URLs for sharing                                     │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 1.1 Initialize S3 Client\n",
    "\n",
    "Configure the S3 client with **Signature Version 4** (required for presigned URLs) and regional endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f414ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an s3 client\n",
    "from botocore.config import Config\n",
    "\n",
    "# Configure S3 client with Signature Version 4 and regional endpoint\n",
    "# Regional endpoint is required for presigned URLs to work correctly\n",
    "\n",
    "s3_client = boto3.client('s3', \n",
    "                         endpoint_url=f'https://s3.{AWS_REGION}.amazonaws.com',\n",
    "                         config=Config(signature_version='s3v4'),\n",
    "                         region_name=AWS_REGION,\n",
    "                         aws_secret_access_key=SECRET_KEY,\n",
    "                         aws_access_key_id=ACCESS_KEY,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05809552",
   "metadata": {},
   "source": [
    "## 1.2 Create S3 Bucket (Landing Zone)\n",
    "\n",
    "The bucket serves as the **landing zone** for all raw data. All incoming files go to the `raw/` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "259c235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Bucket 's3-complete-pipeline' already exists and is owned by you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in a specified region\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): Name for the bucket (must be globally unique)\n",
    "        region (str): AWS region (if None, uses default from client)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if bucket created, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region is None or region == 'us-east-1':\n",
    "            # us-east-1 doesn't require LocationConstraint\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            # Other regions require LocationConstraint\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"SUCCESS: Bucket '{bucket_name}' created successfully\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'BucketAlreadyExists':\n",
    "            print(f\"ERROR: Bucket '{bucket_name}' already exists (owned by someone else)\")\n",
    "        elif error_code == 'BucketAlreadyOwnedByYou':\n",
    "            print(f\"INFO: Bucket '{bucket_name}' already exists and is owned by you\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create bucket - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "create_bucket(S3_BUCKET_COMPLETE_PIPELINE, region=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8bbb1",
   "metadata": {},
   "source": [
    "## 1.3 Upload Raw Data\n",
    "\n",
    "Upload files to the `raw/` folder. This is where Glue Crawlers will discover and catalog the data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff822e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/hosts.csv' uploaded to 's3-complete-pipeline/raw/hosts.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload a file to s3\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    # If S3 object_name not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_file('data/hosts.csv', S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699d30b",
   "metadata": {},
   "source": [
    "## 1.4 Verify Upload (Read Content)\n",
    "\n",
    "Read the uploaded file directly from S3 to confirm the data landed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40690886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Read 13083 characters from 's3-complete-pipeline/raw/hosts.csv'\n",
      "Content:\n",
      "host_id,host_name,host_since,is_superhost,response_rate,created_at\n",
      "1,Timothy Parker,2018-03-20,False,99,2025-12-26 14:15:54.011160\n",
      "2,Hannah Evans,2024-01-01,False,95,2025-12-26 14:15:54.011160\n",
      "3,Crystal Green,2016-08-06,False,74,2025-12-26 14:15:54.011160\n",
      "4,Kevin Johnson,2020-02-25,False,100,2025-12-26 14:15:54.011160\n",
      "5,Monica Johnson,2024-11-11,False,77,2025-12-26 14:15:54.011160\n",
      "6,Nancy Turner,2016-11-03,False,96,2025-12-26 14:15:54.011160\n",
      "7,Gerald Hunt,2022-01-21,True,99,2025-12-26 14:\n"
     ]
    }
   ],
   "source": [
    "def read_object(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Read S3 object content directly into memory\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to read\n",
    "    \n",
    "    Returns:\n",
    "        str: File content as string, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=object_name)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        print(f\"SUCCESS: Read {len(content)} characters from '{bucket}/{object_name}'\")\n",
    "        return content\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'NoSuchKey':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to read object - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "content = read_object(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')\n",
    "if content:\n",
    "    print(\"Content:\")\n",
    "    print(content[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d883c",
   "metadata": {},
   "source": [
    "## 1.5 List Objects in Bucket\n",
    "\n",
    "View all objects in the `raw/` folder with detailed metadata (size, last modified, storage class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5fca925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 's3-complete-pipeline/raw/':\n",
      "Key: raw/bookings.csv\n",
      "Size: 501.35 KB (513,378 bytes)\n",
      "Last Modified: 2026-01-31 03:36:02\n",
      "Storage Class: STANDARD\n",
      "ETag: \"203775ebda6b0e99de614895de78159f\"\n",
      "Key: raw/hosts.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-31 07:15:50\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Key: raw/lambda-bookings.csv\n",
      "Size: 1.90 KB (1,942 bytes)\n",
      "Last Modified: 2026-01-31 07:10:26\n",
      "Storage Class: STANDARD\n",
      "ETag: \"3a54a253c74c489770db51e83dda2644\"\n",
      "Total: 3 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'raw/bookings.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 31, 3, 36, 2, tzinfo=tzutc()),\n",
       "  'ETag': '\"203775ebda6b0e99de614895de78159f\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 513378,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'raw/hosts.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 31, 7, 15, 50, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'raw/lambda-bookings.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 31, 7, 10, 26, tzinfo=tzutc()),\n",
       "  'ETag': '\"3a54a253c74c489770db51e83dda2644\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 1942,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def list_objects_detailed(bucket, prefix=''):\n",
    "    \"\"\"\n",
    "    List all objects in a bucket with detailed metadata\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        prefix (str): Filter objects by prefix (folder path)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of object metadata dictionaries, or empty list if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(f\"No objects found in bucket '{bucket}' with prefix '{prefix}'\")\n",
    "            return []\n",
    "        \n",
    "        objects = []\n",
    "        print(f\"Objects in '{bucket}/{prefix}':\")\n",
    "        \n",
    "        for obj in response['Contents']:\n",
    "            # Convert size to human-readable format\n",
    "            size_bytes = obj['Size']\n",
    "            if size_bytes < 1024:\n",
    "                size_str = f\"{size_bytes} B\"\n",
    "            elif size_bytes < 1024**2:\n",
    "                size_str = f\"{size_bytes/1024:.2f} KB\"\n",
    "            else:\n",
    "                size_str = f\"{size_bytes/(1024**2):.2f} MB\"\n",
    "            \n",
    "            # Format last modified date\n",
    "            last_modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            print(f\"Key: {obj['Key']}\")\n",
    "            print(f\"Size: {size_str} ({size_bytes:,} bytes)\")\n",
    "            print(f\"Last Modified: {last_modified}\")\n",
    "            print(f\"Storage Class: {obj.get('StorageClass', 'STANDARD')}\")\n",
    "            print(f\"ETag: {obj['ETag']}\")\n",
    "            \n",
    "            objects.append(obj)\n",
    "        \n",
    "        print(f\"Total: {len(objects)} object(s)\")\n",
    "        return objects\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to list objects - {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "list_objects_detailed(S3_BUCKET_COMPLETE_PIPELINE, prefix=f'{S3_UPLOAD_FOLDER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1895a2",
   "metadata": {},
   "source": [
    "## 1.6 Get Object Metadata\n",
    "\n",
    "Retrieve detailed metadata for a specific object (content type, encryption, custom tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b907126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 's3-complete-pipeline/raw/hosts.csv':\n",
      "SYSTEM METADATA:\n",
      "Content-Type: binary/octet-stream\n",
      "Content-Length: 13,083 bytes\n",
      "Last-Modified: 2026-01-31 07:15:50+00:00\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Storage-Class: STANDARD\n",
      "USER METADATA: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4S4YF0RHWW8FD1PR',\n",
       "  'HostId': 'zEiS5uFU81eio2AjgOwq5BlTqKPYTE1GgDL4/UcVGburzuiNCad5w9k6JOxrPb6xFDvi/wJ0a3f8BGD55aLHv8BuJr2ZaUya',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'zEiS5uFU81eio2AjgOwq5BlTqKPYTE1GgDL4/UcVGburzuiNCad5w9k6JOxrPb6xFDvi/wJ0a3f8BGD55aLHv8BuJr2ZaUya',\n",
       "   'x-amz-request-id': '4S4YF0RHWW8FD1PR',\n",
       "   'date': 'Sat, 31 Jan 2026 07:15:55 GMT',\n",
       "   'last-modified': 'Sat, 31 Jan 2026 07:15:50 GMT',\n",
       "   'etag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'content-length': '13083',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2026, 1, 31, 7, 15, 50, tzinfo=tzutc()),\n",
       " 'ContentLength': 13083,\n",
       " 'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_object_metadata(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Retrieve metadata for an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata dictionary, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=object_name)\n",
    "        \n",
    "        print(f\"Metadata for '{bucket}/{object_name}':\")\n",
    "        \n",
    "        # System metadata\n",
    "        print(\"SYSTEM METADATA:\")\n",
    "        print(f\"Content-Type: {response.get('ContentType', 'N/A')}\")\n",
    "        print(f\"Content-Length: {response.get('ContentLength', 0):,} bytes\")\n",
    "        print(f\"Last-Modified: {response.get('LastModified', 'N/A')}\")\n",
    "        print(f\"ETag: {response.get('ETag', 'N/A')}\")\n",
    "        print(f\"Storage-Class: {response.get('StorageClass', 'STANDARD')}\")\n",
    "        \n",
    "        # User metadata (custom)\n",
    "        user_metadata = response.get('Metadata', {})\n",
    "        if user_metadata:\n",
    "            print(\"USER METADATA (Custom):\")\n",
    "            for key, value in user_metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(\"USER METADATA: None\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to get metadata - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "get_object_metadata(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89bf53",
   "metadata": {},
   "source": [
    "## 1.7 Generate Presigned URL (Share Data)\n",
    "\n",
    "Create time-limited URLs for secure sharing without exposing AWS credentials.\n",
    "\n",
    "| Expiration | Use Case |\n",
    "|------------|----------|\n",
    "| 600s (10 min) | Quick one-time downloads |\n",
    "| 3600s (1 hour) | Team collaboration |\n",
    "| 86400s (24 hours) | External sharing |\n",
    "| 604800s (7 days) | Maximum allowed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acbab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Presigned URL generated for 's3-complete-pipeline/raw/hosts.csv'\n",
      "Expires in: 600 seconds (0.2 hours)\n",
      "\n",
      "URL (valid for 0.2 hours):\n",
      "https://s3.us-east-2.amazonaws.com/s3-complete-pipeline/raw/hosts.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAWPVGU3OO6BQJAVE3%2F20260131%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20260131T071555Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Signature=0a5f86b0a6bc7daa56a1e6fdd6d75b4e6f55793654239777eca7dd78c3ba8a1d\n",
      "\n",
      "Anyone with this URL can download the file until it expires.\n"
     ]
    }
   ],
   "source": [
    "def generate_presigned_download_url(bucket, object_name, expiration=3600):\n",
    "    \"\"\"\n",
    "    Generate a presigned URL for downloading an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "        expiration (int): URL expiration time in seconds (default 3600 = 1 hour)\n",
    "    \n",
    "    Returns:\n",
    "        str: Presigned URL, or None if error\n",
    "    \n",
    "    Common expiration times:\n",
    "        - 3600 = 1 hour (default)\n",
    "        - 7200 = 2 hours\n",
    "        - 86400 = 24 hours\n",
    "        - 604800 = 7 days (maximum)\n",
    "    \n",
    "    Note: Uses AWS Signature Version 4 (required by S3)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = s3_client.generate_presigned_url(\n",
    "            'get_object',\n",
    "            Params={\n",
    "                'Bucket': bucket,\n",
    "                'Key': object_name\n",
    "            },\n",
    "            ExpiresIn=expiration\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Presigned URL generated for '{bucket}/{object_name}'\")\n",
    "        print(f\"Expires in: {expiration} seconds ({expiration/3600:.1f} hours)\")\n",
    "        print(f\"\\nURL (valid for {expiration/3600:.1f} hours):\")\n",
    "        print(url)\n",
    "        print(\"\\nAnyone with this URL can download the file until it expires.\")\n",
    "        \n",
    "        return url\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to generate presigned URL - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "url = generate_presigned_download_url(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv', expiration=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06a066",
   "metadata": {},
   "source": [
    "# PHASE 2: DATA CATALOG\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 2: CATALOG DATA WITH GLUE CRAWLER                                    │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   S3 (raw/)  ────►  Glue Crawler  ────►  Data Catalog                       │\n",
    "│                     (auto-schema)        (database + tables)                │\n",
    "│                                                                             │\n",
    "│   Key Operations:                                                           │\n",
    "│   • Create Glue database (metadata container)                               │\n",
    "│   • Create & run crawler (discovers schema from S3)                         │\n",
    "│   • List tables (verify catalog entries)                                    │\n",
    "│   • Query-ready tables for Athena/ETL                                       │\n",
    "│                                                                             │\n",
    "│   Crawler detects:                                                          │\n",
    "│   • File format (CSV, JSON, Parquet, etc.)                                  │\n",
    "│   • Column names and data types                                             │\n",
    "│   • Partition structure                                                     │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 2.1 Initialize Glue Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1ecb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Glue Client\n",
    "glue_client = boto3.client(\n",
    "    'glue',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "# Helper function for security\n",
    "def redact_account_id(arn):\n",
    "    \"\"\"Redact AWS account ID from ARN\"\"\"\n",
    "    return re.sub(r':\\d{12}:', ':************:', str(arn))\n",
    "\n",
    "print(\"Glue client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mp7xulz552i",
   "metadata": {},
   "source": [
    "## 2.2 Create Glue Database\n",
    "\n",
    "The database is a **logical container** for tables in the Data Catalog. Tables discovered by crawlers are stored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d46e359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Glue database 'aws_full_pipeline_db'...\n",
      "Database 'aws_full_pipeline_db' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_database(database_name, description=''):\n",
    "    \"\"\"\n",
    "    Create a database in the AWS Glue Data Catalog\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Name (lowercase, no spaces)\n",
    "        description (str): Optional description\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if created successfully\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating Glue database '{database_name}'...\")\n",
    "        \n",
    "        glue_client.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': database_name,\n",
    "                'Description': description\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Database '{database_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Database '{database_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test\n",
    "create_glue_database('aws_full_pipeline_db', 'AWS Glue database for complete pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jo41tmysnlf",
   "metadata": {},
   "source": [
    "## 2.3 List Databases\n",
    "\n",
    "View all databases in the Glue Data Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75698ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing Glue databases...\n",
      "------------------------------------------------------------\n",
      "Found 4 database(s):\n",
      "\n",
      "Database: aws_full_pipeline_db\n",
      "  Description: AWS Glue database for complete pipeline\n",
      "\n",
      "Database: data_engineering_db\n",
      "  Description: Learning database\n",
      "\n",
      "Database: default\n",
      "  Description: Default Hive database\n",
      "\n",
      "Database: glue_db\n",
      "  Description: A glue databse\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aws_full_pipeline_db', 'data_engineering_db', 'default', 'glue_db']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_glue_databases():\n",
    "    \"\"\"\n",
    "    List all databases in the Glue Data Catalog\n",
    "    \n",
    "    Returns:\n",
    "        list: Database names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Listing Glue databases...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_databases()\n",
    "        databases = response.get('DatabaseList', [])\n",
    "        \n",
    "        if not databases:\n",
    "            print(\"No databases found\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(databases)} database(s):\\n\")\n",
    "        \n",
    "        for db in databases:\n",
    "            print(f\"Database: {db['Name']}\")\n",
    "            print(f\"  Description: {db.get('Description', 'N/A')}\")\n",
    "            print()\n",
    "        \n",
    "        return [db['Name'] for db in databases]\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "list_glue_databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8p4ifj8i9ne",
   "metadata": {},
   "source": [
    "## 2.4 Create Crawler\n",
    "\n",
    "Crawlers automatically discover schema from S3 data and populate the Data Catalog with table definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103666ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating crawler 'full_pipeline_crawler'...\n",
      "Target: aws_full_pipeline_db\n",
      "Path: s3://s3-complete-pipeline/raw/\n",
      "Crawler 'full_pipeline_crawler' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_crawler(crawler_name, database_name, s3_path, description=''):\n",
    "    \"\"\"\n",
    "    Create a Glue crawler\n",
    "    \n",
    "    Args:\n",
    "        crawler_name (str): Crawler name\n",
    "        database_name (str): Target database\n",
    "        s3_path (str): S3 path to crawl (e.g., 's3://bucket/path/')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating crawler '{crawler_name}'...\")\n",
    "        print(f\"Target: {database_name}\")\n",
    "        print(f\"Path: {s3_path}\")\n",
    "        \n",
    "        glue_client.create_crawler(\n",
    "            Name=crawler_name,\n",
    "            Role=GLUE_ROLE_ARN,\n",
    "            DatabaseName=database_name,\n",
    "            Description=description,\n",
    "            Targets={'S3Targets': [{'Path': s3_path}]},\n",
    "            SchemaChangePolicy={\n",
    "                'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                'DeleteBehavior': 'LOG'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Crawler '{crawler_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Crawler '{crawler_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example\n",
    "create_glue_crawler('full_pipeline_crawler', 'aws_full_pipeline_db', f's3://{S3_BUCKET_COMPLETE_PIPELINE}/raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lf6gpj0778b",
   "metadata": {},
   "source": [
    "## 2.5 Run Crawler\n",
    "\n",
    "Execute the crawler to scan S3 and update the Data Catalog. Use `wait=True` to block until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f75c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawler 'full_pipeline_crawler'...\n",
      "Crawler started\n",
      "Waiting for completion...\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "Completed: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_crawler(crawler_name, wait=False):\n",
    "    \"\"\"\n",
    "    Start a Glue crawler\n",
    "    \n",
    "    Args:\n",
    "        crawler_name (str): Crawler name\n",
    "        wait (bool): Wait for completion\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting crawler '{crawler_name}'...\")\n",
    "        glue_client.start_crawler(Name=crawler_name)\n",
    "        print(f\"Crawler started\")\n",
    "        \n",
    "        if wait:\n",
    "            print(\"Waiting for completion...\")\n",
    "            while True:\n",
    "                response = glue_client.get_crawler(Name=crawler_name)\n",
    "                state = response['Crawler']['State']\n",
    "                \n",
    "                if state == 'READY':\n",
    "                    # Wait for stats to update, then re-fetch\n",
    "                    time.sleep(2)\n",
    "                    response = glue_client.get_crawler(Name=crawler_name)\n",
    "                    last = response['Crawler'].get('LastCrawl', {})\n",
    "                    print(f\"Completed: {last.get('Status', 'Unknown')}\")\n",
    "                    break\n",
    "                print(f\"  State: {state}\")\n",
    "                time.sleep(10)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'CrawlerRunningException':\n",
    "            print(\"Crawler already running\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example\n",
    "run_crawler('full_pipeline_crawler', wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n7hs4i3onib",
   "metadata": {},
   "source": [
    "## 2.6 List Tables (Verify Catalog)\n",
    "\n",
    "**Critical step** - Verify the crawler created tables in the Data Catalog. These tables are now queryable by Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a057cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in database 'aws_full_pipeline_db':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Table: bookings_csv\n",
      "  Location: s3://s3-complete-pipeline/raw/bookings.csv\n",
      "  Format: csv\n",
      "  Columns (9):\n",
      "    - booking_id: string\n",
      "    - listing_id: bigint\n",
      "    - booking_date: string\n",
      "    - nights_booked: bigint\n",
      "    - booking_amount: bigint\n",
      "    ... and 4 more\n",
      "\n",
      "Table: hosts_csv\n",
      "  Location: s3://s3-complete-pipeline/raw/hosts.csv\n",
      "  Format: csv\n",
      "  Columns (6):\n",
      "    - host_id: bigint\n",
      "    - host_name: string\n",
      "    - host_since: string\n",
      "    - is_superhost: boolean\n",
      "    - response_rate: bigint\n",
      "    ... and 1 more\n",
      "\n",
      "Table: lambda_bookings_csv\n",
      "  Location: s3://s3-complete-pipeline/raw/lambda-bookings.csv\n",
      "  Format: csv\n",
      "  Columns (9):\n",
      "    - booking_id: string\n",
      "    - listing_id: bigint\n",
      "    - booking_date: string\n",
      "    - nights_booked: bigint\n",
      "    - booking_amount: bigint\n",
      "    ... and 4 more\n",
      "\n",
      "Table: processed\n",
      "  Location: s3://s3-complete-pipeline/processed/\n",
      "  Format: parquet\n",
      "  Columns (13):\n",
      "    - booking_id: string\n",
      "    - listing_id: int\n",
      "    - booking_date: timestamp\n",
      "    - nights_booked: int\n",
      "    - booking_amount: int\n",
      "    ... and 8 more\n",
      "\n",
      "Table: raw\n",
      "  Location: s3://s3-complete-pipeline/raw/\n",
      "  Format: csv\n",
      "  Columns (6):\n",
      "    - host_id: bigint\n",
      "    - host_name: string\n",
      "    - host_since: string\n",
      "    - is_superhost: boolean\n",
      "    - response_rate: bigint\n",
      "    ... and 1 more\n",
      "\n",
      "Total: 5 table(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'bookings_csv',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 23, 27, 3, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 23, 27, 3, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 23, 27, 3, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'booking_id', 'Type': 'string'},\n",
       "    {'Name': 'listing_id', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_date', 'Type': 'string'},\n",
       "    {'Name': 'nights_booked', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_amount', 'Type': 'bigint'},\n",
       "    {'Name': 'cleaning_fee', 'Type': 'bigint'},\n",
       "    {'Name': 'service_fee', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_status', 'Type': 'string'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/bookings.csv',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '513378',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '4936',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '104',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': '8f513bb5-937e-40ce-80e1-537505e4899b',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '513378',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '4936',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '104',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': '8f513bb5-937e-40ce-80e1-537505e4899b',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False},\n",
       " {'Name': 'hosts_csv',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 23, 27, 2, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 23, 27, 2, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 23, 27, 2, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'host_id', 'Type': 'bigint'},\n",
       "    {'Name': 'host_name', 'Type': 'string'},\n",
       "    {'Name': 'host_since', 'Type': 'string'},\n",
       "    {'Name': 'is_superhost', 'Type': 'boolean'},\n",
       "    {'Name': 'response_rate', 'Type': 'bigint'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/hosts.csv',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '13083',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '207',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '63',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': '8f513bb5-937e-40ce-80e1-537505e4899b',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '13083',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '207',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '63',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': '8f513bb5-937e-40ce-80e1-537505e4899b',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False},\n",
       " {'Name': 'lambda_bookings_csv',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 31, 2, 16, 52, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 31, 2, 16, 52, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 31, 2, 16, 52, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'booking_id', 'Type': 'string'},\n",
       "    {'Name': 'listing_id', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_date', 'Type': 'string'},\n",
       "    {'Name': 'nights_booked', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_amount', 'Type': 'bigint'},\n",
       "    {'Name': 'cleaning_fee', 'Type': 'bigint'},\n",
       "    {'Name': 'service_fee', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_status', 'Type': 'string'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/lambda-bookings.csv',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '1942',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '18',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '104',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': 'd6c19c28-e9c7-4c1d-9a73-da604df486eb',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '1942',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '18',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '104',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': 'd6c19c28-e9c7-4c1d-9a73-da604df486eb',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False},\n",
       " {'Name': 'processed',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 23, 29, 13, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 23, 29, 14, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 23, 29, 14, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'booking_id', 'Type': 'string'},\n",
       "    {'Name': 'listing_id', 'Type': 'int'},\n",
       "    {'Name': 'booking_date', 'Type': 'timestamp'},\n",
       "    {'Name': 'nights_booked', 'Type': 'int'},\n",
       "    {'Name': 'booking_amount', 'Type': 'int'},\n",
       "    {'Name': 'cleaning_fee', 'Type': 'int'},\n",
       "    {'Name': 'service_fee', 'Type': 'int'},\n",
       "    {'Name': 'booking_status', 'Type': 'string'},\n",
       "    {'Name': 'created_at', 'Type': 'timestamp'},\n",
       "    {'Name': 'total_booking_amount', 'Type': 'decimal(10,2)'},\n",
       "    {'Name': 'additional_cost', 'Type': 'decimal(10,2)'},\n",
       "    {'Name': 'total_cost', 'Type': 'decimal(10,2)'},\n",
       "    {'Name': 'processed_at', 'Type': 'timestamp'}],\n",
       "   'Location': 's3://s3-complete-pipeline/processed/',\n",
       "   'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
       "    'Parameters': {'serialization.format': '1'}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'sizeKey': '268529',\n",
       "    'objectCount': '1',\n",
       "    'UPDATED_BY_CRAWLER': 'my_processed_data_crawler',\n",
       "    'CRAWL_RUN_ID': 'f90f6af0-6910-49c7-912b-24608f2b1684',\n",
       "    'recordCount': '5000',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '62',\n",
       "    'partition_filtering.enabled': 'true',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'parquet',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [{'Name': 'partition_0', 'Type': 'string'}],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'sizeKey': '268529',\n",
       "   'objectCount': '1',\n",
       "   'UPDATED_BY_CRAWLER': 'my_processed_data_crawler',\n",
       "   'CRAWL_RUN_ID': 'f90f6af0-6910-49c7-912b-24608f2b1684',\n",
       "   'recordCount': '5000',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '62',\n",
       "   'partition_filtering.enabled': 'true',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'parquet',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '1',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False},\n",
       " {'Name': 'raw',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'host_id', 'Type': 'bigint'},\n",
       "    {'Name': 'host_name', 'Type': 'string'},\n",
       "    {'Name': 'host_since', 'Type': 'string'},\n",
       "    {'Name': 'is_superhost', 'Type': 'boolean'},\n",
       "    {'Name': 'response_rate', 'Type': 'bigint'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '13083',\n",
       "    'objectCount': '1',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '207',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '63',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': 'ca8177af-3451-41f6-8098-5213f431aa33',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '13083',\n",
       "   'objectCount': '1',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '207',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '63',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': 'ca8177af-3451-41f6-8098-5213f431aa33',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_tables(database_name):\n",
    "    \"\"\"\n",
    "    List all tables in a Glue database\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Database name\n",
    "    \n",
    "    Returns:\n",
    "        list: Table information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Tables in database '{database_name}':\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_tables(DatabaseName=database_name)\n",
    "        tables = response.get('TableList', [])\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables found\")\n",
    "            return []\n",
    "        \n",
    "        for table in tables:\n",
    "            print(f\"\\nTable: {table['Name']}\")\n",
    "            print(f\"  Location: {table.get('StorageDescriptor', {}).get('Location', 'N/A')}\")\n",
    "            print(f\"  Format: {table.get('Parameters', {}).get('classification', 'N/A')}\")\n",
    "            \n",
    "            # Show columns\n",
    "            columns = table.get('StorageDescriptor', {}).get('Columns', [])\n",
    "            if columns:\n",
    "                print(f\"  Columns ({len(columns)}):\")\n",
    "                for col in columns[:5]:  # Show first 5\n",
    "                    print(f\"    - {col['Name']}: {col['Type']}\")\n",
    "                if len(columns) > 5:\n",
    "                    print(f\"    ... and {len(columns) - 5} more\")\n",
    "        \n",
    "        print(f\"\\nTotal: {len(tables)} table(s)\")\n",
    "        return tables\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# Verify crawler created tables\n",
    "list_tables('aws_full_pipeline_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x0us7nb3rfd",
   "metadata": {},
   "source": [
    "## 2.7 List Crawlers\n",
    "\n",
    "View all crawlers in your account with their current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "s04fu1uel7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Crawlers:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Crawler: db_s3_crawler\n",
      "  State: READY\n",
      "  Database: glue_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: full_pipeline_crawler\n",
      "  State: READY\n",
      "  Database: aws_full_pipeline_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: my-crawler\n",
      "  State: READY\n",
      "  Database: data_engineering_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: my_processed_data_crawler\n",
      "  State: READY\n",
      "  Database: aws_full_pipeline_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Total: 4 crawler(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'db_s3_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://real-learn-s3/processed/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'glue_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'DEPRECATE_IN_DATABASE'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 0, 15, 47, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 0, 15, 47, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:db_s3_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: ad4a76dd-3d4f-4471-9a73-0b8c40264d90; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'db_s3_crawler',\n",
       "   'MessagePrefix': '59e7981a-85f4-4423-b698-0f8c29c94bf5',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 0, 20, 59, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'Configuration': '{\"Version\":1.0,\"CreatePartitionIndex\":true}',\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'full_pipeline_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://s3-complete-pipeline/raw/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 22, 10, 54, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 22, 10, 54, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:full_pipeline_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: b00699c9-8748-415c-9965-240364c6d695; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'full_pipeline_crawler',\n",
       "   'MessagePrefix': 'd6c19c28-e9c7-4c1d-9a73-da604df486eb',\n",
       "   'StartTime': datetime.datetime(2026, 1, 31, 2, 16, 5, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'my-crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://real-learn-s3/raw/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'data_engineering_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 1, 25, 15, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 1, 25, 15, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:my-crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: 5c03d4eb-525f-4f08-b119-6309c565b76b; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'my-crawler',\n",
       "   'MessagePrefix': '6933cefa-b8c2-45d9-b00b-9a5f54546f3a',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 1, 36, 25, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'my_processed_data_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://s3-complete-pipeline/processed/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 23, 27, 49, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 23, 27, 49, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:my_processed_data_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: 31b60544-e3ab-49aa-be22-d29462469db5; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'my_processed_data_crawler',\n",
       "   'MessagePrefix': 'f90f6af0-6910-49c7-912b-24608f2b1684',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 23, 28, 30, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_crawlers():\n",
    "    \"\"\"\n",
    "    List all Glue crawlers\n",
    "    \n",
    "    Returns:\n",
    "        list: Crawler names and states\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Glue Crawlers:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_crawlers()\n",
    "        crawlers = response.get('Crawlers', [])\n",
    "        \n",
    "        if not crawlers:\n",
    "            print(\"No crawlers found\")\n",
    "            return []\n",
    "        \n",
    "        for crawler in crawlers:\n",
    "            print(f\"\\nCrawler: {crawler['Name']}\")\n",
    "            print(f\"  State: {crawler['State']}\")\n",
    "            print(f\"  Database: {crawler.get('DatabaseName', 'N/A')}\")\n",
    "            \n",
    "            # Last crawl info\n",
    "            last = crawler.get('LastCrawl', {})\n",
    "            if last:\n",
    "                print(f\"  Last Run: {last.get('Status', 'N/A')}\")\n",
    "                print(f\"  Tables Created: {last.get('TablesCreated', 0)}\")\n",
    "                print(f\"  Tables Updated: {last.get('TablesUpdated', 0)}\")\n",
    "        \n",
    "        print(f\"\\nTotal: {len(crawlers)} crawler(s)\")\n",
    "        return crawlers\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# List all crawlers\n",
    "list_crawlers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w1segdt9fn",
   "metadata": {},
   "source": [
    "## 2.8 Cleanup Functions\n",
    "\n",
    "Delete crawlers and databases when no longer needed. **Use with caution!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7tpay1ca8mn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_crawler(crawler_name):\n",
    "    \"\"\"\n",
    "    Delete a Glue crawler\n",
    "    \n",
    "    Args:\n",
    "        crawler_name (str): Crawler name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting crawler '{crawler_name}'...\")\n",
    "        glue_client.delete_crawler(Name=crawler_name)\n",
    "        print(f\"SUCCESS: Crawler '{crawler_name}' deleted\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityNotFoundException':\n",
    "            print(f\"Crawler '{crawler_name}' not found\")\n",
    "        else:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def delete_database(database_name):\n",
    "    \"\"\"\n",
    "    Delete a Glue database and all its tables\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Database name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting database '{database_name}'...\")\n",
    "        glue_client.delete_database(Name=database_name)\n",
    "        print(f\"SUCCESS: Database '{database_name}' deleted\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityNotFoundException':\n",
    "            print(f\"Database '{database_name}' not found\")\n",
    "        else:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example (uncomment to use):\n",
    "# delete_crawler('full_pipeline_crawler')\n",
    "# delete_database('aws_full_pipeline_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4c360",
   "metadata": {},
   "source": [
    "# PHASE 3: DATA TRANSFORMATION (ETL)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 3: TRANSFORM DATA WITH GLUE ETL JOB                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   S3 (raw/)  ────►  Glue ETL Job  ────►  S3 (processed/)                    │\n",
    "│   (CSV)             (Spark/Python)       (Parquet)                          │\n",
    "│                                                                             │\n",
    "│   Key Operations:                                                           │\n",
    "│   • Upload ETL script to S3                                                 │\n",
    "│   • Create Glue job pointing to script                                      │\n",
    "│   • Run job with parameters                                                 │\n",
    "│   • Monitor job execution                                                   │\n",
    "│                                                                             │\n",
    "│   Transformations in aws_glue_etl.py:                                       │\n",
    "│   • Null value checking & rejection                                         │\n",
    "│   • Duplicate detection & handling                                          │\n",
    "│   • Calculated columns:                                                     │\n",
    "│     - total_booking_amount = nights_booked * booking_amount                 │\n",
    "│     - additional_cost = cleaning_fee + service_fee                          │\n",
    "│     - total_cost = total_booking_amount + additional_cost                   │\n",
    "│   • Output as Parquet (compressed, columnar)                                │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 3.1 Upload Raw Data (bookings.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "621396fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/bookings.csv' uploaded to 's3-complete-pipeline/raw/bookings.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_file('data/bookings.csv', S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}bookings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6h7i7ilhnx",
   "metadata": {},
   "source": [
    "## 3.2 Upload ETL Script to S3\n",
    "\n",
    "Glue jobs require the Python script to be stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbc8f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'scripts/aws_glue_etl.py' uploaded to 's3-complete-pipeline/scripts/aws_glue_etl.py'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload the ETL script to S3\n",
    "upload_file('scripts/aws_glue_etl.py', S3_BUCKET_COMPLETE_PIPELINE, 'scripts/aws_glue_etl.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9335b66",
   "metadata": {},
   "source": [
    "## 3.3 Create Glue Job\n",
    "\n",
    "Create an ETL job that points to the script in S3. Job types:\n",
    "\n",
    "| Type | Use Case | Workers |\n",
    "|------|----------|---------|\n",
    "| `glueetl` | Spark-based, large datasets | 2+ DPUs |\n",
    "| `pythonshell` | Simple Python, small data | 0.0625-1 DPU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a18ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Glue job 'bookings-etl-job'...\n",
      "  Script: s3://s3-complete-pipeline/scripts/aws_glue_etl.py\n",
      "  Type: glueetl\n",
      "SUCCESS: Job 'bookings-etl-job' created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_job(job_name, script_location, description='', job_type='glueetl', \n",
    "                    worker_type='G.1X', num_workers=2, timeout=60, max_retries=0):\n",
    "    \"\"\"\n",
    "    Create a Glue ETL job\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "        script_location (str): S3 path to ETL script (s3://bucket/path/script.py)\n",
    "        description (str): Job description\n",
    "        job_type (str): 'glueetl' (Spark) or 'pythonshell'\n",
    "        worker_type (str): 'G.1X', 'G.2X', 'G.025X' (for pythonshell)\n",
    "        num_workers (int): Number of workers (min 2 for glueetl)\n",
    "        timeout (int): Job timeout in minutes\n",
    "        max_retries (int): Number of retries on failure\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if created successfully\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating Glue job '{job_name}'...\")\n",
    "        print(f\"  Script: {script_location}\")\n",
    "        print(f\"  Type: {job_type}\")\n",
    "        \n",
    "        job_config = {\n",
    "            'Name': job_name,\n",
    "            'Description': description,\n",
    "            'Role': GLUE_ROLE_ARN,\n",
    "            'Command': {\n",
    "                'Name': job_type,\n",
    "                'ScriptLocation': script_location,\n",
    "                'PythonVersion': '3'\n",
    "            },\n",
    "            'DefaultArguments': {\n",
    "                '--job-language': 'python',\n",
    "                '--enable-metrics': 'true',\n",
    "                '--enable-continuous-cloudwatch-log': 'true'\n",
    "            },\n",
    "            'Timeout': timeout,\n",
    "            'MaxRetries': max_retries,\n",
    "            'GlueVersion': '4.0'\n",
    "        }\n",
    "        \n",
    "        # Add worker config for glueetl jobs\n",
    "        if job_type == 'glueetl':\n",
    "            job_config['WorkerType'] = worker_type\n",
    "            job_config['NumberOfWorkers'] = num_workers\n",
    "        \n",
    "        glue_client.create_job(**job_config)\n",
    "        \n",
    "        print(f\"SUCCESS: Job '{job_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Job '{job_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Create the bookings ETL job\n",
    "script_path = f's3://{S3_BUCKET_COMPLETE_PIPELINE}/scripts/aws_glue_etl.py'\n",
    "create_glue_job(\n",
    "    job_name='bookings-etl-job',\n",
    "    script_location=script_path,\n",
    "    description='Transform bookings data: null check, dedup, calculate totals'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y40wylhjxam",
   "metadata": {},
   "source": [
    "## 3.4 Run Glue Job\n",
    "\n",
    "Execute the job with parameters. The job reads from `raw/bookings.csv` and writes to `processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1df09c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 'bookings-etl-job'...\n",
      "  Arguments: {'--S3_BUCKET': 's3-complete-pipeline', '--SOURCE_PREFIX': 'raw', '--TARGET_PREFIX': 'processed', '--DUPLICATE_HANDLING': 'keep_first'}\n",
      "Job started. Run ID: jr_aff837deeced3a49b6ac7a2e761f39cd321686edb57574fbf12b03f8397a2e7a\n",
      "Waiting for completion...\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "Job SUCCEEDED\n",
      "Duration: 105 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jr_aff837deeced3a49b6ac7a2e761f39cd321686edb57574fbf12b03f8397a2e7a'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_glue_job(job_name, arguments=None, wait=False):\n",
    "    \"\"\"\n",
    "    Start a Glue job run\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "        arguments (dict): Job arguments (e.g., {'--S3_BUCKET': 'my-bucket'})\n",
    "        wait (bool): Wait for completion\n",
    "    \n",
    "    Returns:\n",
    "        str: Job run ID, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting job '{job_name}'...\")\n",
    "        \n",
    "        run_config = {'JobName': job_name}\n",
    "        if arguments:\n",
    "            run_config['Arguments'] = arguments\n",
    "            print(f\"  Arguments: {arguments}\")\n",
    "        \n",
    "        response = glue_client.start_job_run(**run_config)\n",
    "        run_id = response['JobRunId']\n",
    "        \n",
    "        print(f\"Job started. Run ID: {run_id}\")\n",
    "        \n",
    "        if wait:\n",
    "            print(\"Waiting for completion...\")\n",
    "            while True:\n",
    "                status_response = glue_client.get_job_run(JobName=job_name, RunId=run_id)\n",
    "                state = status_response['JobRun']['JobRunState']\n",
    "                \n",
    "                if state in ['SUCCEEDED', 'FAILED', 'STOPPED', 'TIMEOUT']:\n",
    "                    print(f\"Job {state}\")\n",
    "                    if state == 'FAILED':\n",
    "                        error = status_response['JobRun'].get('ErrorMessage', 'Unknown')\n",
    "                        print(f\"Error: {error}\")\n",
    "                    elif state == 'SUCCEEDED':\n",
    "                        duration = status_response['JobRun'].get('ExecutionTime', 0)\n",
    "                        print(f\"Duration: {duration} seconds\")\n",
    "                    break\n",
    "                    \n",
    "                print(f\"  State: {state}\")\n",
    "                time.sleep(30)\n",
    "        \n",
    "        return run_id\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Run the bookings ETL job\n",
    "run_glue_job(\n",
    "    job_name='bookings-etl-job',\n",
    "    arguments={\n",
    "        '--S3_BUCKET': S3_BUCKET_COMPLETE_PIPELINE,\n",
    "        '--SOURCE_PREFIX': 'raw',\n",
    "        '--TARGET_PREFIX': 'processed',\n",
    "        '--DUPLICATE_HANDLING': 'keep_first'\n",
    "    },\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fkb3ohfljw",
   "metadata": {},
   "source": [
    "## 3.5 Get Job Run Status\n",
    "\n",
    "Check the status of a specific job run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ykodtehg1qo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Run Status for 'bookings-etl-job':\n",
      "--------------------------------------------------\n",
      "  Run ID: jr_aff837deeced3a49b6ac7a2e761f39cd321686edb57574fbf12b03f8397a2e7a\n",
      "  State: SUCCEEDED\n",
      "  Started: 2026-01-31 02:17:55.659000-05:00\n",
      "  Completed: 2026-01-31 02:19:48.985000-05:00\n",
      "  Duration: 105 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Id': 'jr_aff837deeced3a49b6ac7a2e761f39cd321686edb57574fbf12b03f8397a2e7a',\n",
       " 'Attempt': 0,\n",
       " 'JobName': 'bookings-etl-job',\n",
       " 'JobMode': 'SCRIPT',\n",
       " 'JobRunQueuingEnabled': False,\n",
       " 'StartedOn': datetime.datetime(2026, 1, 31, 2, 17, 55, 659000, tzinfo=tzlocal()),\n",
       " 'LastModifiedOn': datetime.datetime(2026, 1, 31, 2, 19, 48, 985000, tzinfo=tzlocal()),\n",
       " 'CompletedOn': datetime.datetime(2026, 1, 31, 2, 19, 48, 985000, tzinfo=tzlocal()),\n",
       " 'JobRunState': 'SUCCEEDED',\n",
       " 'Arguments': {'--TARGET_PREFIX': 'processed',\n",
       "  '--DUPLICATE_HANDLING': 'keep_first',\n",
       "  '--S3_BUCKET': 's3-complete-pipeline',\n",
       "  '--SOURCE_PREFIX': 'raw'},\n",
       " 'PredecessorRuns': [],\n",
       " 'AllocatedCapacity': 2,\n",
       " 'ExecutionTime': 105,\n",
       " 'Timeout': 60,\n",
       " 'MaxCapacity': 2.0,\n",
       " 'WorkerType': 'G.1X',\n",
       " 'NumberOfWorkers': 2,\n",
       " 'LogGroupName': '/aws-glue/jobs',\n",
       " 'GlueVersion': '4.0'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_job_run_status(job_name, run_id=None):\n",
    "    \"\"\"\n",
    "    Get status of a job run (latest if run_id not specified)\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "        run_id (str): Specific run ID (optional)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Job run details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if run_id:\n",
    "            response = glue_client.get_job_run(JobName=job_name, RunId=run_id)\n",
    "            runs = [response['JobRun']]\n",
    "        else:\n",
    "            response = glue_client.get_job_runs(JobName=job_name, MaxResults=1)\n",
    "            runs = response.get('JobRuns', [])\n",
    "        \n",
    "        if not runs:\n",
    "            print(f\"No runs found for job '{job_name}'\")\n",
    "            return None\n",
    "        \n",
    "        run = runs[0]\n",
    "        print(f\"Job Run Status for '{job_name}':\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"  Run ID: {run['Id']}\")\n",
    "        print(f\"  State: {run['JobRunState']}\")\n",
    "        print(f\"  Started: {run.get('StartedOn', 'N/A')}\")\n",
    "        print(f\"  Completed: {run.get('CompletedOn', 'N/A')}\")\n",
    "        print(f\"  Duration: {run.get('ExecutionTime', 0)} seconds\")\n",
    "        \n",
    "        if run['JobRunState'] == 'FAILED':\n",
    "            print(f\"  Error: {run.get('ErrorMessage', 'Unknown')}\")\n",
    "        \n",
    "        return run\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Check latest run status\n",
    "get_job_run_status('bookings-etl-job')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t7gt90dnpfm",
   "metadata": {},
   "source": [
    "## 3.6 List Jobs\n",
    "\n",
    "View all Glue jobs in the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eatai18key",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Jobs:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Job: bookings-etl-job\n",
      "  Description: Transform bookings data: null check, dedup, calculate totals\n",
      "  Type: glueetl\n",
      "  Glue Version: 4.0\n",
      "  Workers: 2\n",
      "  Timeout: 60 min\n",
      "\n",
      "Total: 1 job(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'bookings-etl-job',\n",
       "  'JobMode': 'SCRIPT',\n",
       "  'JobRunQueuingEnabled': False,\n",
       "  'Description': 'Transform bookings data: null check, dedup, calculate totals',\n",
       "  'Role': 'arn:aws:iam::445952351133:role/glue-access-s3',\n",
       "  'CreatedOn': datetime.datetime(2026, 1, 31, 2, 17, 28, 339000, tzinfo=tzlocal()),\n",
       "  'LastModifiedOn': datetime.datetime(2026, 1, 31, 2, 17, 28, 339000, tzinfo=tzlocal()),\n",
       "  'ExecutionProperty': {'MaxConcurrentRuns': 1},\n",
       "  'Command': {'Name': 'glueetl',\n",
       "   'ScriptLocation': 's3://s3-complete-pipeline/scripts/aws_glue_etl.py',\n",
       "   'PythonVersion': '3'},\n",
       "  'DefaultArguments': {'--enable-metrics': 'true',\n",
       "   '--job-language': 'python',\n",
       "   '--enable-continuous-cloudwatch-log': 'true'},\n",
       "  'MaxRetries': 0,\n",
       "  'AllocatedCapacity': 2,\n",
       "  'Timeout': 60,\n",
       "  'MaxCapacity': 2.0,\n",
       "  'WorkerType': 'G.1X',\n",
       "  'NumberOfWorkers': 2,\n",
       "  'GlueVersion': '4.0'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_jobs():\n",
    "    \"\"\"\n",
    "    List all Glue jobs\n",
    "    \n",
    "    Returns:\n",
    "        list: Job details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Glue Jobs:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_jobs()\n",
    "        jobs = response.get('Jobs', [])\n",
    "        \n",
    "        if not jobs:\n",
    "            print(\"No jobs found\")\n",
    "            return []\n",
    "        \n",
    "        for job in jobs:\n",
    "            print(f\"\\nJob: {job['Name']}\")\n",
    "            print(f\"  Description: {job.get('Description', 'N/A')}\")\n",
    "            print(f\"  Type: {job['Command']['Name']}\")\n",
    "            print(f\"  Glue Version: {job.get('GlueVersion', 'N/A')}\")\n",
    "            print(f\"  Workers: {job.get('NumberOfWorkers', 'N/A')}\")\n",
    "            print(f\"  Timeout: {job.get('Timeout', 'N/A')} min\")\n",
    "        \n",
    "        print(f\"\\nTotal: {len(jobs)} job(s)\")\n",
    "        return jobs\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# List all jobs\n",
    "list_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neevf3ydxg",
   "metadata": {},
   "source": [
    "## 3.7 Verify Output\n",
    "\n",
    "Check the processed folder to confirm Parquet files were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f13gv07tniq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 's3-complete-pipeline/processed/':\n",
      "Key: processed/bookings/part-00000-cba2c189-2dd3-4c74-84ed-147fc7009b83-c000.snappy.parquet\n",
      "Size: 262.24 KB (268,529 bytes)\n",
      "Last Modified: 2026-01-31 07:19:37\n",
      "Storage Class: STANDARD\n",
      "ETag: \"223992fff8c7a102c9d42e521ebb0997-1\"\n",
      "Total: 1 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'processed/bookings/part-00000-cba2c189-2dd3-4c74-84ed-147fc7009b83-c000.snappy.parquet',\n",
       "  'LastModified': datetime.datetime(2026, 1, 31, 7, 19, 37, tzinfo=tzutc()),\n",
       "  'ETag': '\"223992fff8c7a102c9d42e521ebb0997-1\"',\n",
       "  'ChecksumAlgorithm': ['CRC64NVME'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 268529,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify processed data was created\n",
    "list_objects_detailed(S3_BUCKET_COMPLETE_PIPELINE, prefix='processed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5nex43tuyxo",
   "metadata": {},
   "source": [
    "## 3.8 Cleanup Functions\n",
    "\n",
    "Delete jobs when no longer needed. **Use with caution!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sd0r7yuktgo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting job 'bookings-etl-job'...\n",
      "SUCCESS: Job 'bookings-etl-job' deleted\n",
      "Deleting job 'my-etl-job'...\n",
      "SUCCESS: Job 'my-etl-job' deleted\n",
      "Deleting job 'aws-glue-pipeline'...\n",
      "SUCCESS: Job 'aws-glue-pipeline' deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_job(job_name):\n",
    "    \"\"\"\n",
    "    Delete a Glue job\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting job '{job_name}'...\")\n",
    "        glue_client.delete_job(JobName=job_name)\n",
    "        print(f\"SUCCESS: Job '{job_name}' deleted\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityNotFoundException':\n",
    "            print(f\"Job '{job_name}' not found\")\n",
    "        else:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example (uncomment to use):\n",
    "delete_job('bookings-etl-job')\n",
    "delete_job('my-etl-job')\n",
    "delete_job('aws-glue-pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a0b2f",
   "metadata": {},
   "source": [
    "# PHASE 4: CATALOG PROCESSED DATA\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 4: RE-CATALOG TRANSFORMED DATA                                       │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   S3 (processed/)  ────►  Glue Crawler  ────►  Data Catalog                 │\n",
    "│   (Parquet)               (auto-schema)        (new table)                  │\n",
    "│                                                                             │\n",
    "│   Why re-crawl?                                                             │\n",
    "│   • Parquet has different schema than CSV                                   │\n",
    "│   • New calculated columns need to be cataloged                             │\n",
    "│   • Enables Athena queries on processed data                                │\n",
    "│                                                                             │\n",
    "│   Result:                                                                   │\n",
    "│   • New table 'bookings' in Data Catalog                                    │\n",
    "│   • Columns include: total_booking_amount, additional_cost, total_cost      │\n",
    "│   • Format: Parquet (faster queries, columnar storage)                      │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 4.1 Create Crawler for Processed Data\n",
    "\n",
    "Point a new crawler at the `processed/` folder to discover the Parquet schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f54dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating crawler 'my_processed_data_crawler'...\n",
      "Target: aws_full_pipeline_db\n",
      "Path: s3://s3-complete-pipeline/processed/\n",
      "Crawler 'my_processed_data_crawler' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_glue_crawler('my_processed_data_crawler', 'aws_full_pipeline_db', f's3://{S3_BUCKET_COMPLETE_PIPELINE}/processed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hrqqnhj9tu",
   "metadata": {},
   "source": [
    "## 4.2 Run Crawler\n",
    "\n",
    "Execute the crawler to scan processed Parquet files and update the Data Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb9d5b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawler 'my_processed_data_crawler'...\n",
      "Crawler started\n",
      "Waiting for completion...\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "Completed: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_crawler('my_processed_data_crawler', wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sf3qaeee6ya",
   "metadata": {},
   "source": [
    "## 4.3 Verify New Table\n",
    "\n",
    "Confirm the crawler created a new table with the calculated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4ebef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in database 'aws_full_pipeline_db':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Table: bookings_csv\n",
      "  Location: s3://s3-complete-pipeline/raw/bookings.csv\n",
      "  Format: csv\n",
      "  Columns (9):\n",
      "    - booking_id: string\n",
      "    - listing_id: bigint\n",
      "    - booking_date: string\n",
      "    - nights_booked: bigint\n",
      "    - booking_amount: bigint\n",
      "    ... and 4 more\n",
      "\n",
      "Table: hosts_csv\n",
      "  Location: s3://s3-complete-pipeline/raw/hosts.csv\n",
      "  Format: csv\n",
      "  Columns (6):\n",
      "    - host_id: bigint\n",
      "    - host_name: string\n",
      "    - host_since: string\n",
      "    - is_superhost: boolean\n",
      "    - response_rate: bigint\n",
      "    ... and 1 more\n",
      "\n",
      "Table: lambda_bookings_csv\n",
      "  Location: s3://s3-complete-pipeline/raw/lambda-bookings.csv\n",
      "  Format: csv\n",
      "  Columns (9):\n",
      "    - booking_id: string\n",
      "    - listing_id: bigint\n",
      "    - booking_date: string\n",
      "    - nights_booked: bigint\n",
      "    - booking_amount: bigint\n",
      "    ... and 4 more\n",
      "\n",
      "Table: processed\n",
      "  Location: s3://s3-complete-pipeline/processed/\n",
      "  Format: parquet\n",
      "  Columns (13):\n",
      "    - booking_id: string\n",
      "    - listing_id: int\n",
      "    - booking_date: timestamp\n",
      "    - nights_booked: int\n",
      "    - booking_amount: int\n",
      "    ... and 8 more\n",
      "\n",
      "Table: raw\n",
      "  Location: s3://s3-complete-pipeline/raw/\n",
      "  Format: csv\n",
      "  Columns (6):\n",
      "    - host_id: bigint\n",
      "    - host_name: string\n",
      "    - host_since: string\n",
      "    - is_superhost: boolean\n",
      "    - response_rate: bigint\n",
      "    ... and 1 more\n",
      "\n",
      "Total: 5 table(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'bookings_csv',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 23, 27, 3, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 23, 27, 3, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 23, 27, 3, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'booking_id', 'Type': 'string'},\n",
       "    {'Name': 'listing_id', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_date', 'Type': 'string'},\n",
       "    {'Name': 'nights_booked', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_amount', 'Type': 'bigint'},\n",
       "    {'Name': 'cleaning_fee', 'Type': 'bigint'},\n",
       "    {'Name': 'service_fee', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_status', 'Type': 'string'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/bookings.csv',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '513378',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '4936',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '104',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': '8f513bb5-937e-40ce-80e1-537505e4899b',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '513378',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '4936',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '104',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': '8f513bb5-937e-40ce-80e1-537505e4899b',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False},\n",
       " {'Name': 'hosts_csv',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 23, 27, 2, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 23, 27, 2, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 23, 27, 2, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'host_id', 'Type': 'bigint'},\n",
       "    {'Name': 'host_name', 'Type': 'string'},\n",
       "    {'Name': 'host_since', 'Type': 'string'},\n",
       "    {'Name': 'is_superhost', 'Type': 'boolean'},\n",
       "    {'Name': 'response_rate', 'Type': 'bigint'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/hosts.csv',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '13083',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '207',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '63',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': '8f513bb5-937e-40ce-80e1-537505e4899b',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '13083',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '207',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '63',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': '8f513bb5-937e-40ce-80e1-537505e4899b',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False},\n",
       " {'Name': 'lambda_bookings_csv',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 31, 2, 16, 52, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 31, 2, 16, 52, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 31, 2, 16, 52, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'booking_id', 'Type': 'string'},\n",
       "    {'Name': 'listing_id', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_date', 'Type': 'string'},\n",
       "    {'Name': 'nights_booked', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_amount', 'Type': 'bigint'},\n",
       "    {'Name': 'cleaning_fee', 'Type': 'bigint'},\n",
       "    {'Name': 'service_fee', 'Type': 'bigint'},\n",
       "    {'Name': 'booking_status', 'Type': 'string'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/lambda-bookings.csv',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '1942',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '18',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '104',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': 'd6c19c28-e9c7-4c1d-9a73-da604df486eb',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '1942',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '18',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '104',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': 'd6c19c28-e9c7-4c1d-9a73-da604df486eb',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False},\n",
       " {'Name': 'processed',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 23, 29, 13, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 23, 29, 14, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 23, 29, 14, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'booking_id', 'Type': 'string'},\n",
       "    {'Name': 'listing_id', 'Type': 'int'},\n",
       "    {'Name': 'booking_date', 'Type': 'timestamp'},\n",
       "    {'Name': 'nights_booked', 'Type': 'int'},\n",
       "    {'Name': 'booking_amount', 'Type': 'int'},\n",
       "    {'Name': 'cleaning_fee', 'Type': 'int'},\n",
       "    {'Name': 'service_fee', 'Type': 'int'},\n",
       "    {'Name': 'booking_status', 'Type': 'string'},\n",
       "    {'Name': 'created_at', 'Type': 'timestamp'},\n",
       "    {'Name': 'total_booking_amount', 'Type': 'decimal(10,2)'},\n",
       "    {'Name': 'additional_cost', 'Type': 'decimal(10,2)'},\n",
       "    {'Name': 'total_cost', 'Type': 'decimal(10,2)'},\n",
       "    {'Name': 'processed_at', 'Type': 'timestamp'}],\n",
       "   'Location': 's3://s3-complete-pipeline/processed/',\n",
       "   'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
       "    'Parameters': {'serialization.format': '1'}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'sizeKey': '268529',\n",
       "    'objectCount': '1',\n",
       "    'UPDATED_BY_CRAWLER': 'my_processed_data_crawler',\n",
       "    'CRAWL_RUN_ID': 'f90f6af0-6910-49c7-912b-24608f2b1684',\n",
       "    'recordCount': '5000',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '62',\n",
       "    'partition_filtering.enabled': 'true',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'parquet',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [{'Name': 'partition_0', 'Type': 'string'}],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'sizeKey': '268529',\n",
       "   'objectCount': '1',\n",
       "   'UPDATED_BY_CRAWLER': 'my_processed_data_crawler',\n",
       "   'CRAWL_RUN_ID': 'f90f6af0-6910-49c7-912b-24608f2b1684',\n",
       "   'recordCount': '5000',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '62',\n",
       "   'partition_filtering.enabled': 'true',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'parquet',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '1',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False},\n",
       " {'Name': 'raw',\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2026, 1, 30, 22, 12, 26, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'host_id', 'Type': 'bigint'},\n",
       "    {'Name': 'host_name', 'Type': 'string'},\n",
       "    {'Name': 'host_since', 'Type': 'string'},\n",
       "    {'Name': 'is_superhost', 'Type': 'boolean'},\n",
       "    {'Name': 'response_rate', 'Type': 'bigint'},\n",
       "    {'Name': 'created_at', 'Type': 'string'}],\n",
       "   'Location': 's3://s3-complete-pipeline/raw/',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'skip.header.line.count': '1',\n",
       "    'sizeKey': '13083',\n",
       "    'objectCount': '1',\n",
       "    'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "    'recordCount': '207',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'averageRecordSize': '63',\n",
       "    'compressionType': 'none',\n",
       "    'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'delimiter': ',',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'CRAWL_RUN_ID': 'ca8177af-3451-41f6-8098-5213f431aa33',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'skip.header.line.count': '1',\n",
       "   'sizeKey': '13083',\n",
       "   'objectCount': '1',\n",
       "   'UPDATED_BY_CRAWLER': 'full_pipeline_crawler',\n",
       "   'recordCount': '207',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'averageRecordSize': '63',\n",
       "   'compressionType': 'none',\n",
       "   'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'delimiter': ',',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'CRAWL_RUN_ID': 'ca8177af-3451-41f6-8098-5213f431aa33',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::445952351133:assumed-role/glue-access-s3/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '445952351133',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False,\n",
       "  'IsMaterializedView': False}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify crawler created tables\n",
    "list_tables('aws_full_pipeline_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4p50rwwl9",
   "metadata": {},
   "source": [
    "## 4.4 List All Crawlers\n",
    "\n",
    "View all crawlers to confirm both raw and processed crawlers exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "821b23df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Crawlers:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Crawler: db_s3_crawler\n",
      "  State: READY\n",
      "  Database: glue_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: full_pipeline_crawler\n",
      "  State: READY\n",
      "  Database: aws_full_pipeline_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: my-crawler\n",
      "  State: READY\n",
      "  Database: data_engineering_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: my_processed_data_crawler\n",
      "  State: READY\n",
      "  Database: aws_full_pipeline_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Total: 4 crawler(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'db_s3_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://real-learn-s3/processed/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'glue_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'DEPRECATE_IN_DATABASE'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 0, 15, 47, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 0, 15, 47, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:db_s3_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: ad4a76dd-3d4f-4471-9a73-0b8c40264d90; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'db_s3_crawler',\n",
       "   'MessagePrefix': '59e7981a-85f4-4423-b698-0f8c29c94bf5',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 0, 20, 59, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'Configuration': '{\"Version\":1.0,\"CreatePartitionIndex\":true}',\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'full_pipeline_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://s3-complete-pipeline/raw/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 22, 10, 54, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 22, 10, 54, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:full_pipeline_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: b00699c9-8748-415c-9965-240364c6d695; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'full_pipeline_crawler',\n",
       "   'MessagePrefix': 'd6c19c28-e9c7-4c1d-9a73-da604df486eb',\n",
       "   'StartTime': datetime.datetime(2026, 1, 31, 2, 16, 5, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'my-crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://real-learn-s3/raw/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'data_engineering_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 1, 25, 15, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 1, 25, 15, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:my-crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: 5c03d4eb-525f-4f08-b119-6309c565b76b; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'my-crawler',\n",
       "   'MessagePrefix': '6933cefa-b8c2-45d9-b00b-9a5f54546f3a',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 1, 36, 25, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'my_processed_data_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://s3-complete-pipeline/processed/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 23, 27, 49, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 23, 27, 49, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:my_processed_data_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: af74fc22-3305-43b9-901a-a0ed3cbea308; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'my_processed_data_crawler',\n",
       "   'MessagePrefix': 'ca572774-1b46-4cb0-9d95-cc2e2974fdc2',\n",
       "   'StartTime': datetime.datetime(2026, 1, 31, 2, 20, 10, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# List all crawlers\n",
    "list_crawlers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09feb0cc",
   "metadata": {},
   "source": [
    "# PHASE 5: ANALYZE WITH ATHENA\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 5: QUERY DATA WITH ATHENA                                            │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   Data Catalog  ────►  Athena SQL  ────►  Results                           │\n",
    "│   (tables)             (serverless)       (S3 + DataFrame)                  │\n",
    "│                                                                             │\n",
    "│   Key Features:                                                             │\n",
    "│   • Serverless - no infrastructure to manage                                │\n",
    "│   • Pay per query - $5 per TB scanned                                       │\n",
    "│   • Standard SQL syntax                                                     │\n",
    "│   • Direct query on S3 data (Parquet = faster + cheaper)                    │\n",
    "│                                                                             │\n",
    "│   Query Examples:                                                           │\n",
    "│   • SELECT, WHERE, GROUP BY, HAVING                                         │\n",
    "│   • Aggregations: COUNT, SUM, AVG                                           │\n",
    "│   • Date functions: DATE(), YEAR(), MONTH()                                 │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 5.1 Configure Athena Output Location\n",
    "\n",
    "Athena stores query results in S3. This location is required for all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f42dfe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena query results will be stored in: s3://s3-complete-pipeline/athena-results/\n"
     ]
    }
   ],
   "source": [
    "ATHENA_OUTPUT = f's3://{S3_BUCKET_COMPLETE_PIPELINE}/athena-results/'\n",
    "print(f\"Athena query results will be stored in: {ATHENA_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03s2z7balhaj",
   "metadata": {},
   "source": [
    "## 5.2 Initialize Athena Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c5eb623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena client initialized\n"
     ]
    }
   ],
   "source": [
    "# initialize athena client\n",
    "# Initialize Athena Client\n",
    "athena_client = boto3.client(\n",
    "    'athena',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "# Helper function for security\n",
    "def redact_account_id(text):\n",
    "    \"\"\"Redact AWS account ID from text\"\"\"\n",
    "    return re.sub(r':\\d{12}:', ':************:', str(text))\n",
    "\n",
    "print(\"Athena client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5vm40eimhvs",
   "metadata": {},
   "source": [
    "## 5.3 Run Athena Query\n",
    "\n",
    "Core function to execute SQL queries and return results. Handles polling for completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d69189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_athena_query(query, database, output_location=None, max_results=100):\n",
    "    \"\"\"\n",
    "    Execute an Athena query and return results\n",
    "    \n",
    "    Args:\n",
    "        query (str): SQL query to execute\n",
    "        database (str): Glue database name\n",
    "        output_location (str): S3 path for results (default: ATHENA_OUTPUT)\n",
    "        max_results (int): Maximum rows to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries with query results\n",
    "    \"\"\"\n",
    "    if output_location is None:\n",
    "        output_location = ATHENA_OUTPUT\n",
    "    \n",
    "    try:\n",
    "        print(f\"Executing query on database '{database}'...\")\n",
    "        print(f\"Query: {query[:100]}{'...' if len(query) > 100 else ''}\")\n",
    "        \n",
    "        # Start query execution\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            QueryExecutionContext={'Database': database},\n",
    "            ResultConfiguration={'OutputLocation': output_location}\n",
    "        )\n",
    "        \n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        print(f\"Query ID: {query_execution_id}\")\n",
    "        \n",
    "        # Wait for query to complete\n",
    "        while True:\n",
    "            result = athena_client.get_query_execution(\n",
    "                QueryExecutionId=query_execution_id\n",
    "            )\n",
    "            status = result['QueryExecution']['Status']['State']\n",
    "            \n",
    "            if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                break\n",
    "            \n",
    "            print(f\"  Status: {status}\")\n",
    "            time.sleep(1)\n",
    "        \n",
    "        if status == 'SUCCEEDED':\n",
    "            # Get execution stats\n",
    "            stats = result['QueryExecution']['Statistics']\n",
    "            data_scanned = stats.get('DataScannedInBytes', 0)\n",
    "            exec_time = stats.get('TotalExecutionTimeInMillis', 0)\n",
    "            \n",
    "            print(f\"\\nQuery SUCCEEDED\")\n",
    "            print(f\"Data scanned: {data_scanned / 1024 / 1024:.2f} MB\")\n",
    "            print(f\"Execution time: {exec_time / 1000:.2f} seconds\")\n",
    "            print(f\"Estimated cost: ${data_scanned / 1024 / 1024 / 1024 / 1024 * 5:.6f}\")\n",
    "            \n",
    "            # Get query results\n",
    "            results = athena_client.get_query_results(\n",
    "                QueryExecutionId=query_execution_id,\n",
    "                MaxResults=max_results\n",
    "            )\n",
    "            \n",
    "            # Parse results\n",
    "            rows = results['ResultSet']['Rows']\n",
    "            if not rows:\n",
    "                return []\n",
    "            \n",
    "            # First row is headers\n",
    "            headers = [col.get('VarCharValue', '') for col in rows[0]['Data']]\n",
    "            data = []\n",
    "            \n",
    "            for row in rows[1:]:\n",
    "                values = [col.get('VarCharValue', '') for col in row['Data']]\n",
    "                data.append(dict(zip(headers, values)))\n",
    "            \n",
    "            print(f\"Rows returned: {len(data)}\")\n",
    "            return data\n",
    "            \n",
    "        else:\n",
    "            error = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')\n",
    "            print(f\"Query {status}: {error}\")\n",
    "            return None\n",
    "            \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee7mz7y6en",
   "metadata": {},
   "source": [
    "## 5.4 Helper Functions\n",
    "\n",
    "Convenience functions for common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9802c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'aws_full_pipeline_db'...\n",
      "Query: SHOW TABLES\n",
      "Query ID: 58e0f58e-eee8-4f2a-88af-6d2d573b7fe6\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.23 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 4\n",
      "\n",
      "Tables:\n",
      "  - hosts_csv\n",
      "  - lambda_bookings_csv\n",
      "  - processed\n",
      "  - raw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'bookings_csv': 'hosts_csv'},\n",
       " {'bookings_csv': 'lambda_bookings_csv'},\n",
       " {'bookings_csv': 'processed'},\n",
       " {'bookings_csv': 'raw'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_tables(database):\n",
    "    \"\"\"\n",
    "    List all tables in a database\n",
    "    \"\"\"\n",
    "    query = \"SHOW TABLES\"\n",
    "    results = run_athena_query(query, database)\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nTables:\")\n",
    "        for row in results:\n",
    "            print(f\"  - {list(row.values())[0]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example\n",
    "show_tables('aws_full_pipeline_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "361f4026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'aws_full_pipeline_db'...\n",
      "Query: DESCRIBE processed\n",
      "Query ID: de293f46-f4c7-4f3b-8227-a69318218183\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.73 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 18\n",
      "\n",
      "Schema for 'processed':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'booking_id          \\tstring              \\t                    ': 'listing_id          \\tint                 \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'booking_date        \\ttimestamp           \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'nights_booked       \\tint                 \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'booking_amount      \\tint                 \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'cleaning_fee        \\tint                 \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'service_fee         \\tint                 \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'booking_status      \\tstring              \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'created_at          \\ttimestamp           \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'total_booking_amount\\tdecimal(10,2)       \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'additional_cost     \\tdecimal(10,2)       \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'total_cost          \\tdecimal(10,2)       \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'processed_at        \\ttimestamp           \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'partition_0         \\tstring              \\t                    '},\n",
       " {'booking_id          \\tstring              \\t                    ': '\\t \\t '},\n",
       " {'booking_id          \\tstring              \\t                    ': '# Partition Information\\t \\t '},\n",
       " {'booking_id          \\tstring              \\t                    ': '# col_name            \\tdata_type           \\tcomment             '},\n",
       " {'booking_id          \\tstring              \\t                    ': '\\t \\t '},\n",
       " {'booking_id          \\tstring              \\t                    ': 'partition_0         \\tstring              \\t                    '}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def describe_table(database, table_name):\n",
    "    \"\"\"\n",
    "    Show table schema\n",
    "    \"\"\"\n",
    "    query = f\"DESCRIBE {table_name}\"\n",
    "    results = run_athena_query(query, database)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nSchema for '{table_name}':\")\n",
    "        for row in results:\n",
    "            col_name = row.get('col_name', '')\n",
    "            data_type = row.get('data_type', '')\n",
    "            if col_name and not col_name.startswith('#'):\n",
    "                print(f\"  {col_name}: {data_type}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example\n",
    "describe_table('aws_full_pipeline_db', 'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gwqgm94quin",
   "metadata": {},
   "source": [
    "## 5.5 Convert Results to DataFrame\n",
    "\n",
    "Convert Athena results to Pandas DataFrame for analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c085ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function to return the results in dataframe format\n",
    "import pandas as pd\n",
    "# the function takes in results from run_athena_query and returns a dataframe\n",
    "def athena_results_to_dataframe(results):\n",
    "    \"\"\"\n",
    "    Convert Athena query results to a Pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        results (list): List of dictionaries from run_athena_query\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with query results\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to convert\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"Converted {len(df)} rows to DataFrame\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gdg6pxtvezc",
   "metadata": {},
   "source": [
    "## 5.6 Query Examples\n",
    "\n",
    "### Basic SELECT with LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa7020c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'aws_full_pipeline_db'...\n",
      "Query: \n",
      "SELECT * \n",
      "FROM processed \n",
      "LIMIT 10\n",
      "\n",
      "Query ID: f8efe835-cbcd-41b1-a7cf-1909e8c0f9c3\n",
      "  Status: QUEUED\n",
      "  Status: RUNNING\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.25 MB\n",
      "Execution time: 1.58 seconds\n",
      "Estimated cost: $0.000001\n",
      "Rows returned: 10\n",
      "Converted 10 rows to DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>booking_date</th>\n",
       "      <th>nights_booked</th>\n",
       "      <th>booking_amount</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>service_fee</th>\n",
       "      <th>booking_status</th>\n",
       "      <th>created_at</th>\n",
       "      <th>total_booking_amount</th>\n",
       "      <th>additional_cost</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>processed_at</th>\n",
       "      <th>partition_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008e04a-d69f-448b-82f5-de35a4ffe0eb</td>\n",
       "      <td>229</td>\n",
       "      <td>2025-09-25 00:00:00.000</td>\n",
       "      <td>14</td>\n",
       "      <td>2912</td>\n",
       "      <td>76</td>\n",
       "      <td>27</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>40768.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>40871.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00127cd0-c342-4a32-b9ce-f9defb8932f4</td>\n",
       "      <td>461</td>\n",
       "      <td>2025-05-28 00:00:00.000</td>\n",
       "      <td>14</td>\n",
       "      <td>994</td>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>13916.00</td>\n",
       "      <td>94.00</td>\n",
       "      <td>14010.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00186338-2229-4a7f-922d-1bc2adcd785f</td>\n",
       "      <td>455</td>\n",
       "      <td>2025-04-20 00:00:00.000</td>\n",
       "      <td>4</td>\n",
       "      <td>964</td>\n",
       "      <td>22</td>\n",
       "      <td>49</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>3856.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>3927.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bc7c8-0eea-412d-802c-b2c4d7136462</td>\n",
       "      <td>101</td>\n",
       "      <td>2025-10-06 00:00:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>1345</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>6725.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>6788.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001c3f21-9ecc-4a69-869f-261ee28e890f</td>\n",
       "      <td>238</td>\n",
       "      <td>2025-07-14 00:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>456</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>912.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>964.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0045bcd6-a019-419c-b8eb-00d66dba626a</td>\n",
       "      <td>402</td>\n",
       "      <td>2025-07-31 00:00:00.000</td>\n",
       "      <td>11</td>\n",
       "      <td>3047</td>\n",
       "      <td>67</td>\n",
       "      <td>26</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>33517.00</td>\n",
       "      <td>93.00</td>\n",
       "      <td>33610.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00468a09-1ece-477c-95c2-721e850cf52d</td>\n",
       "      <td>89</td>\n",
       "      <td>2025-03-09 00:00:00.000</td>\n",
       "      <td>14</td>\n",
       "      <td>742</td>\n",
       "      <td>23</td>\n",
       "      <td>46</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>10388.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>10457.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00516937-91bd-412a-acb7-cdc844e33ac3</td>\n",
       "      <td>157</td>\n",
       "      <td>2025-02-08 00:00:00.000</td>\n",
       "      <td>4</td>\n",
       "      <td>724</td>\n",
       "      <td>56</td>\n",
       "      <td>23</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>2896.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2975.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00531b5f-1010-4947-b5cf-11c259a16968</td>\n",
       "      <td>79</td>\n",
       "      <td>2025-03-10 00:00:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>530</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>2650.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>2704.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>006247af-0ac0-494f-b1a8-15b401d1056e</td>\n",
       "      <td>214</td>\n",
       "      <td>2025-07-15 00:00:00.000</td>\n",
       "      <td>12</td>\n",
       "      <td>1344</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>16128.00</td>\n",
       "      <td>91.00</td>\n",
       "      <td>16219.00</td>\n",
       "      <td>2026-01-31 07:19:33.437</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             booking_id listing_id             booking_date  \\\n",
       "0  0008e04a-d69f-448b-82f5-de35a4ffe0eb        229  2025-09-25 00:00:00.000   \n",
       "1  00127cd0-c342-4a32-b9ce-f9defb8932f4        461  2025-05-28 00:00:00.000   \n",
       "2  00186338-2229-4a7f-922d-1bc2adcd785f        455  2025-04-20 00:00:00.000   \n",
       "3  001bc7c8-0eea-412d-802c-b2c4d7136462        101  2025-10-06 00:00:00.000   \n",
       "4  001c3f21-9ecc-4a69-869f-261ee28e890f        238  2025-07-14 00:00:00.000   \n",
       "5  0045bcd6-a019-419c-b8eb-00d66dba626a        402  2025-07-31 00:00:00.000   \n",
       "6  00468a09-1ece-477c-95c2-721e850cf52d         89  2025-03-09 00:00:00.000   \n",
       "7  00516937-91bd-412a-acb7-cdc844e33ac3        157  2025-02-08 00:00:00.000   \n",
       "8  00531b5f-1010-4947-b5cf-11c259a16968         79  2025-03-10 00:00:00.000   \n",
       "9  006247af-0ac0-494f-b1a8-15b401d1056e        214  2025-07-15 00:00:00.000   \n",
       "\n",
       "  nights_booked booking_amount cleaning_fee service_fee booking_status  \\\n",
       "0            14           2912           76          27      cancelled   \n",
       "1            14            994           54          40      confirmed   \n",
       "2             4            964           22          49      confirmed   \n",
       "3             5           1345           26          37      confirmed   \n",
       "4             2            456           20          32      cancelled   \n",
       "5            11           3047           67          26      confirmed   \n",
       "6            14            742           23          46      confirmed   \n",
       "7             4            724           56          23      confirmed   \n",
       "8             5            530           44          10      confirmed   \n",
       "9            12           1344           59          32      cancelled   \n",
       "\n",
       "                created_at total_booking_amount additional_cost total_cost  \\\n",
       "0  2025-12-26 14:15:54.011             40768.00          103.00   40871.00   \n",
       "1  2025-12-26 14:15:54.011             13916.00           94.00   14010.00   \n",
       "2  2025-12-26 14:15:54.011              3856.00           71.00    3927.00   \n",
       "3  2025-12-26 14:15:54.011              6725.00           63.00    6788.00   \n",
       "4  2025-12-26 14:15:54.011               912.00           52.00     964.00   \n",
       "5  2025-12-26 14:15:54.011             33517.00           93.00   33610.00   \n",
       "6  2025-12-26 14:15:54.011             10388.00           69.00   10457.00   \n",
       "7  2025-12-26 14:15:54.011              2896.00           79.00    2975.00   \n",
       "8  2025-12-26 14:15:54.011              2650.00           54.00    2704.00   \n",
       "9  2025-12-26 14:15:54.011             16128.00           91.00   16219.00   \n",
       "\n",
       "              processed_at partition_0  \n",
       "0  2026-01-31 07:19:33.437    bookings  \n",
       "1  2026-01-31 07:19:33.437    bookings  \n",
       "2  2026-01-31 07:19:33.437    bookings  \n",
       "3  2026-01-31 07:19:33.437    bookings  \n",
       "4  2026-01-31 07:19:33.437    bookings  \n",
       "5  2026-01-31 07:19:33.437    bookings  \n",
       "6  2026-01-31 07:19:33.437    bookings  \n",
       "7  2026-01-31 07:19:33.437    bookings  \n",
       "8  2026-01-31 07:19:33.437    bookings  \n",
       "9  2026-01-31 07:19:33.437    bookings  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple SELECT with LIMIT\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM processed \n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'aws_full_pipeline_db')\n",
    "df = athena_results_to_dataframe(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qlsvhx2s6ye",
   "metadata": {},
   "source": [
    "### Filter with WHERE Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8e25e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'aws_full_pipeline_db'...\n",
      "Query: \n",
      "SELECT booking_date, nights_booked, booking_amount, booking_status\n",
      "FROM processed\n",
      "WHERE booking_sta...\n",
      "Query ID: 58118f6c-bd2a-4567-bcf1-76f28aef8f77\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.02 MB\n",
      "Execution time: 0.70 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 99\n",
      "Converted 99 rows to DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_date</th>\n",
       "      <th>nights_booked</th>\n",
       "      <th>booking_amount</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-25 00:00:00.000</td>\n",
       "      <td>14</td>\n",
       "      <td>2912</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-14 00:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>456</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-15 00:00:00.000</td>\n",
       "      <td>12</td>\n",
       "      <td>1344</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-23 00:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>406</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-06 00:00:00.000</td>\n",
       "      <td>12</td>\n",
       "      <td>3192</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2025-09-09 00:00:00.000</td>\n",
       "      <td>12</td>\n",
       "      <td>3216</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2025-02-21 00:00:00.000</td>\n",
       "      <td>10</td>\n",
       "      <td>1430</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2025-08-20 00:00:00.000</td>\n",
       "      <td>6</td>\n",
       "      <td>1302</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2025-10-25 00:00:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>445</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2025-10-20 00:00:00.000</td>\n",
       "      <td>10</td>\n",
       "      <td>1520</td>\n",
       "      <td>cancelled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               booking_date nights_booked booking_amount booking_status\n",
       "0   2025-09-25 00:00:00.000            14           2912      cancelled\n",
       "1   2025-07-14 00:00:00.000             2            456      cancelled\n",
       "2   2025-07-15 00:00:00.000            12           1344      cancelled\n",
       "3   2025-10-23 00:00:00.000             2            406      cancelled\n",
       "4   2025-08-06 00:00:00.000            12           3192      cancelled\n",
       "..                      ...           ...            ...            ...\n",
       "94  2025-09-09 00:00:00.000            12           3216      cancelled\n",
       "95  2025-02-21 00:00:00.000            10           1430      cancelled\n",
       "96  2025-08-20 00:00:00.000             6           1302      cancelled\n",
       "97  2025-10-25 00:00:00.000             5            445      cancelled\n",
       "98  2025-10-20 00:00:00.000            10           1520      cancelled\n",
       "\n",
       "[99 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SELECT with WHERE clause\n",
    "query = \"\"\"\n",
    "SELECT booking_date, nights_booked, booking_amount, booking_status\n",
    "FROM processed\n",
    "WHERE booking_status = 'cancelled'\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'aws_full_pipeline_db')\n",
    "df_cancelled = athena_results_to_dataframe(results)\n",
    "df_cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tip2pge1ygs",
   "metadata": {},
   "source": [
    "### Aggregation with GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86ce0e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'aws_full_pipeline_db'...\n",
      "Query: \n",
      "SELECT \n",
      "    DATE(booking_date) as date,\n",
      "    COUNT(*) as number_of_bookings\n",
      "FROM processed\n",
      "GROUP BY ...\n",
      "Query ID: cf88a41e-57fa-4155-a6e1-0f0f23878090\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.01 MB\n",
      "Execution time: 0.88 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 99\n",
      "Converted 99 rows to DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>number_of_bookings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-25</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-26</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-28</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-29</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2025-03-29</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2025-03-30</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date number_of_bookings\n",
       "0   2024-12-25                 18\n",
       "1   2024-12-26                 13\n",
       "2   2024-12-27                 12\n",
       "3   2024-12-28                 12\n",
       "4   2024-12-29                 12\n",
       "..         ...                ...\n",
       "94  2025-03-29                 15\n",
       "95  2025-03-30                 18\n",
       "96  2025-03-31                 11\n",
       "97  2025-04-01                  9\n",
       "98  2025-04-02                 14\n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COUNT and GROUP BY\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    DATE(booking_date) as date,\n",
    "    COUNT(*) as number_of_bookings\n",
    "FROM processed\n",
    "GROUP BY DATE(booking_date)\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'aws_full_pipeline_db')\n",
    "df_booking_counts = athena_results_to_dataframe(results)\n",
    "df_booking_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90t9pemyle",
   "metadata": {},
   "source": [
    "### GROUP BY with HAVING (Revenue Analysis)\n",
    "\n",
    "Find dates where cancelled bookings exceeded $200,000 - potential revenue loss to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90454068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'aws_full_pipeline_db'...\n",
      "Query: \n",
      "SELECT \n",
      "    booking_date,\n",
      "    COUNT(*) as number_of_bookings,\n",
      "    SUM(total_booking_amount) as tota...\n",
      "Query ID: f7935e86-4d8b-4f05-a5b0-472346ad1d2f\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.03 MB\n",
      "Execution time: 0.71 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 9\n",
      "Converted 9 rows to DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_date</th>\n",
       "      <th>number_of_bookings</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-01 00:00:00.000</td>\n",
       "      <td>14</td>\n",
       "      <td>308194.00</td>\n",
       "      <td>22013.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-27 00:00:00.000</td>\n",
       "      <td>13</td>\n",
       "      <td>302383.00</td>\n",
       "      <td>23260.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-28 00:00:00.000</td>\n",
       "      <td>12</td>\n",
       "      <td>252236.00</td>\n",
       "      <td>21019.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-21 00:00:00.000</td>\n",
       "      <td>11</td>\n",
       "      <td>230151.00</td>\n",
       "      <td>20922.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-14 00:00:00.000</td>\n",
       "      <td>10</td>\n",
       "      <td>221291.00</td>\n",
       "      <td>22129.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-10-25 00:00:00.000</td>\n",
       "      <td>13</td>\n",
       "      <td>213973.00</td>\n",
       "      <td>16459.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-01-10 00:00:00.000</td>\n",
       "      <td>9</td>\n",
       "      <td>212630.00</td>\n",
       "      <td>23625.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-04-20 00:00:00.000</td>\n",
       "      <td>9</td>\n",
       "      <td>208072.00</td>\n",
       "      <td>23119.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-04-30 00:00:00.000</td>\n",
       "      <td>13</td>\n",
       "      <td>204486.00</td>\n",
       "      <td>15729.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              booking_date number_of_bookings total_revenue  avg_sale\n",
       "0  2025-03-01 00:00:00.000                 14     308194.00  22013.86\n",
       "1  2025-01-27 00:00:00.000                 13     302383.00  23260.23\n",
       "2  2025-06-28 00:00:00.000                 12     252236.00  21019.67\n",
       "3  2025-11-21 00:00:00.000                 11     230151.00  20922.82\n",
       "4  2025-11-14 00:00:00.000                 10     221291.00  22129.10\n",
       "5  2025-10-25 00:00:00.000                 13     213973.00  16459.46\n",
       "6  2025-01-10 00:00:00.000                  9     212630.00  23625.56\n",
       "7  2025-04-20 00:00:00.000                  9     208072.00  23119.11\n",
       "8  2025-04-30 00:00:00.000                 13     204486.00  15729.69"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GROUP BY with HAVING, dates where total revenue from cancelled bookings > $200,000\n",
    "# This is a loss of revenue we may want to investigate\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    booking_date,\n",
    "    COUNT(*) as number_of_bookings,\n",
    "    SUM(total_booking_amount) as total_revenue,\n",
    "    ROUND(AVG(total_booking_amount), 2) as avg_sale\n",
    "FROM processed\n",
    "WHERE booking_status = 'cancelled'\n",
    "GROUP BY booking_date\n",
    "HAVING SUM(total_booking_amount) > 200000\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'aws_full_pipeline_db')\n",
    "df_revenue_by_status = athena_results_to_dataframe(results)\n",
    "df_revenue_by_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gkiph9b7x9",
   "metadata": {},
   "source": [
    "## 5.7 Cleanup Athena Results\n",
    "\n",
    "Delete old query results from S3 to manage storage costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87532cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Athena results older than 7 days...\n",
      "Cutoff date: 2026-01-24\n",
      "No old files to delete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def cleanup_athena_results(bucket, prefix='athena-results/', days_old=7):\n",
    "    \"\"\"\n",
    "    Delete Athena query results older than specified days\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): S3 bucket name\n",
    "        prefix (str): Prefix for Athena results folder\n",
    "        days_old (int): Delete files older than this many days\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of files deleted\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cutoff_date = datetime.now(tz=None) - timedelta(days=days_old)\n",
    "        \n",
    "        print(f\"Cleaning up Athena results older than {days_old} days...\")\n",
    "        print(f\"Cutoff date: {cutoff_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(\"No files found\")\n",
    "            return 0\n",
    "        \n",
    "        files_to_delete = []\n",
    "        for obj in response['Contents']:\n",
    "            # Remove timezone info for comparison\n",
    "            last_modified = obj['LastModified'].replace(tzinfo=None)\n",
    "            if last_modified < cutoff_date:\n",
    "                files_to_delete.append({'Key': obj['Key']})\n",
    "        \n",
    "        if not files_to_delete:\n",
    "            print(\"No old files to delete\")\n",
    "            return 0\n",
    "        \n",
    "        # Delete in batches of 1000 (S3 limit)\n",
    "        s3_client.delete_objects(\n",
    "            Bucket=bucket,\n",
    "            Delete={'Objects': files_to_delete[:1000]}\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Deleted {len(files_to_delete)} files\")\n",
    "        return len(files_to_delete)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Example (uncomment to use):\n",
    "cleanup_athena_results(S3_BUCKET_COMPLETE_PIPELINE, days_old=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8a2b4",
   "metadata": {},
   "source": [
    "# PHASE 7: AUTOMATE WITH LAMBDA\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 7: AUTOMATED PIPELINE WITH LAMBDA CHAIN                              │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   S3 Upload ──► Lambda 1 ──► EventBridge ──► Lambda 2 ──► EventBridge ──►  │\n",
    "│   (trigger)     (crawler)    (crawler done)  (ETL job)   (job done)        │\n",
    "│                                                                             │\n",
    "│                              ┌──────────────────────────────────────────┐   │\n",
    "│                              │ Lambda 3 ──► Processed Crawler ──► Done! │   │\n",
    "│                              └──────────────────────────────────────────┘   │\n",
    "│                                                                             │\n",
    "│   Lambda Functions:                                                         │\n",
    "│   1. start_raw_crawler      - Triggered by S3 upload                        │\n",
    "│   2. start_etl_job          - Triggered by crawler completion               │\n",
    "│   3. start_processed_crawler - Triggered by ETL job completion              │\n",
    "│                                                                             │\n",
    "│   EventBridge Rules:                                                        │\n",
    "│   • Glue Crawler State Change → Lambda 2                                    │\n",
    "│   • Glue Job State Change → Lambda 3                                        │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dce9bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Glue job 'bookings-etl-job'...\n",
      "  Script: s3://s3-complete-pipeline/scripts/aws_glue_etl.py\n",
      "  Type: glueetl\n",
      "SUCCESS: Job 'bookings-etl-job' created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the job exists before running this\n",
    "script_path = f's3://{S3_BUCKET_COMPLETE_PIPELINE}/scripts/aws_glue_etl.py'\n",
    "create_glue_job(\n",
    "    job_name='bookings-etl-job',\n",
    "    script_location=script_path,\n",
    "    description='Transform bookings data: null check, dedup, calculate totals'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d84c7",
   "metadata": {},
   "source": [
    "\n",
    "## 7.1 Initialize Lambda Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b9jyvucho6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda and EventBridge clients initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Lambda and EventBridge clients\n",
    "lambda_client = boto3.client(\n",
    "    'lambda',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "events_client = boto3.client(\n",
    "    'events',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "print(\"Lambda and EventBridge clients initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mq9w4cgolu",
   "metadata": {},
   "source": [
    "## 7.2 Create Lambda Function Helper\n",
    "\n",
    "Reusable function to create Lambda functions with inline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "jfyxo6r3fip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "\n",
    "def create_lambda_function(function_name, code_string, description='', timeout=300, memory=128):\n",
    "    \"\"\"\n",
    "    Create a Lambda function with inline Python code\n",
    "    \n",
    "    Args:\n",
    "        function_name (str): Function name\n",
    "        code_string (str): Python code as string\n",
    "        description (str): Function description\n",
    "        timeout (int): Timeout in seconds (max 900)\n",
    "        memory (int): Memory in MB\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if created successfully\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating Lambda function '{function_name}'...\")\n",
    "        \n",
    "        # Create zip file in memory\n",
    "        zip_buffer = io.BytesIO()\n",
    "        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "            zf.writestr('lambda_function.py', code_string)\n",
    "        zip_buffer.seek(0)\n",
    "        \n",
    "        lambda_client.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Runtime='python3.12',\n",
    "            Role=LAMBDA_ROLE_ARN,\n",
    "            Handler='lambda_function.lambda_handler',\n",
    "            Code={'ZipFile': zip_buffer.read()},\n",
    "            Description=description,\n",
    "            Timeout=timeout,\n",
    "            MemorySize=memory\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Lambda function '{function_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ResourceConflictException':\n",
    "            print(f\"Function '{function_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fpizxd6c",
   "metadata": {},
   "source": [
    "## 7.3 Lambda 1: Start Raw Crawler\n",
    "\n",
    "Triggered when a file is uploaded to `raw/` folder. Starts the Glue crawler to catalog new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "hch9ex9twvs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Lambda function 'pipeline-start-raw-crawler'...\n",
      "Function 'pipeline-start-raw-crawler' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lambda 1: Start Raw Crawler\n",
    "lambda1_code = '''\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Triggered by S3 upload to raw/ folder\n",
    "    Starts the Glue crawler for raw data\n",
    "    \"\"\"\n",
    "    glue = boto3.client(\"glue\")\n",
    "    crawler_name = \"full_pipeline_crawler\"\n",
    "    \n",
    "    print(f\"S3 event received: {json.dumps(event)}\")\n",
    "    \n",
    "    try:\n",
    "        glue.start_crawler(Name=crawler_name)\n",
    "        print(f\"Started crawler: {crawler_name}\")\n",
    "        return {\"statusCode\": 200, \"body\": f\"Started {crawler_name}\"}\n",
    "    except glue.exceptions.CrawlerRunningException:\n",
    "        print(f\"Crawler {crawler_name} is already running\")\n",
    "        return {\"statusCode\": 200, \"body\": \"Crawler already running\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {\"statusCode\": 500, \"body\": str(e)}\n",
    "'''\n",
    "\n",
    "create_lambda_function(\n",
    "    function_name='pipeline-start-raw-crawler',\n",
    "    code_string=lambda1_code,\n",
    "    description='Triggered by S3 upload - starts raw data crawler'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dzrsn0rvv4e",
   "metadata": {},
   "source": [
    "## 7.4 Lambda 2: Start ETL Job\n",
    "\n",
    "Triggered when raw crawler completes. Starts the Glue ETL job to transform data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "s0dbekkk3hp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Lambda function 'pipeline-start-etl-job'...\n",
      "Function 'pipeline-start-etl-job' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lambda 2: Start ETL Job\n",
    "lambda2_code = f'''\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Triggered by EventBridge when crawler completes\n",
    "    Starts the Glue ETL job\n",
    "    \"\"\"\n",
    "    glue = boto3.client(\"glue\")\n",
    "    job_name = \"bookings-etl-job\"\n",
    "    \n",
    "    print(f\"Crawler event received: {{json.dumps(event)}}\")\n",
    "    \n",
    "    # Check if it was the raw crawler that completed\n",
    "    crawler_name = event.get(\"detail\", {{}}).get(\"crawlerName\", \"\")\n",
    "    state = event.get(\"detail\", {{}}).get(\"state\", \"\")\n",
    "    \n",
    "    if crawler_name != \"full_pipeline_crawler\" or state != \"Succeeded\":\n",
    "        print(f\"Ignoring event: crawler={{crawler_name}}, state={{state}}\")\n",
    "        return {{\"statusCode\": 200, \"body\": \"Ignored\"}}\n",
    "    \n",
    "    try:\n",
    "        response = glue.start_job_run(\n",
    "            JobName=job_name,\n",
    "            Arguments={{\n",
    "                \"--S3_BUCKET\": \"{S3_BUCKET_COMPLETE_PIPELINE}\",\n",
    "                \"--SOURCE_PREFIX\": \"raw\",\n",
    "                \"--TARGET_PREFIX\": \"processed\"\n",
    "            }}\n",
    "        )\n",
    "        run_id = response[\"JobRunId\"]\n",
    "        print(f\"Started ETL job: {{job_name}}, RunId: {{run_id}}\")\n",
    "        return {{\"statusCode\": 200, \"body\": f\"Started {{job_name}}\"}}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {{str(e)}}\")\n",
    "        return {{\"statusCode\": 500, \"body\": str(e)}}\n",
    "'''\n",
    "\n",
    "create_lambda_function(\n",
    "    function_name='pipeline-start-etl-job',\n",
    "    code_string=lambda2_code,\n",
    "    description='Triggered by crawler completion - starts ETL job'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i448w2bzat",
   "metadata": {},
   "source": [
    "## 7.5 Lambda 3: Start Processed Crawler\n",
    "\n",
    "Triggered when ETL job completes. Starts the crawler for processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "j7xx1ql3m6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Lambda function 'pipeline-start-processed-crawler'...\n",
      "Function 'pipeline-start-processed-crawler' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lambda 3: Start Processed Crawler\n",
    "lambda3_code = '''\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Triggered by EventBridge when ETL job completes\n",
    "    Starts the crawler for processed data\n",
    "    \"\"\"\n",
    "    glue = boto3.client(\"glue\")\n",
    "    crawler_name = \"my_processed_data_crawler\"\n",
    "    \n",
    "    print(f\"Job event received: {json.dumps(event)}\")\n",
    "    \n",
    "    # Check if it was our ETL job that completed successfully\n",
    "    job_name = event.get(\"detail\", {}).get(\"jobName\", \"\")\n",
    "    state = event.get(\"detail\", {}).get(\"state\", \"\")\n",
    "    \n",
    "    if job_name != \"bookings-etl-job\" or state != \"SUCCEEDED\":\n",
    "        print(f\"Ignoring event: job={job_name}, state={state}\")\n",
    "        return {\"statusCode\": 200, \"body\": \"Ignored\"}\n",
    "    \n",
    "    try:\n",
    "        glue.start_crawler(Name=crawler_name)\n",
    "        print(f\"Started crawler: {crawler_name}\")\n",
    "        print(\"Pipeline complete! Data is ready for Athena queries.\")\n",
    "        return {\"statusCode\": 200, \"body\": f\"Started {crawler_name}\"}\n",
    "    except glue.exceptions.CrawlerRunningException:\n",
    "        print(f\"Crawler {crawler_name} is already running\")\n",
    "        return {\"statusCode\": 200, \"body\": \"Crawler already running\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {\"statusCode\": 500, \"body\": str(e)}\n",
    "'''\n",
    "\n",
    "create_lambda_function(\n",
    "    function_name='pipeline-start-processed-crawler',\n",
    "    code_string=lambda3_code,\n",
    "    description='Triggered by ETL completion - starts processed data crawler'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j2ns3ev2bo",
   "metadata": {},
   "source": [
    "## 7.6 Add S3 Trigger for Lambda 1\n",
    "\n",
    "Configure S3 bucket to trigger Lambda 1 when files are uploaded to `raw/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "rdqveeoc6u",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding S3 trigger to 'pipeline-start-raw-crawler'...\n",
      "  Bucket: s3-complete-pipeline\n",
      "  Prefix: raw/\n",
      "SUCCESS: S3 trigger added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_s3_trigger(function_name, bucket_name, prefix='raw/'):\n",
    "    \"\"\"\n",
    "    Add S3 trigger to Lambda function\n",
    "    \n",
    "    Args:\n",
    "        function_name (str): Lambda function name\n",
    "        bucket_name (str): S3 bucket name\n",
    "        prefix (str): S3 prefix to filter events\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get Lambda function ARN\n",
    "        response = lambda_client.get_function(FunctionName=function_name)\n",
    "        function_arn = response['Configuration']['FunctionArn']\n",
    "        \n",
    "        print(f\"Adding S3 trigger to '{function_name}'...\")\n",
    "        print(f\"  Bucket: {bucket_name}\")\n",
    "        print(f\"  Prefix: {prefix}\")\n",
    "        \n",
    "        # Add permission for S3 to invoke Lambda\n",
    "        try:\n",
    "            lambda_client.add_permission(\n",
    "                FunctionName=function_name,\n",
    "                StatementId=f's3-trigger-{bucket_name}',\n",
    "                Action='lambda:InvokeFunction',\n",
    "                Principal='s3.amazonaws.com',\n",
    "                SourceArn=f'arn:aws:s3:::{bucket_name}'\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            if 'ResourceConflictException' not in str(e):\n",
    "                raise\n",
    "        \n",
    "        # Configure S3 bucket notification\n",
    "        s3_client.put_bucket_notification_configuration(\n",
    "            Bucket=bucket_name,\n",
    "            NotificationConfiguration={\n",
    "                'LambdaFunctionConfigurations': [\n",
    "                    {\n",
    "                        'LambdaFunctionArn': function_arn,\n",
    "                        'Events': ['s3:ObjectCreated:*'],\n",
    "                        'Filter': {\n",
    "                            'Key': {\n",
    "                                'FilterRules': [\n",
    "                                    {'Name': 'prefix', 'Value': prefix}\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: S3 trigger added\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Add S3 trigger to Lambda 1\n",
    "add_s3_trigger('pipeline-start-raw-crawler', S3_BUCKET_COMPLETE_PIPELINE, 'raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8v6xsb4g24b",
   "metadata": {},
   "source": [
    "## 7.7 Add EventBridge Rules\n",
    "\n",
    "Create EventBridge rules to trigger Lambda 2 and 3 when Glue events occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7tq2my8kmwv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating EventBridge rule 'pipeline-crawler-complete'...\n",
      "SUCCESS: Rule 'pipeline-crawler-complete' created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def create_eventbridge_rule(rule_name, event_pattern, target_lambda, description=''):\n",
    "    \"\"\"\n",
    "    Create EventBridge rule to trigger Lambda function\n",
    "    \n",
    "    Args:\n",
    "        rule_name (str): Rule name\n",
    "        event_pattern (dict): Event pattern to match\n",
    "        target_lambda (str): Lambda function name to invoke\n",
    "        description (str): Rule description\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating EventBridge rule '{rule_name}'...\")\n",
    "        \n",
    "        # Get Lambda ARN\n",
    "        response = lambda_client.get_function(FunctionName=target_lambda)\n",
    "        lambda_arn = response['Configuration']['FunctionArn']\n",
    "        \n",
    "        # Create or update rule\n",
    "        events_client.put_rule(\n",
    "            Name=rule_name,\n",
    "            EventPattern=json.dumps(event_pattern),\n",
    "            State='ENABLED',\n",
    "            Description=description\n",
    "        )\n",
    "        \n",
    "        # Add Lambda as target\n",
    "        events_client.put_targets(\n",
    "            Rule=rule_name,\n",
    "            Targets=[\n",
    "                {\n",
    "                    'Id': f'{rule_name}-target',\n",
    "                    'Arn': lambda_arn\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Add permission for EventBridge to invoke Lambda\n",
    "        try:\n",
    "            lambda_client.add_permission(\n",
    "                FunctionName=target_lambda,\n",
    "                StatementId=f'eventbridge-{rule_name}',\n",
    "                Action='lambda:InvokeFunction',\n",
    "                Principal='events.amazonaws.com',\n",
    "                SourceArn=f'arn:aws:events:{AWS_REGION}:{lambda_arn.split(\":\")[4]}:rule/{rule_name}'\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            if 'ResourceConflictException' not in str(e):\n",
    "                raise\n",
    "        \n",
    "        print(f\"SUCCESS: Rule '{rule_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Rule 1: Crawler completion → Lambda 2 (Start ETL Job)\n",
    "crawler_complete_pattern = {\n",
    "    \"source\": [\"aws.glue\"],\n",
    "    \"detail-type\": [\"Glue Crawler State Change\"],\n",
    "    \"detail\": {\n",
    "        \"state\": [\"Succeeded\"],\n",
    "        \"crawlerName\": [\"full_pipeline_crawler\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "create_eventbridge_rule(\n",
    "    rule_name='pipeline-crawler-complete',\n",
    "    event_pattern=crawler_complete_pattern,\n",
    "    target_lambda='pipeline-start-etl-job',\n",
    "    description='Trigger ETL job when raw crawler completes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffwjcrkft3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating EventBridge rule 'pipeline-etl-complete'...\n",
      "SUCCESS: Rule 'pipeline-etl-complete' created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rule 2: ETL Job completion → Lambda 3 (Start Processed Crawler)\n",
    "job_complete_pattern = {\n",
    "    \"source\": [\"aws.glue\"],\n",
    "    \"detail-type\": [\"Glue Job State Change\"],\n",
    "    \"detail\": {\n",
    "        \"state\": [\"SUCCEEDED\"],\n",
    "        \"jobName\": [\"bookings-etl-job\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "create_eventbridge_rule(\n",
    "    rule_name='pipeline-etl-complete',\n",
    "    event_pattern=job_complete_pattern,\n",
    "    target_lambda='pipeline-start-processed-crawler',\n",
    "    description='Trigger processed crawler when ETL job completes'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rne8ejwae29",
   "metadata": {},
   "source": [
    "## 7.8 Test the Automated Pipeline\n",
    "\n",
    "Upload a file to `raw/` and watch the pipeline run automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cya6vca4stt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/lambda-bookings.csv' uploaded to 's3-complete-pipeline/raw/lambda-bookings.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test: Upload a file to trigger the pipeline\n",
    "# This will automatically:\n",
    "# 1. Trigger Lambda 1 → Start raw crawler\n",
    "# 2. Crawler completes → Lambda 2 → Start ETL job\n",
    "# 3. ETL completes → Lambda 3 → Start processed crawler\n",
    "# 4. Done! Data ready for Athena\n",
    "\n",
    "# Example: Re-upload bookings.csv to trigger the pipeline\n",
    "upload_file('data/lambda-bookings.csv', S3_BUCKET_COMPLETE_PIPELINE, 'raw/lambda-bookings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bfc0547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Crawlers:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Crawler: db_s3_crawler\n",
      "  State: READY\n",
      "  Database: glue_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: full_pipeline_crawler\n",
      "  State: READY\n",
      "  Database: aws_full_pipeline_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: my-crawler\n",
      "  State: READY\n",
      "  Database: data_engineering_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Crawler: my_processed_data_crawler\n",
      "  State: RUNNING\n",
      "  Database: aws_full_pipeline_db\n",
      "  Last Run: SUCCEEDED\n",
      "  Tables Created: 0\n",
      "  Tables Updated: 0\n",
      "\n",
      "Total: 4 crawler(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'db_s3_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://real-learn-s3/processed/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'glue_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'DEPRECATE_IN_DATABASE'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 0, 15, 47, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 0, 15, 47, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:db_s3_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: ad4a76dd-3d4f-4471-9a73-0b8c40264d90; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'db_s3_crawler',\n",
       "   'MessagePrefix': '59e7981a-85f4-4423-b698-0f8c29c94bf5',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 0, 20, 59, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'Configuration': '{\"Version\":1.0,\"CreatePartitionIndex\":true}',\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'full_pipeline_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://s3-complete-pipeline/raw/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 22, 10, 54, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 22, 10, 54, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:full_pipeline_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: 62615e29-63f0-43ee-a8b1-746e3c5f6bc7; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'full_pipeline_crawler',\n",
       "   'MessagePrefix': 'f3dab472-9253-4512-ab5e-2b491f6c868f',\n",
       "   'StartTime': datetime.datetime(2026, 1, 31, 2, 36, 27, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'my-crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://real-learn-s3/raw/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'data_engineering_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'READY',\n",
       "  'CrawlElapsedTime': 0,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 1, 25, 15, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 1, 25, 15, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:my-crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: 5c03d4eb-525f-4f08-b119-6309c565b76b; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'my-crawler',\n",
       "   'MessagePrefix': '6933cefa-b8c2-45d9-b00b-9a5f54546f3a',\n",
       "   'StartTime': datetime.datetime(2026, 1, 30, 1, 36, 25, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}},\n",
       " {'Name': 'my_processed_data_crawler',\n",
       "  'Role': 'glue-access-s3',\n",
       "  'Targets': {'S3Targets': [{'Path': 's3://s3-complete-pipeline/processed/',\n",
       "     'Exclusions': []}],\n",
       "   'JdbcTargets': [],\n",
       "   'MongoDBTargets': [],\n",
       "   'DynamoDBTargets': [],\n",
       "   'CatalogTargets': [],\n",
       "   'DeltaTargets': [],\n",
       "   'IcebergTargets': [],\n",
       "   'HudiTargets': []},\n",
       "  'DatabaseName': 'aws_full_pipeline_db',\n",
       "  'Classifiers': [],\n",
       "  'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'},\n",
       "  'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
       "   'DeleteBehavior': 'LOG'},\n",
       "  'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'},\n",
       "  'State': 'RUNNING',\n",
       "  'CrawlElapsedTime': 8297,\n",
       "  'CreationTime': datetime.datetime(2026, 1, 30, 23, 27, 49, tzinfo=tzlocal()),\n",
       "  'LastUpdated': datetime.datetime(2026, 1, 30, 23, 27, 49, tzinfo=tzlocal()),\n",
       "  'LastCrawl': {'Status': 'SUCCEEDED',\n",
       "   'ErrorMessage': 'Service Principal: glue.amazonaws.com is not authorized to perform: logs:PutLogEvents on resource: arn:aws:logs:us-east-2:445952351133:log-group:/aws-glue/crawlers:log-stream:my_processed_data_crawler because no identity-based policy allows the logs:PutLogEvents action (Service: AWSLogs; Status Code: 400; Error Code: AccessDeniedException; Request ID: af74fc22-3305-43b9-901a-a0ed3cbea308; Proxy: null). For more information, see Setting up IAM Permissions in the Developer Guide (http://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html).',\n",
       "   'LogGroup': '/aws-glue/crawlers',\n",
       "   'LogStream': 'my_processed_data_crawler',\n",
       "   'MessagePrefix': 'ca572774-1b46-4cb0-9d95-cc2e2974fdc2',\n",
       "   'StartTime': datetime.datetime(2026, 1, 31, 2, 20, 10, tzinfo=tzlocal())},\n",
       "  'Version': 1,\n",
       "  'LakeFormationConfiguration': {'UseLakeFormationCredentials': False,\n",
       "   'AccountId': ''}}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait ~2 min, then check crawler status\n",
    "list_crawlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "314d8484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Run Status for 'bookings-etl-job':\n",
      "--------------------------------------------------\n",
      "  Run ID: jr_487dc52fdbd95fa8338d3c84a9858649ee584f64ddc057cfd568a4530edb605d\n",
      "  State: SUCCEEDED\n",
      "  Started: 2026-01-31 02:36:20.379000-05:00\n",
      "  Completed: 2026-01-31 02:37:37.733000-05:00\n",
      "  Duration: 71 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Id': 'jr_487dc52fdbd95fa8338d3c84a9858649ee584f64ddc057cfd568a4530edb605d',\n",
       " 'Attempt': 0,\n",
       " 'JobName': 'bookings-etl-job',\n",
       " 'JobMode': 'SCRIPT',\n",
       " 'JobRunQueuingEnabled': False,\n",
       " 'StartedOn': datetime.datetime(2026, 1, 31, 2, 36, 20, 379000, tzinfo=tzlocal()),\n",
       " 'LastModifiedOn': datetime.datetime(2026, 1, 31, 2, 37, 37, 733000, tzinfo=tzlocal()),\n",
       " 'CompletedOn': datetime.datetime(2026, 1, 31, 2, 37, 37, 733000, tzinfo=tzlocal()),\n",
       " 'JobRunState': 'SUCCEEDED',\n",
       " 'Arguments': {'--S3_BUCKET': 's3-complete-pipeline',\n",
       "  '--SOURCE_PREFIX': 'raw',\n",
       "  '--TARGET_PREFIX': 'processed'},\n",
       " 'PredecessorRuns': [],\n",
       " 'AllocatedCapacity': 2,\n",
       " 'ExecutionTime': 71,\n",
       " 'Timeout': 60,\n",
       " 'MaxCapacity': 2.0,\n",
       " 'WorkerType': 'G.1X',\n",
       " 'NumberOfWorkers': 2,\n",
       " 'LogGroupName': '/aws-glue/jobs',\n",
       " 'GlueVersion': '4.0'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait ~3 min after crawler completes, check ETL job\n",
    "get_job_run_status('bookings-etl-job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a4260d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'aws_full_pipeline_db'...\n",
      "Query: \n",
      "SELECT * \n",
      "FROM processed \n",
      "LIMIT 100\n",
      "\n",
      "Query ID: f7d119bd-5439-40a9-8916-806e83177081\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.25 MB\n",
      "Execution time: 1.08 seconds\n",
      "Estimated cost: $0.000001\n",
      "Rows returned: 99\n",
      "Converted 99 rows to DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>booking_date</th>\n",
       "      <th>nights_booked</th>\n",
       "      <th>booking_amount</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>service_fee</th>\n",
       "      <th>booking_status</th>\n",
       "      <th>created_at</th>\n",
       "      <th>total_booking_amount</th>\n",
       "      <th>additional_cost</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>processed_at</th>\n",
       "      <th>partition_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008e04a-d69f-448b-82f5-de35a4ffe0eb</td>\n",
       "      <td>229</td>\n",
       "      <td>2025-09-25 00:00:00.000</td>\n",
       "      <td>14</td>\n",
       "      <td>2912</td>\n",
       "      <td>76</td>\n",
       "      <td>27</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>40768.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>40871.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001bc7c8-0eea-412d-802c-b2c4d7136462</td>\n",
       "      <td>101</td>\n",
       "      <td>2025-10-06 00:00:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>1345</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>6725.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>6788.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001c3f21-9ecc-4a69-869f-261ee28e890f</td>\n",
       "      <td>238</td>\n",
       "      <td>2025-07-14 00:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>456</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>912.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>964.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0045bcd6-a019-419c-b8eb-00d66dba626a</td>\n",
       "      <td>402</td>\n",
       "      <td>2025-07-31 00:00:00.000</td>\n",
       "      <td>11</td>\n",
       "      <td>3047</td>\n",
       "      <td>67</td>\n",
       "      <td>26</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>33517.00</td>\n",
       "      <td>93.00</td>\n",
       "      <td>33610.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00468a09-1ece-477c-95c2-721e850cf52d</td>\n",
       "      <td>89</td>\n",
       "      <td>2025-03-09 00:00:00.000</td>\n",
       "      <td>14</td>\n",
       "      <td>742</td>\n",
       "      <td>23</td>\n",
       "      <td>46</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>10388.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>10457.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>02b8b62b-9867-432d-82c7-b3f3a9a0620b</td>\n",
       "      <td>478</td>\n",
       "      <td>2025-09-23 00:00:00.000</td>\n",
       "      <td>11</td>\n",
       "      <td>550</td>\n",
       "      <td>58</td>\n",
       "      <td>19</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>6050.00</td>\n",
       "      <td>77.00</td>\n",
       "      <td>6127.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>02b8dc11-4922-4e8b-9d78-3b765501a639</td>\n",
       "      <td>484</td>\n",
       "      <td>2025-10-28 00:00:00.000</td>\n",
       "      <td>4</td>\n",
       "      <td>1200</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>4800.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>4853.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>02c7158f-dd42-425f-83c0-2919ecf7702f</td>\n",
       "      <td>413</td>\n",
       "      <td>2025-10-19 00:00:00.000</td>\n",
       "      <td>12</td>\n",
       "      <td>3300</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>39600.00</td>\n",
       "      <td>91.00</td>\n",
       "      <td>39691.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>02d9c5ba-7b07-488a-9774-72408d185507</td>\n",
       "      <td>235</td>\n",
       "      <td>2025-10-18 00:00:00.000</td>\n",
       "      <td>8</td>\n",
       "      <td>1256</td>\n",
       "      <td>52</td>\n",
       "      <td>36</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>10048.00</td>\n",
       "      <td>88.00</td>\n",
       "      <td>10136.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>02dfad85-47ab-40f8-b6e3-b7ef0b3190b9</td>\n",
       "      <td>406</td>\n",
       "      <td>2025-07-24 00:00:00.000</td>\n",
       "      <td>13</td>\n",
       "      <td>2717</td>\n",
       "      <td>71</td>\n",
       "      <td>50</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2025-12-26 14:15:54.011</td>\n",
       "      <td>35321.00</td>\n",
       "      <td>121.00</td>\n",
       "      <td>35442.00</td>\n",
       "      <td>2026-01-31 07:37:22.324</td>\n",
       "      <td>bookings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              booking_id listing_id             booking_date  \\\n",
       "0   0008e04a-d69f-448b-82f5-de35a4ffe0eb        229  2025-09-25 00:00:00.000   \n",
       "1   001bc7c8-0eea-412d-802c-b2c4d7136462        101  2025-10-06 00:00:00.000   \n",
       "2   001c3f21-9ecc-4a69-869f-261ee28e890f        238  2025-07-14 00:00:00.000   \n",
       "3   0045bcd6-a019-419c-b8eb-00d66dba626a        402  2025-07-31 00:00:00.000   \n",
       "4   00468a09-1ece-477c-95c2-721e850cf52d         89  2025-03-09 00:00:00.000   \n",
       "..                                   ...        ...                      ...   \n",
       "94  02b8b62b-9867-432d-82c7-b3f3a9a0620b        478  2025-09-23 00:00:00.000   \n",
       "95  02b8dc11-4922-4e8b-9d78-3b765501a639        484  2025-10-28 00:00:00.000   \n",
       "96  02c7158f-dd42-425f-83c0-2919ecf7702f        413  2025-10-19 00:00:00.000   \n",
       "97  02d9c5ba-7b07-488a-9774-72408d185507        235  2025-10-18 00:00:00.000   \n",
       "98  02dfad85-47ab-40f8-b6e3-b7ef0b3190b9        406  2025-07-24 00:00:00.000   \n",
       "\n",
       "   nights_booked booking_amount cleaning_fee service_fee booking_status  \\\n",
       "0             14           2912           76          27      cancelled   \n",
       "1              5           1345           26          37      confirmed   \n",
       "2              2            456           20          32      cancelled   \n",
       "3             11           3047           67          26      confirmed   \n",
       "4             14            742           23          46      confirmed   \n",
       "..           ...            ...          ...         ...            ...   \n",
       "94            11            550           58          19      confirmed   \n",
       "95             4           1200           27          26      confirmed   \n",
       "96            12           3300           45          46      confirmed   \n",
       "97             8           1256           52          36      confirmed   \n",
       "98            13           2717           71          50      confirmed   \n",
       "\n",
       "                 created_at total_booking_amount additional_cost total_cost  \\\n",
       "0   2025-12-26 14:15:54.011             40768.00          103.00   40871.00   \n",
       "1   2025-12-26 14:15:54.011              6725.00           63.00    6788.00   \n",
       "2   2025-12-26 14:15:54.011               912.00           52.00     964.00   \n",
       "3   2025-12-26 14:15:54.011             33517.00           93.00   33610.00   \n",
       "4   2025-12-26 14:15:54.011             10388.00           69.00   10457.00   \n",
       "..                      ...                  ...             ...        ...   \n",
       "94  2025-12-26 14:15:54.011              6050.00           77.00    6127.00   \n",
       "95  2025-12-26 14:15:54.011              4800.00           53.00    4853.00   \n",
       "96  2025-12-26 14:15:54.011             39600.00           91.00   39691.00   \n",
       "97  2025-12-26 14:15:54.011             10048.00           88.00   10136.00   \n",
       "98  2025-12-26 14:15:54.011             35321.00          121.00   35442.00   \n",
       "\n",
       "               processed_at partition_0  \n",
       "0   2026-01-31 07:37:22.324    bookings  \n",
       "1   2026-01-31 07:37:22.324    bookings  \n",
       "2   2026-01-31 07:37:22.324    bookings  \n",
       "3   2026-01-31 07:37:22.324    bookings  \n",
       "4   2026-01-31 07:37:22.324    bookings  \n",
       "..                      ...         ...  \n",
       "94  2026-01-31 07:37:22.324    bookings  \n",
       "95  2026-01-31 07:37:22.324    bookings  \n",
       "96  2026-01-31 07:37:22.324    bookings  \n",
       "97  2026-01-31 07:37:22.324    bookings  \n",
       "98  2026-01-31 07:37:22.324    bookings  \n",
       "\n",
       "[99 rows x 14 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple SELECT with LIMIT\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM processed \n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'aws_full_pipeline_db')\n",
    "df = athena_results_to_dataframe(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yfbq3d291h9",
   "metadata": {},
   "source": [
    "## 7.9 Cleanup Functions\n",
    "\n",
    "Delete Lambda functions and EventBridge rules when no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m774gtzpbmq",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lambda_function(function_name):\n",
    "    \"\"\"Delete a Lambda function\"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting Lambda function '{function_name}'...\")\n",
    "        lambda_client.delete_function(FunctionName=function_name)\n",
    "        print(f\"SUCCESS: Function '{function_name}' deleted\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            print(f\"Function '{function_name}' not found\")\n",
    "        else:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def delete_eventbridge_rule(rule_name):\n",
    "    \"\"\"Delete an EventBridge rule and its targets\"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting EventBridge rule '{rule_name}'...\")\n",
    "        \n",
    "        # Remove targets first\n",
    "        events_client.remove_targets(Rule=rule_name, Ids=[f'{rule_name}-target'])\n",
    "        \n",
    "        # Delete rule\n",
    "        events_client.delete_rule(Name=rule_name)\n",
    "        \n",
    "        print(f\"SUCCESS: Rule '{rule_name}' deleted\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            print(f\"Rule '{rule_name}' not found\")\n",
    "        else:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example (uncomment to cleanup):\n",
    "# delete_lambda_function('pipeline-start-raw-crawler')\n",
    "# delete_lambda_function('pipeline-start-etl-job')\n",
    "# delete_lambda_function('pipeline-start-processed-crawler')\n",
    "# delete_eventbridge_rule('pipeline-crawler-complete')\n",
    "# delete_eventbridge_rule('pipeline-etl-complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
