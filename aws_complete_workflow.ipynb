{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb1445a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "699b71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "GLUE_ROLE_ARN = os.getenv(\"GLUE_ROLE_ARN\")\n",
    "LAMBDA_ROLE_ARN = os.getenv(\"LAMBDA_ROLE_ARN\")\n",
    "S3_BUCKET_COMPLETE_PIPELINE = os.getenv(\"S3_BUCKET_COMPLETE_PIPELINE\")\n",
    "S3_UPLOAD_FOLDER = \"raw/\"\n",
    "S3_PROCESSED_FOLDER = \"processed/\"\n",
    "S3_GLUE_OUTPUT_FOLDER = \"glue-output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca8f71",
   "metadata": {},
   "source": [
    "# COMPLETE DATA LAKE WORKFLOW\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                        COMPLETE DATA LAKE PIPELINE                                |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                                                                                   |\n",
    "|  1. INGEST                                                                        |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Upload to S3      |  <-- Manual, Lambda, Kinesis Firehose                   |\n",
    "|     | (raw/ folder)     |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  2. CATALOG                                                                       |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Glue Crawler      |  --> Discovers schema, creates tables                   |\n",
    "|     | (Data Catalog)    |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  3. TRANSFORM                                                                     |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Glue ETL Job      |  --> Clean, transform, convert to Parquet               |\n",
    "|     | (processed/)      |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  4. CATALOG PROCESSED                                                             |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Another Crawler   |  --> Update catalog with processed tables               |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  5. ANALYZE                                                                       |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Athena SQL        |  --> Fast, serverless analytics                         |\n",
    "|     | Queries           |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  6. VISUALIZE (Optional)                                                          |\n",
    "|     +-------------------+                                                         |\n",
    "|     | QuickSight        |  --> Dashboards, reports                                |\n",
    "|     +-------------------+                                                         |\n",
    "|                                                                                   |\n",
    "|  All serverless, managed, pay-per-use!                                            |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e327485",
   "metadata": {},
   "source": [
    "# PHASE 1: DATA INGESTION\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STEP 1: INGEST DATA TO S3                                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│   Local Files ──────┐                                                       │\n",
    "│                     │                                                       │\n",
    "│   APIs/Streams ─────┼────►  S3 Bucket  ────►  raw/ folder                   │\n",
    "│                     │      (Landing Zone)                                   │\n",
    "│   Lambda Events ────┘                                                       │\n",
    "│                                                                             │\n",
    "│   Key Operations:                                                           │\n",
    "│   • Create S3 bucket (landing zone)                                         │\n",
    "│   • Upload raw data files                                                   │\n",
    "│   • Verify uploads & read content                                           │\n",
    "│   • Generate presigned URLs for sharing                                     │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 1.1 Initialize S3 Client\n",
    "\n",
    "Configure the S3 client with **Signature Version 4** (required for presigned URLs) and regional endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f414ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an s3 client\n",
    "from botocore.config import Config\n",
    "\n",
    "# Configure S3 client with Signature Version 4 and regional endpoint\n",
    "# Regional endpoint is required for presigned URLs to work correctly\n",
    "\n",
    "s3_client = boto3.client('s3', \n",
    "                         endpoint_url=f'https://s3.{AWS_REGION}.amazonaws.com',\n",
    "                         config=Config(signature_version='s3v4'),\n",
    "                         region_name=AWS_REGION,\n",
    "                         aws_secret_access_key=SECRET_KEY,\n",
    "                         aws_access_key_id=ACCESS_KEY,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05809552",
   "metadata": {},
   "source": [
    "## 1.2 Create S3 Bucket (Landing Zone)\n",
    "\n",
    "The bucket serves as the **landing zone** for all raw data. All incoming files go to the `raw/` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "259c235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Bucket 's3-complete-pipeline' already exists and is owned by you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in a specified region\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): Name for the bucket (must be globally unique)\n",
    "        region (str): AWS region (if None, uses default from client)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if bucket created, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region is None or region == 'us-east-1':\n",
    "            # us-east-1 doesn't require LocationConstraint\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            # Other regions require LocationConstraint\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"SUCCESS: Bucket '{bucket_name}' created successfully\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'BucketAlreadyExists':\n",
    "            print(f\"ERROR: Bucket '{bucket_name}' already exists (owned by someone else)\")\n",
    "        elif error_code == 'BucketAlreadyOwnedByYou':\n",
    "            print(f\"INFO: Bucket '{bucket_name}' already exists and is owned by you\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create bucket - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "create_bucket(S3_BUCKET_COMPLETE_PIPELINE, region=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8bbb1",
   "metadata": {},
   "source": [
    "## 1.3 Upload Raw Data\n",
    "\n",
    "Upload files to the `raw/` folder. This is where Glue Crawlers will discover and catalog the data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aff822e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 'data/hosts.csv' uploaded to 's3-complete-pipeline/raw/hosts.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload a file to s3\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to file to upload\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (if None, uses file_name)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    # If S3 object_name not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print(f\"SUCCESS: '{file_name}' uploaded to '{bucket}/{object_name}'\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File '{file_name}' not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to upload file - {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "upload_file('data/hosts.csv', S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699d30b",
   "metadata": {},
   "source": [
    "## 1.4 Verify Upload (Read Content)\n",
    "\n",
    "Read the uploaded file directly from S3 to confirm the data landed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40690886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Read 13083 characters from 's3-complete-pipeline/raw/hosts.csv'\n",
      "Content:\n",
      "host_id,host_name,host_since,is_superhost,response_rate,created_at\n",
      "1,Timothy Parker,2018-03-20,False,99,2025-12-26 14:15:54.011160\n",
      "2,Hannah Evans,2024-01-01,False,95,2025-12-26 14:15:54.011160\n",
      "3,Crystal Green,2016-08-06,False,74,2025-12-26 14:15:54.011160\n",
      "4,Kevin Johnson,2020-02-25,False,100,2025-12-26 14:15:54.011160\n",
      "5,Monica Johnson,2024-11-11,False,77,2025-12-26 14:15:54.011160\n",
      "6,Nancy Turner,2016-11-03,False,96,2025-12-26 14:15:54.011160\n",
      "7,Gerald Hunt,2022-01-21,True,99,2025-12-26 14:\n"
     ]
    }
   ],
   "source": [
    "def read_object(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Read S3 object content directly into memory\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name to read\n",
    "    \n",
    "    Returns:\n",
    "        str: File content as string, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=object_name)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        print(f\"SUCCESS: Read {len(content)} characters from '{bucket}/{object_name}'\")\n",
    "        return content\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'NoSuchKey':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to read object - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "content = read_object(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')\n",
    "if content:\n",
    "    print(\"Content:\")\n",
    "    print(content[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d883c",
   "metadata": {},
   "source": [
    "## 1.5 List Objects in Bucket\n",
    "\n",
    "View all objects in the `raw/` folder with detailed metadata (size, last modified, storage class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5fca925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in 's3-complete-pipeline/raw/':\n",
      "Key: raw/hosts.csv\n",
      "Size: 12.78 KB (13,083 bytes)\n",
      "Last Modified: 2026-01-31 03:02:32\n",
      "Storage Class: STANDARD\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Total: 1 object(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Key': 'raw/hosts.csv',\n",
       "  'LastModified': datetime.datetime(2026, 1, 31, 3, 2, 32, tzinfo=tzutc()),\n",
       "  'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 13083,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def list_objects_detailed(bucket, prefix=''):\n",
    "    \"\"\"\n",
    "    List all objects in a bucket with detailed metadata\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        prefix (str): Filter objects by prefix (folder path)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of object metadata dictionaries, or empty list if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(f\"No objects found in bucket '{bucket}' with prefix '{prefix}'\")\n",
    "            return []\n",
    "        \n",
    "        objects = []\n",
    "        print(f\"Objects in '{bucket}/{prefix}':\")\n",
    "        \n",
    "        for obj in response['Contents']:\n",
    "            # Convert size to human-readable format\n",
    "            size_bytes = obj['Size']\n",
    "            if size_bytes < 1024:\n",
    "                size_str = f\"{size_bytes} B\"\n",
    "            elif size_bytes < 1024**2:\n",
    "                size_str = f\"{size_bytes/1024:.2f} KB\"\n",
    "            else:\n",
    "                size_str = f\"{size_bytes/(1024**2):.2f} MB\"\n",
    "            \n",
    "            # Format last modified date\n",
    "            last_modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            print(f\"Key: {obj['Key']}\")\n",
    "            print(f\"Size: {size_str} ({size_bytes:,} bytes)\")\n",
    "            print(f\"Last Modified: {last_modified}\")\n",
    "            print(f\"Storage Class: {obj.get('StorageClass', 'STANDARD')}\")\n",
    "            print(f\"ETag: {obj['ETag']}\")\n",
    "            \n",
    "            objects.append(obj)\n",
    "        \n",
    "        print(f\"Total: {len(objects)} object(s)\")\n",
    "        return objects\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to list objects - {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "list_objects_detailed(S3_BUCKET_COMPLETE_PIPELINE, prefix=f'{S3_UPLOAD_FOLDER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1895a2",
   "metadata": {},
   "source": [
    "## 1.6 Get Object Metadata\n",
    "\n",
    "Retrieve detailed metadata for a specific object (content type, encryption, custom tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b907126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 's3-complete-pipeline/raw/hosts.csv':\n",
      "SYSTEM METADATA:\n",
      "Content-Type: binary/octet-stream\n",
      "Content-Length: 13,083 bytes\n",
      "Last-Modified: 2026-01-31 03:02:32+00:00\n",
      "ETag: \"7588197a4f4c485949e7bfc641356122\"\n",
      "Storage-Class: STANDARD\n",
      "USER METADATA: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'W1QD945F3JFZGWF0',\n",
       "  'HostId': 'JDY8bqiBgPQQdc9mhRexUjWGk+L4itRZrVyrD+77LMFXx7seuBMPzgEeN6dAuj4WDzNYLB4zhws=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'JDY8bqiBgPQQdc9mhRexUjWGk+L4itRZrVyrD+77LMFXx7seuBMPzgEeN6dAuj4WDzNYLB4zhws=',\n",
       "   'x-amz-request-id': 'W1QD945F3JFZGWF0',\n",
       "   'date': 'Sat, 31 Jan 2026 03:03:04 GMT',\n",
       "   'last-modified': 'Sat, 31 Jan 2026 03:02:32 GMT',\n",
       "   'etag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'content-length': '13083',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2026, 1, 31, 3, 2, 32, tzinfo=tzutc()),\n",
       " 'ContentLength': 13083,\n",
       " 'ETag': '\"7588197a4f4c485949e7bfc641356122\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_object_metadata(bucket, object_name):\n",
    "    \"\"\"\n",
    "    Retrieve metadata for an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata dictionary, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=object_name)\n",
    "        \n",
    "        print(f\"Metadata for '{bucket}/{object_name}':\")\n",
    "        \n",
    "        # System metadata\n",
    "        print(\"SYSTEM METADATA:\")\n",
    "        print(f\"Content-Type: {response.get('ContentType', 'N/A')}\")\n",
    "        print(f\"Content-Length: {response.get('ContentLength', 0):,} bytes\")\n",
    "        print(f\"Last-Modified: {response.get('LastModified', 'N/A')}\")\n",
    "        print(f\"ETag: {response.get('ETag', 'N/A')}\")\n",
    "        print(f\"Storage-Class: {response.get('StorageClass', 'STANDARD')}\")\n",
    "        \n",
    "        # User metadata (custom)\n",
    "        user_metadata = response.get('Metadata', {})\n",
    "        if user_metadata:\n",
    "            print(\"USER METADATA (Custom):\")\n",
    "            for key, value in user_metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(\"USER METADATA: None\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"ERROR: Object '{object_name}' not found in bucket '{bucket}'\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to get metadata - {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "get_object_metadata(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89bf53",
   "metadata": {},
   "source": [
    "## 1.7 Generate Presigned URL (Share Data)\n",
    "\n",
    "Create time-limited URLs for secure sharing without exposing AWS credentials.\n",
    "\n",
    "| Expiration | Use Case |\n",
    "|------------|----------|\n",
    "| 600s (10 min) | Quick one-time downloads |\n",
    "| 3600s (1 hour) | Team collaboration |\n",
    "| 86400s (24 hours) | External sharing |\n",
    "| 604800s (7 days) | Maximum allowed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9acbab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Presigned URL generated for 's3-complete-pipeline/raw/hosts.csv'\n",
      "Expires in: 600 seconds (0.2 hours)\n",
      "\n",
      "URL (valid for 0.2 hours):\n",
      "https://s3.us-east-2.amazonaws.com/s3-complete-pipeline/raw/hosts.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAWPVGU3OO6BQJAVE3%2F20260131%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20260131T030321Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Signature=32c0c87db9fa04e5c387a2c5e06e1ff6d693ac6fa3b0b96553197a457e9e52ae\n",
      "\n",
      "Anyone with this URL can download the file until it expires.\n"
     ]
    }
   ],
   "source": [
    "def generate_presigned_download_url(bucket, object_name, expiration=3600):\n",
    "    \"\"\"\n",
    "    Generate a presigned URL for downloading an S3 object\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): Bucket name\n",
    "        object_name (str): S3 object name (key)\n",
    "        expiration (int): URL expiration time in seconds (default 3600 = 1 hour)\n",
    "    \n",
    "    Returns:\n",
    "        str: Presigned URL, or None if error\n",
    "    \n",
    "    Common expiration times:\n",
    "        - 3600 = 1 hour (default)\n",
    "        - 7200 = 2 hours\n",
    "        - 86400 = 24 hours\n",
    "        - 604800 = 7 days (maximum)\n",
    "    \n",
    "    Note: Uses AWS Signature Version 4 (required by S3)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = s3_client.generate_presigned_url(\n",
    "            'get_object',\n",
    "            Params={\n",
    "                'Bucket': bucket,\n",
    "                'Key': object_name\n",
    "            },\n",
    "            ExpiresIn=expiration\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Presigned URL generated for '{bucket}/{object_name}'\")\n",
    "        print(f\"Expires in: {expiration} seconds ({expiration/3600:.1f} hours)\")\n",
    "        print(f\"\\nURL (valid for {expiration/3600:.1f} hours):\")\n",
    "        print(url)\n",
    "        print(\"\\nAnyone with this URL can download the file until it expires.\")\n",
    "        \n",
    "        return url\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: Failed to generate presigned URL - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "url = generate_presigned_download_url(S3_BUCKET_COMPLETE_PIPELINE, f'{S3_UPLOAD_FOLDER}hosts.csv', expiration=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e359c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
