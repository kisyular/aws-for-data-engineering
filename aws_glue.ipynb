{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-main",
   "metadata": {},
   "source": [
    "# AWS Glue - Comprehensive Guide for Data Engineers\n",
    "\n",
    "This notebook provides a complete walkthrough of AWS Glue, Amazon's serverless ETL (Extract, Transform, Load) service.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **Introduction & Core Concepts** - What is Glue, architecture, components\n",
    "2. **Setup & Prerequisites** - IAM roles, boto3 configuration\n",
    "3. **Data Catalog Operations** - Databases, tables, metadata\n",
    "4. **Crawlers** - Schema discovery and management\n",
    "5. **ETL Jobs** - Spark and Python Shell jobs\n",
    "6. **Advanced Topics** - Workflows, triggers, optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1-intro",
   "metadata": {},
   "source": [
    "## PHASE 1: INTRODUCTION & CORE CONCEPTS\n",
    "\n",
    "### What is AWS Glue?\n",
    "\n",
    "AWS Glue is a **fully managed, serverless ETL service** that makes it easy to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning, and application development.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Serverless**: No infrastructure to manage\n",
    "- **Pay-per-use**: Charged only for resources consumed during job runs\n",
    "- **Scalable**: Automatically scales based on workload\n",
    "- **Integrated**: Works seamlessly with S3, RDS, Redshift, Athena\n",
    "\n",
    "---\n",
    "\n",
    "### AWS Glue vs Traditional ETL\n",
    "\n",
    "| Aspect | Traditional ETL | AWS Glue |\n",
    "|--------|-----------------|----------|\n",
    "| Infrastructure | Self-managed servers | Serverless |\n",
    "| Scaling | Manual capacity planning | Automatic |\n",
    "| Cost Model | Fixed infrastructure costs | Pay-per-use |\n",
    "| Setup Time | Days to weeks | Minutes |\n",
    "| Schema Management | Manual | Automated (Crawlers) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-diagram",
   "metadata": {},
   "source": [
    "### AWS Glue Architecture Overview\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                              AWS GLUE ARCHITECTURE                                |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                                                                                   |\n",
    "|   DATA SOURCES                      AWS GLUE                      DATA TARGETS    |\n",
    "|   +-------------+                                                +-------------+  |\n",
    "|   |    S3       |                                                |    S3       |  |\n",
    "|   | (Data Lake) |---+                                        +---|  (Parquet)  |  |\n",
    "|   +-------------+   |                                        |   +-------------+  |\n",
    "|                     |    +---------------------------+       |                    |\n",
    "|   +-------------+   |    |                           |       |   +-------------+  |\n",
    "|   |    RDS      |---+--->|      ETL JOBS             |-------+-->|  Redshift   |  |\n",
    "|   | (MySQL/PG)  |   |    |   (Spark / Python)        |       |   | (Warehouse) |  |\n",
    "|   +-------------+   |    |                           |       |   +-------------+  |\n",
    "|                     |    +---------------------------+       |                    |\n",
    "|   +-------------+   |              |                         |   +-------------+  |\n",
    "|   |  DynamoDB   |---+              |                         +-->|  Athena     |  |\n",
    "|   +-------------+                  v                             | (Analytics) |  |\n",
    "|                     +---------------------------+                +-------------+  |\n",
    "|                     |      DATA CATALOG         |                                 |\n",
    "|                     |  (Metadata Repository)    |<----+                           |\n",
    "|                     +---------------------------+     |                           |\n",
    "|                                    ^                  |                           |\n",
    "|                     +---------------------------+     |                           |\n",
    "|                     |        CRAWLERS           |-----+                           |\n",
    "|                     |   (Schema Discovery)      |                                 |\n",
    "|                     +---------------------------+                                 |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-components",
   "metadata": {},
   "source": [
    "### Key Components of AWS Glue\n",
    "\n",
    "#### 1. Data Catalog\n",
    "The **central metadata repository** that stores table definitions, job definitions, and other control information.\n",
    "\n",
    "```\n",
    "+------------------------------------------+\n",
    "|            DATA CATALOG                  |\n",
    "+------------------------------------------+\n",
    "|  +----------------+  +----------------+  |\n",
    "|  |   Database A   |  |   Database B   |  |\n",
    "|  +----------------+  +----------------+  |\n",
    "|  | - Table 1      |  | - Table 1      |  |\n",
    "|  |   - Columns    |  |   - Columns    |  |\n",
    "|  |   - Location   |  |   - Location   |  |\n",
    "|  |   - Format     |  |   - Format     |  |\n",
    "|  | - Table 2      |  | - Table 2      |  |\n",
    "|  +----------------+  +----------------+  |\n",
    "+------------------------------------------+\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Compatible with Apache Hive Metastore\n",
    "- Accessible from Athena, EMR, Redshift Spectrum\n",
    "- Stores schema, location, and properties of data\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Crawlers\n",
    "Automated programs that **scan your data sources** and populate the Data Catalog.\n",
    "\n",
    "```\n",
    "DATA SOURCE          CRAWLER              DATA CATALOG\n",
    "+--------+        +----------+          +------------+\n",
    "|   S3   | -----> | Analyze  | -------> |   Table    |\n",
    "| (CSV)  |        | Schema   |          | Definition |\n",
    "+--------+        +----------+          +------------+\n",
    "                       |\n",
    "                       v\n",
    "               Infers columns,\n",
    "               data types,\n",
    "               partitions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. ETL Jobs\n",
    "The actual **data transformation workloads**.\n",
    "\n",
    "| Type | Engine | Use Case | Min Resources |\n",
    "|------|--------|----------|---------------|\n",
    "| Spark | Apache Spark | Large-scale data | 2 DPUs |\n",
    "| Python Shell | Python | Small datasets | 0.0625 DPU |\n",
    "| Streaming | Spark Streaming | Real-time | 2 DPUs |\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Workflows\n",
    "Orchestrate multiple crawlers and jobs into a pipeline.\n",
    "\n",
    "```\n",
    "+--------+     +--------+     +--------+     +--------+\n",
    "| Start  |---->|Crawler |---->|  Job   |---->|  Job   |\n",
    "+--------+     +--------+     +--------+     +--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-use",
   "metadata": {},
   "source": [
    "### When to Use AWS Glue\n",
    "\n",
    "**Good Fit:**\n",
    "- Building data lakes on S3\n",
    "- Cataloging data across multiple sources\n",
    "- Batch ETL processing (hourly, daily)\n",
    "- Schema discovery and management\n",
    "- Data preparation for Athena/Redshift\n",
    "\n",
    "**Consider Alternatives When:**\n",
    "- Real-time with sub-second latency (use Kinesis)\n",
    "- Simple file transfers (use DataSync)\n",
    "- Very small datasets (Lambda might be cheaper)\n",
    "\n",
    "---\n",
    "\n",
    "### Cost Model\n",
    "\n",
    "| Component | Pricing |\n",
    "|-----------|--------|\n",
    "| ETL Jobs | $0.44 per DPU-hour |\n",
    "| Crawlers | $0.44 per DPU-hour |\n",
    "| Data Catalog | Free (first 1M objects) |\n",
    "\n",
    "**1 DPU = 4 vCPUs + 16 GB memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2-setup",
   "metadata": {},
   "source": [
    "## PHASE 2: SETUP & PREREQUISITES\n",
    "\n",
    "### IAM Role Requirements\n",
    "\n",
    "AWS Glue requires an IAM role with:\n",
    "\n",
    "```\n",
    "+------------------------------------------+\n",
    "|          GLUE IAM ROLE                   |\n",
    "+------------------------------------------+\n",
    "|  Trust Policy:                           |\n",
    "|  - glue.amazonaws.com can assume role    |\n",
    "|                                          |\n",
    "|  Permissions:                            |\n",
    "|  - AWSGlueServiceRole (managed policy)   |\n",
    "|  - S3 access to your data buckets        |\n",
    "|  - CloudWatch Logs for logging           |\n",
    "+------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup-credentials",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-2\n",
      "Bucket: real-learn-s3\n"
     ]
    }
   ],
   "source": [
    "# Environment Configuration\n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "BUCKET_NAME = os.getenv(\"AWS_BUCKET_NAME\")\n",
    "GLUE_ROLE_ARN = os.getenv(\"GLUE_ROLE_ARN\")  # Add to your .env file\n",
    "\n",
    "print(f\"Region: {AWS_REGION}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup-clients",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Glue Client\n",
    "glue_client = boto3.client(\n",
    "    'glue',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "# Helper function for security\n",
    "def redact_account_id(arn):\n",
    "    \"\"\"Redact AWS account ID from ARN\"\"\"\n",
    "    return re.sub(r':\\d{12}:', ':************:', str(arn))\n",
    "\n",
    "print(\"Glue client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3-catalog",
   "metadata": {},
   "source": [
    "## PHASE 3: DATA CATALOG OPERATIONS\n",
    "\n",
    "### Hierarchy\n",
    "\n",
    "```\n",
    "Data Catalog\n",
    "    +-- Database (logical grouping)\n",
    "    |       +-- Table (metadata definition)\n",
    "    |       |       +-- Columns (name, type)\n",
    "    |       |       +-- Location (S3 path)\n",
    "    |       |       +-- Format (CSV, Parquet)\n",
    "    |       |       +-- Partitions (optional)\n",
    "    |       +-- Table\n",
    "    +-- Database\n",
    "            +-- Table\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Function 1: Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "func-create-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Glue database 'data_engineering_db'...\n",
      "Database 'data_engineering_db' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_database(database_name, description=''):\n",
    "    \"\"\"\n",
    "    Create a database in the AWS Glue Data Catalog\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Name (lowercase, no spaces)\n",
    "        description (str): Optional description\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if created successfully\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating Glue database '{database_name}'...\")\n",
    "        \n",
    "        glue_client.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': database_name,\n",
    "                'Description': description\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Database '{database_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Database '{database_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test\n",
    "create_glue_database('data_engineering_db', 'Learning database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "list-databases-intro",
   "metadata": {},
   "source": [
    "### Function 2: List Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "func-list-databases",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing Glue databases...\n",
      "------------------------------------------------------------\n",
      "Found 2 database(s):\n",
      "\n",
      "Database: data_engineering_db\n",
      "  Description: Learning database\n",
      "\n",
      "Database: glue_db\n",
      "  Description: A glue databse\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data_engineering_db', 'glue_db']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_glue_databases():\n",
    "    \"\"\"\n",
    "    List all databases in the Glue Data Catalog\n",
    "    \n",
    "    Returns:\n",
    "        list: Database names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Listing Glue databases...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_databases()\n",
    "        databases = response.get('DatabaseList', [])\n",
    "        \n",
    "        if not databases:\n",
    "            print(\"No databases found\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(databases)} database(s):\\n\")\n",
    "        \n",
    "        for db in databases:\n",
    "            print(f\"Database: {db['Name']}\")\n",
    "            print(f\"  Description: {db.get('Description', 'N/A')}\")\n",
    "            print()\n",
    "        \n",
    "        return [db['Name'] for db in databases]\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "list_glue_databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-table-intro",
   "metadata": {},
   "source": [
    "### Function 3: Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "func-create-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table 'users' in 'data_engineering_db'...\n",
      "Table 'users' already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_table(database_name, table_name, columns, s3_location, \n",
    "                      input_format='csv', description=''):\n",
    "    \"\"\"\n",
    "    Create a table in the Glue Data Catalog\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Target database\n",
    "        table_name (str): Table name\n",
    "        columns (list): [{'Name': 'col1', 'Type': 'string'}, ...]\n",
    "        s3_location (str): S3 path to data\n",
    "        input_format (str): 'csv', 'json', or 'parquet'\n",
    "    \n",
    "    Common Glue Types: string, int, bigint, double, boolean, date, timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    format_configs = {\n",
    "        'csv': {\n",
    "            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
    "            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
    "            'SerdeInfo': {\n",
    "                'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
    "                'Parameters': {'field.delim': ',', 'skip.header.line.count': '1'}\n",
    "            }\n",
    "        },\n",
    "        'json': {\n",
    "            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
    "            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
    "            'SerdeInfo': {\n",
    "                'SerializationLibrary': 'org.openx.data.jsonserde.JsonSerDe',\n",
    "                'Parameters': {}\n",
    "            }\n",
    "        },\n",
    "        'parquet': {\n",
    "            'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
    "            'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
    "            'SerdeInfo': {\n",
    "                'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
    "                'Parameters': {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config = format_configs.get(input_format)\n",
    "    if not config:\n",
    "        print(f\"ERROR: Unsupported format '{input_format}'\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"Creating table '{table_name}' in '{database_name}'...\")\n",
    "        \n",
    "        glue_client.create_table(\n",
    "            DatabaseName=database_name,\n",
    "            TableInput={\n",
    "                'Name': table_name,\n",
    "                'Description': description,\n",
    "                'StorageDescriptor': {\n",
    "                    'Columns': columns,\n",
    "                    'Location': s3_location,\n",
    "                    'InputFormat': config['InputFormat'],\n",
    "                    'OutputFormat': config['OutputFormat'],\n",
    "                    'SerdeInfo': config['SerdeInfo']\n",
    "                },\n",
    "                'TableType': 'EXTERNAL_TABLE'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Table '{table_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Table '{table_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example\n",
    "sample_columns = [\n",
    "    {'Name': 'id', 'Type': 'int'},\n",
    "    {'Name': 'name', 'Type': 'string'},\n",
    "    {'Name': 'created_at', 'Type': 'timestamp'}\n",
    "]\n",
    "create_glue_table('data_engineering_db', 'users', sample_columns, f's3://{BUCKET_NAME}/data/users/', 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "list-tables-intro",
   "metadata": {},
   "source": [
    "### Function 4: List Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "func-list-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing tables in 'data_engineering_db'...\n",
      "------------------------------------------------------------\n",
      "Found 2 table(s):\n",
      "\n",
      "Table: raw\n",
      "  Location: s3://real-learn-s3/raw/\n",
      "  Columns: 9\n",
      "\n",
      "Table: users\n",
      "  Location: s3://real-learn-s3/data/users/\n",
      "  Columns: 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['raw', 'users']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_glue_tables(database_name):\n",
    "    \"\"\"\n",
    "    List all tables in a Glue database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Listing tables in '{database_name}'...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response = glue_client.get_tables(DatabaseName=database_name)\n",
    "        tables = response.get('TableList', [])\n",
    "        \n",
    "        if not tables:\n",
    "            print(f\"No tables found in '{database_name}'\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(tables)} table(s):\\n\")\n",
    "        \n",
    "        for table in tables:\n",
    "            print(f\"Table: {table['Name']}\")\n",
    "            print(f\"  Location: {table.get('StorageDescriptor', {}).get('Location', 'N/A')}\")\n",
    "            print(f\"  Columns: {len(table.get('StorageDescriptor', {}).get('Columns', []))}\")\n",
    "            print()\n",
    "        \n",
    "        return [t['Name'] for t in tables]\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "list_glue_tables('data_engineering_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase4-crawlers",
   "metadata": {},
   "source": [
    "## PHASE 4: CRAWLERS\n",
    "\n",
    "Crawlers automatically discover schema and populate the Data Catalog.\n",
    "\n",
    "### How Crawlers Work\n",
    "\n",
    "```\n",
    "1. CONFIGURE           2. RUN                3. CATALOG\n",
    "+-------------+       +-------------+       +-------------+\n",
    "| Define:     | ----> | Crawler     | ----> | Creates/    |\n",
    "| - S3 path   |       | scans files |       | updates     |\n",
    "| - IAM role  |       | infers      |       | tables in   |\n",
    "| - Schedule  |       | schema      |       | catalog     |\n",
    "+-------------+       +-------------+       +-------------+\n",
    "```\n",
    "\n",
    "### Function 5: Create Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "func-create-crawler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating crawler 'my-crawler'...\n",
      "Target: data_engineering_db\n",
      "Path: s3://real-learn-s3/raw/\n",
      "SUCCESS: Crawler 'my-crawler' created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_glue_crawler(crawler_name, database_name, s3_path, description=''):\n",
    "    \"\"\"\n",
    "    Create a Glue crawler\n",
    "    \n",
    "    Args:\n",
    "        crawler_name (str): Crawler name\n",
    "        database_name (str): Target database\n",
    "        s3_path (str): S3 path to crawl (e.g., 's3://bucket/path/')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating crawler '{crawler_name}'...\")\n",
    "        print(f\"Target: {database_name}\")\n",
    "        print(f\"Path: {s3_path}\")\n",
    "        \n",
    "        glue_client.create_crawler(\n",
    "            Name=crawler_name,\n",
    "            Role=GLUE_ROLE_ARN,\n",
    "            DatabaseName=database_name,\n",
    "            Description=description,\n",
    "            Targets={'S3Targets': [{'Path': s3_path}]},\n",
    "            SchemaChangePolicy={\n",
    "                'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                'DeleteBehavior': 'LOG'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Crawler '{crawler_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Crawler '{crawler_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example\n",
    "create_glue_crawler('my-crawler', 'data_engineering_db', f's3://{BUCKET_NAME}/raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-crawler-intro",
   "metadata": {},
   "source": [
    "### Function 6: Run Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "func-run-crawler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawler 'my-crawler'...\n",
      "Crawler started\n",
      "Waiting for completion...\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "Completed: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_crawler(crawler_name, wait=False):\n",
    "    \"\"\"\n",
    "    Start a Glue crawler\n",
    "    \n",
    "    Args:\n",
    "        crawler_name (str): Crawler name\n",
    "        wait (bool): Wait for completion\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting crawler '{crawler_name}'...\")\n",
    "        glue_client.start_crawler(Name=crawler_name)\n",
    "        print(f\"Crawler started\")\n",
    "        \n",
    "        if wait:\n",
    "            print(\"Waiting for completion...\")\n",
    "            while True:\n",
    "                response = glue_client.get_crawler(Name=crawler_name)\n",
    "                state = response['Crawler']['State']\n",
    "                \n",
    "                if state == 'READY':\n",
    "                    # Wait for stats to update, then re-fetch\n",
    "                    time.sleep(2)\n",
    "                    response = glue_client.get_crawler(Name=crawler_name)\n",
    "                    last = response['Crawler'].get('LastCrawl', {})\n",
    "                    print(f\"Completed: {last.get('Status', 'Unknown')}\")\n",
    "                    break\n",
    "                print(f\"  State: {state}\")\n",
    "                time.sleep(10)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'CrawlerRunningException':\n",
    "            print(\"Crawler already running\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example\n",
    "run_crawler('my-crawler', wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase5-jobs",
   "metadata": {},
   "source": [
    "## PHASE 5: ETL JOBS\n",
    "\n",
    "### Job Types Comparison\n",
    "\n",
    "```\n",
    "+------------------+-------------------+-------------------+\n",
    "|                  |    SPARK JOB      |  PYTHON SHELL     |\n",
    "+------------------+-------------------+-------------------+\n",
    "| Engine           | Apache Spark      | Python runtime    |\n",
    "| Best For         | Large datasets    | Small datasets    |\n",
    "| Min Resources    | 2 DPUs            | 0.0625 DPU        |\n",
    "| Startup Time     | ~1 minute         | Seconds           |\n",
    "+------------------+-------------------+-------------------+\n",
    "```\n",
    "\n",
    "### Function 7: Create Python Shell Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "func-create-python-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_python_shell_job(job_name, script_location, description=''):\n",
    "    \"\"\"\n",
    "    Create a Python Shell ETL job\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "        script_location (str): S3 path to Python script\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating Python Shell job '{job_name}'...\")\n",
    "        \n",
    "        glue_client.create_job(\n",
    "            Name=job_name,\n",
    "            Description=description,\n",
    "            Role=GLUE_ROLE_ARN,\n",
    "            Command={\n",
    "                'Name': 'pythonshell',\n",
    "                'ScriptLocation': script_location,\n",
    "                'PythonVersion': '3.9'\n",
    "            },\n",
    "            DefaultArguments={\n",
    "                '--TempDir': f's3://{BUCKET_NAME}/glue-temp/',\n",
    "                '--job-language': 'python'\n",
    "            },\n",
    "            MaxCapacity=0.0625,\n",
    "            Timeout=60,\n",
    "            GlueVersion='3.0'\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Job '{job_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Job '{job_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d82d52f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Python Shell job 'my-etl-job'...\n",
      "SUCCESS: Job 'my-etl-job' created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create job\n",
    "create_python_shell_job(\n",
    "    'my-etl-job',\n",
    "    f's3://{BUCKET_NAME}/scripts/test_job.py',\n",
    "    description='Test ETL job'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark-job-intro",
   "metadata": {},
   "source": [
    "### Function 8: Create Spark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "func-create-spark-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_job(job_name, script_location, description='', \n",
    "                     worker_type='G.1X', num_workers=2):\n",
    "    \"\"\"\n",
    "    Create a Spark ETL job\n",
    "    \n",
    "    Worker Types:\n",
    "        - G.025X: 2 vCPU, 4 GB (small jobs)\n",
    "        - G.1X: 4 vCPU, 16 GB (standard)\n",
    "        - G.2X: 8 vCPU, 32 GB (memory-intensive)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating Spark job '{job_name}'...\")\n",
    "        print(f\"Workers: {num_workers} x {worker_type}\")\n",
    "        \n",
    "        glue_client.create_job(\n",
    "            Name=job_name,\n",
    "            Description=description,\n",
    "            Role=GLUE_ROLE_ARN,\n",
    "            Command={\n",
    "                'Name': 'glueetl',\n",
    "                'ScriptLocation': script_location,\n",
    "                'PythonVersion': '3'\n",
    "            },\n",
    "            DefaultArguments={\n",
    "                '--TempDir': f's3://{BUCKET_NAME}/glue-temp/',\n",
    "                '--enable-metrics': 'true',\n",
    "                '--enable-continuous-cloudwatch-log': 'true'\n",
    "            },\n",
    "            WorkerType=worker_type,\n",
    "            NumberOfWorkers=num_workers,\n",
    "            Timeout=120,\n",
    "            GlueVersion='4.0'\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Job '{job_name}' created\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(f\"Job '{job_name}' already exists\")\n",
    "            return True\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-job-intro",
   "metadata": {},
   "source": [
    "### Function 9: Run Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "func-run-job",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 'my-etl-job'...\n",
      "Run ID: jr_94ce13623351cff864d62455474322809fd22ab37296406b95ef67e632f3c1c2\n",
      "Waiting for completion...\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "  State: RUNNING\n",
      "\n",
      "Completed: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jr_94ce13623351cff864d62455474322809fd22ab37296406b95ef67e632f3c1c2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_glue_job(job_name, arguments=None, wait=False):\n",
    "    \"\"\"\n",
    "    Start a Glue job\n",
    "    \n",
    "    Args:\n",
    "        job_name (str): Job name\n",
    "        arguments (dict): Optional job arguments\n",
    "        wait (bool): Wait for completion\n",
    "    \n",
    "    Returns:\n",
    "        str: Job run ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting job '{job_name}'...\")\n",
    "        \n",
    "        params = {'JobName': job_name}\n",
    "        if arguments:\n",
    "            params['Arguments'] = arguments\n",
    "        \n",
    "        response = glue_client.start_job_run(**params)\n",
    "        run_id = response['JobRunId']\n",
    "        print(f\"Run ID: {run_id}\")\n",
    "        \n",
    "        if wait:\n",
    "            print(\"Waiting for completion...\")\n",
    "            while True:\n",
    "                status = glue_client.get_job_run(JobName=job_name, RunId=run_id)\n",
    "                state = status['JobRun']['JobRunState']\n",
    "                \n",
    "                if state in ['SUCCEEDED', 'FAILED', 'STOPPED', 'TIMEOUT']:\n",
    "                    print(f\"\\nCompleted: {state}\")\n",
    "                    if state == 'FAILED':\n",
    "                        print(f\"Error: {status['JobRun'].get('ErrorMessage')}\")\n",
    "                    break\n",
    "                print(f\"  State: {state}\")\n",
    "                time.sleep(15)\n",
    "        \n",
    "        return run_id\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example\n",
    "run_glue_job('my-etl-job', wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "list-jobs-intro",
   "metadata": {},
   "source": [
    "### Function 10: List Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "func-list-jobs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing Glue jobs...\n",
      "Found 2 job(s):\n",
      "\n",
      "Job: aws-glue-pipeline\n",
      "Type: glueetl\n",
      "\n",
      "Job: my-etl-job\n",
      "Type: pythonshell\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aws-glue-pipeline', 'my-etl-job']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_glue_jobs():\n",
    "    \"\"\"\n",
    "    List all Glue jobs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Listing Glue jobs...\")        \n",
    "        response = glue_client.get_jobs()\n",
    "        jobs = response.get('Jobs', [])\n",
    "        \n",
    "        if not jobs:\n",
    "            print(\"No jobs found\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(jobs)} job(s):\\n\")\n",
    "        \n",
    "        for job in jobs:\n",
    "            print(f\"Job: {job['Name']}\")\n",
    "            print(f\"Type: {job['Command']['Name']}\")\n",
    "            print()\n",
    "        \n",
    "        return [j['Name'] for j in jobs]\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "list_glue_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-script",
   "metadata": {},
   "source": [
    "### Sample Spark ETL Script\n",
    "\n",
    "```python\n",
    "# sample_etl.py - Upload to S3\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Read from catalog\n",
    "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"my_database\",\n",
    "    table_name=\"raw_data\"\n",
    ")\n",
    "\n",
    "# Transform\n",
    "filtered = Filter.apply(frame=datasource, f=lambda x: x['status'] == 'active')\n",
    "\n",
    "# Write as Parquet\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=filtered,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": \"s3://bucket/processed/\"},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "job.commit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase6-advanced",
   "metadata": {},
   "source": [
    "## PHASE 6: ADVANCED TOPICS\n",
    "\n",
    "### Job Bookmarks\n",
    "\n",
    "Enable incremental processing - only process new data:\n",
    "\n",
    "```\n",
    "WITHOUT BOOKMARKS              WITH BOOKMARKS\n",
    "\n",
    "Run 1: [A, B, C]               Run 1: [A, B, C] --> Bookmark: C\n",
    "Run 2: [A, B, C, D, E]         Run 2: [D, E]    --> Bookmark: E\n",
    "       (processes all)                (only new!)\n",
    "```\n",
    "\n",
    "Enable with: `--job-bookmark-option job-bookmark-enable`\n",
    "\n",
    "---\n",
    "\n",
    "### Cost Optimization Tips\n",
    "\n",
    "1. **Use Python Shell for small jobs** - 0.0625 DPU vs 2 DPU minimum\n",
    "2. **Enable job bookmarks** - Process only new data\n",
    "3. **Use G.025X workers** - For jobs that don't need much memory\n",
    "4. **Set appropriate timeouts** - Avoid runaway jobs\n",
    "5. **Partition your data** - Faster crawling and querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "func-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_glue_database(database_name):\n",
    "    \"\"\"Delete a Glue database (must be empty)\"\"\"\n",
    "    try:\n",
    "        glue_client.delete_database(Name=database_name)\n",
    "        print(f\"Deleted database '{database_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_glue_table(database_name, table_name):\n",
    "    \"\"\"Delete a table\"\"\"\n",
    "    try:\n",
    "        glue_client.delete_table(DatabaseName=database_name, Name=table_name)\n",
    "        print(f\"Deleted table '{table_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_glue_crawler(crawler_name):\n",
    "    \"\"\"Delete a crawler\"\"\"\n",
    "    try:\n",
    "        glue_client.delete_crawler(Name=crawler_name)\n",
    "        print(f\"Deleted crawler '{crawler_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_glue_job(job_name):\n",
    "    \"\"\"Delete a job\"\"\"\n",
    "    try:\n",
    "        glue_client.delete_job(JobName=job_name)\n",
    "        print(f\"Deleted job '{job_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Phase | Topics |\n",
    "|-------|--------|\n",
    "| 1 | Core concepts, architecture, components |\n",
    "| 2 | Setup, IAM roles, boto3 configuration |\n",
    "| 3 | Data Catalog: databases, tables, metadata |\n",
    "| 4 | Crawlers: automatic schema discovery |\n",
    "| 5 | ETL Jobs: Python Shell and Spark |\n",
    "| 6 | Advanced: bookmarks, optimization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
