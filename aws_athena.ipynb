{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-main",
   "metadata": {},
   "source": [
    "# AWS Athena - Interactive SQL Analytics on S3\n",
    "\n",
    "This notebook provides a comprehensive guide to Amazon Athena, a serverless interactive query service that enables you to analyze data directly in Amazon S3 using standard SQL.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **Introduction & Core Concepts** - What is Athena, architecture, pricing\n",
    "2. **Setup & Prerequisites** - Configuration, boto3 client\n",
    "3. **Basic Queries** - SELECT, WHERE, GROUP BY, ORDER BY\n",
    "4. **Advanced Queries** - JOINs, aggregations, window functions\n",
    "5. **Table Management** - CREATE TABLE, partitioning\n",
    "6. **Best Practices** - Cost optimization, performance tuning\n",
    "7. **Complete Data Lake Workflow** - End-to-end pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1-intro",
   "metadata": {},
   "source": [
    "## PHASE 1: INTRODUCTION & CORE CONCEPTS\n",
    "\n",
    "### What is Amazon Athena?\n",
    "\n",
    "Amazon Athena is a **serverless, interactive query service** that makes it easy to analyze data directly in Amazon S3 using standard SQL. There's no infrastructure to manage - you simply point to your data in S3, define the schema, and start querying.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Serverless**: No servers to provision, manage, or scale\n",
    "- **Pay-per-query**: $5 per TB of data scanned\n",
    "- **Standard SQL**: ANSI SQL compatible (Presto/Trino engine)\n",
    "- **Fast**: Parallel query execution across multiple nodes\n",
    "- **Integrated**: Works with Glue Data Catalog, S3, QuickSight\n",
    "\n",
    "---\n",
    "\n",
    "### Athena vs Traditional Data Warehouses\n",
    "\n",
    "| Aspect | Traditional DW | Amazon Athena |\n",
    "|--------|----------------|---------------|\n",
    "| Infrastructure | Managed clusters | Serverless |\n",
    "| Data Location | Load into warehouse | Query in-place (S3) |\n",
    "| Cost Model | Per-hour/cluster | Per-query (TB scanned) |\n",
    "| Setup Time | Hours to days | Minutes |\n",
    "| Scaling | Manual/Auto-scaling | Automatic |\n",
    "| Best For | Predictable workloads | Ad-hoc analytics |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-diagram",
   "metadata": {},
   "source": [
    "### Athena Architecture & Workflow\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                            ATHENA WORKFLOW                                        |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                                                                                   |\n",
    "|   1. DATA IN S3                        2. SCHEMA IN GLUE CATALOG                  |\n",
    "|   +------------------+                 +------------------+                       |\n",
    "|   |   S3 Bucket      |                 |  Data Catalog    |                       |\n",
    "|   |------------------|                 |------------------|                       |\n",
    "|   | /raw/            |                 | Database:        |                       |\n",
    "|   |   sales.csv      |---------------->|   my_data_lake   |                       |\n",
    "|   |   customers.json |   Crawler or    |                  |                       |\n",
    "|   | /processed/      |   Manual DDL    | Tables:          |                       |\n",
    "|   |   sales.parquet  |                 |   - raw_sales    |                       |\n",
    "|   +------------------+                 |   - customers    |                       |\n",
    "|                                        +------------------+                       |\n",
    "|                                                 |                                 |\n",
    "|                                                 v                                 |\n",
    "|   3. QUERY WITH SQL                    +------------------+                       |\n",
    "|   +------------------+                 |     ATHENA       |                       |\n",
    "|   | SELECT *         |---------------->|------------------|                       |\n",
    "|   | FROM sales       |                 | - Parse SQL      |                       |\n",
    "|   | WHERE region =   |                 | - Read schema    |                       |\n",
    "|   |   'North'        |                 | - Scan S3 data   |                       |\n",
    "|   +------------------+                 | - Execute query  |                       |\n",
    "|                                        +------------------+                       |\n",
    "|                                                 |                                 |\n",
    "|                                                 v                                 |\n",
    "|   4. RESULTS                           +------------------+                       |\n",
    "|   +------------------+                 |  Query Results   |                       |\n",
    "|   | View in Console  |<----------------|------------------|                       |\n",
    "|   | Download CSV     |                 | - Displayed      |                       |\n",
    "|   | Save to S3       |                 | - Saved to S3    |                       |\n",
    "|   | Use via API      |                 | - Available API  |                       |\n",
    "|   +------------------+                 +------------------+                       |\n",
    "|                                                                                   |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-features",
   "metadata": {},
   "source": [
    "### Key Features\n",
    "\n",
    "```\n",
    "+--------------------------------------------------+\n",
    "|              ATHENA KEY FEATURES                 |\n",
    "+--------------------------------------------------+\n",
    "|                                                  |\n",
    "|  SERVERLESS                                      |\n",
    "|  - No infrastructure to manage                   |\n",
    "|  - Auto-scales based on query complexity         |\n",
    "|  - Always available                              |\n",
    "|                                                  |\n",
    "|  PAY-PER-QUERY                                   |\n",
    "|  - $5 per TB of data scanned                     |\n",
    "|  - No charges when not querying                  |\n",
    "|  - Save money with columnar formats              |\n",
    "|                                                  |\n",
    "|  STANDARD SQL                                    |\n",
    "|  - ANSI SQL compatible                           |\n",
    "|  - JOINs, window functions, CTEs                 |\n",
    "|  - Presto/Trino query engine                     |\n",
    "|                                                  |\n",
    "|  SUPPORTED FORMATS                               |\n",
    "|  - CSV, TSV, JSON                                |\n",
    "|  - Parquet, ORC (columnar - recommended)         |\n",
    "|  - Avro, Apache logs                             |\n",
    "|                                                  |\n",
    "|  INTEGRATIONS                                    |\n",
    "|  - AWS Glue Data Catalog                         |\n",
    "|  - Amazon QuickSight                             |\n",
    "|  - JDBC/ODBC drivers                             |\n",
    "|                                                  |\n",
    "+--------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pricing",
   "metadata": {},
   "source": [
    "### Pricing Model\n",
    "\n",
    "| Component | Cost |\n",
    "|-----------|------|\n",
    "| Data scanned | $5.00 per TB |\n",
    "| Cancelled queries | Charged for data scanned before cancellation |\n",
    "| DDL statements | Free (CREATE, ALTER, DROP) |\n",
    "| Failed queries | No charge |\n",
    "\n",
    "**Cost Optimization Example:**\n",
    "```\n",
    "Raw CSV (1 TB)      → Query scans 1 TB    → $5.00\n",
    "Parquet (100 GB)    → Query scans 100 GB  → $0.50  (10x savings!)\n",
    "Parquet + Partition → Query scans 10 GB   → $0.05  (100x savings!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2-setup",
   "metadata": {},
   "source": [
    "## PHASE 2: SETUP & PREREQUISITES\n",
    "\n",
    "### Console Setup (First Time)\n",
    "\n",
    "```\n",
    "+------------------------------------------------+\n",
    "| ATHENA CONSOLE SETUP                           |\n",
    "+------------------------------------------------+\n",
    "| AWS Console -> Athena                          |\n",
    "|                                                |\n",
    "| First time setup:                              |\n",
    "| 1. Settings -> Manage                          |\n",
    "| 2. Query result location:                      |\n",
    "|    s3://your-bucket/athena-results/            |\n",
    "| 3. Save                                        |\n",
    "|                                                |\n",
    "| This is where query results are stored.        |\n",
    "+------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup-credentials",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-2\n",
      "Bucket: real-learn-s3\n",
      "Athena Output: s3://real-learn-s3/athena-results/\n"
     ]
    }
   ],
   "source": [
    "# Environment Configuration\n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "BUCKET_NAME = os.getenv(\"AWS_BUCKET_NAME\")\n",
    "\n",
    "# Athena results location\n",
    "ATHENA_OUTPUT = f's3://{BUCKET_NAME}/athena-results/'\n",
    "\n",
    "print(f\"Region: {AWS_REGION}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Athena Output: {ATHENA_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Athena Client\n",
    "athena_client = boto3.client(\n",
    "    'athena',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "# Helper function for security\n",
    "def redact_account_id(text):\n",
    "    \"\"\"Redact AWS account ID from text\"\"\"\n",
    "    return re.sub(r':\\d{12}:', ':************:', str(text))\n",
    "\n",
    "print(\"Athena client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3-queries",
   "metadata": {},
   "source": [
    "## PHASE 3: RUNNING QUERIES WITH BOTO3\n",
    "\n",
    "### Function 1: Run Athena Query\n",
    "\n",
    "Core function to execute SQL queries against Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "func-run-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_athena_query(query, database, output_location=None, max_results=100):\n",
    "    \"\"\"\n",
    "    Execute an Athena query and return results\n",
    "    \n",
    "    Args:\n",
    "        query (str): SQL query to execute\n",
    "        database (str): Glue database name\n",
    "        output_location (str): S3 path for results (default: ATHENA_OUTPUT)\n",
    "        max_results (int): Maximum rows to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries with query results\n",
    "    \"\"\"\n",
    "    if output_location is None:\n",
    "        output_location = ATHENA_OUTPUT\n",
    "    \n",
    "    try:\n",
    "        print(f\"Executing query on database '{database}'...\")\n",
    "        print(f\"Query: {query[:100]}{'...' if len(query) > 100 else ''}\")\n",
    "        \n",
    "        # Start query execution\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            QueryExecutionContext={'Database': database},\n",
    "            ResultConfiguration={'OutputLocation': output_location}\n",
    "        )\n",
    "        \n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        print(f\"Query ID: {query_execution_id}\")\n",
    "        \n",
    "        # Wait for query to complete\n",
    "        while True:\n",
    "            result = athena_client.get_query_execution(\n",
    "                QueryExecutionId=query_execution_id\n",
    "            )\n",
    "            status = result['QueryExecution']['Status']['State']\n",
    "            \n",
    "            if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                break\n",
    "            \n",
    "            print(f\"  Status: {status}\")\n",
    "            time.sleep(1)\n",
    "        \n",
    "        if status == 'SUCCEEDED':\n",
    "            # Get execution stats\n",
    "            stats = result['QueryExecution']['Statistics']\n",
    "            data_scanned = stats.get('DataScannedInBytes', 0)\n",
    "            exec_time = stats.get('TotalExecutionTimeInMillis', 0)\n",
    "            \n",
    "            print(f\"\\nQuery SUCCEEDED\")\n",
    "            print(f\"Data scanned: {data_scanned / 1024 / 1024:.2f} MB\")\n",
    "            print(f\"Execution time: {exec_time / 1000:.2f} seconds\")\n",
    "            print(f\"Estimated cost: ${data_scanned / 1024 / 1024 / 1024 / 1024 * 5:.6f}\")\n",
    "            \n",
    "            # Get query results\n",
    "            results = athena_client.get_query_results(\n",
    "                QueryExecutionId=query_execution_id,\n",
    "                MaxResults=max_results\n",
    "            )\n",
    "            \n",
    "            # Parse results\n",
    "            rows = results['ResultSet']['Rows']\n",
    "            if not rows:\n",
    "                return []\n",
    "            \n",
    "            # First row is headers\n",
    "            headers = [col.get('VarCharValue', '') for col in rows[0]['Data']]\n",
    "            data = []\n",
    "            \n",
    "            for row in rows[1:]:\n",
    "                values = [col.get('VarCharValue', '') for col in row['Data']]\n",
    "                data.append(dict(zip(headers, values)))\n",
    "            \n",
    "            print(f\"Rows returned: {len(data)}\")\n",
    "            return data\n",
    "            \n",
    "        else:\n",
    "            error = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')\n",
    "            print(f\"Query {status}: {error}\")\n",
    "            return None\n",
    "            \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-queries",
   "metadata": {},
   "source": [
    "### Function 2: Show Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "func-show-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'data_engineering_db'...\n",
      "Query: SHOW TABLES\n",
      "Query ID: 2384315d-016c-45a6-be4a-89f8d29753ef\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.27 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 1\n",
      "\n",
      "Tables:\n",
      "  - users\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'raw': 'users'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_tables(database):\n",
    "    \"\"\"\n",
    "    List all tables in a database\n",
    "    \"\"\"\n",
    "    query = \"SHOW TABLES\"\n",
    "    results = run_athena_query(query, database)\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nTables:\")\n",
    "        for row in results:\n",
    "            print(f\"  - {list(row.values())[0]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example\n",
    "show_tables('data_engineering_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "describe-table",
   "metadata": {},
   "source": [
    "### Function 3: Describe Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "func-describe-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'data_engineering_db'...\n",
      "Query: DESCRIBE users\n",
      "Query ID: fe1ee647-93a2-4eda-a809-b51f4638d73b\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.80 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 2\n",
      "\n",
      "Schema for 'users':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id                  \\tint                 \\t                    ': 'name                \\tstring              \\t                    '},\n",
       " {'id                  \\tint                 \\t                    ': 'created_at          \\ttimestamp           \\t                    '}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def describe_table(database, table_name):\n",
    "    \"\"\"\n",
    "    Show table schema\n",
    "    \"\"\"\n",
    "    query = f\"DESCRIBE {table_name}\"\n",
    "    results = run_athena_query(query, database)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nSchema for '{table_name}':\")\n",
    "        for row in results:\n",
    "            col_name = row.get('col_name', '')\n",
    "            data_type = row.get('data_type', '')\n",
    "            if col_name and not col_name.startswith('#'):\n",
    "                print(f\"  {col_name}: {data_type}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example\n",
    "describe_table('data_engineering_db', 'users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase4-examples",
   "metadata": {},
   "source": [
    "## PHASE 4: QUERY EXAMPLES\n",
    "\n",
    "### Basic SELECT Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "example-select",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'data_engineering_db'...\n",
      "Query: \n",
      "SELECT * \n",
      "FROM users \n",
      "LIMIT 10\n",
      "\n",
      "Query ID: 12000feb-09a5-4089-8d68-232bfd766074\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.54 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 10\n",
      "{'id': '1', 'name': 'Alice Johnson', 'created_at': '2024-01-15 09:30:00.000'}\n",
      "{'id': '2', 'name': 'Bob Smith', 'created_at': '2024-02-20 14:45:30.000'}\n",
      "{'id': '3', 'name': 'Carol Williams', 'created_at': '2024-03-10 11:00:00.000'}\n",
      "{'id': '4', 'name': 'David Brown', 'created_at': '2024-04-05 16:20:15.000'}\n",
      "{'id': '5', 'name': 'Emma Davis', 'created_at': '2024-05-18 08:15:45.000'}\n",
      "{'id': '6', 'name': 'Frank Miller', 'created_at': '2024-06-22 13:30:00.000'}\n",
      "{'id': '7', 'name': 'Grace Wilson', 'created_at': '2024-07-30 10:45:20.000'}\n",
      "{'id': '8', 'name': 'Henry Taylor', 'created_at': '2024-08-12 15:00:00.000'}\n",
      "{'id': '9', 'name': 'Ivy Anderson', 'created_at': '2024-09-25 09:10:30.000'}\n",
      "{'id': '10', 'name': 'Jack Thomas', 'created_at': '2024-10-08 17:25:00.000'}\n"
     ]
    }
   ],
   "source": [
    "# Simple SELECT with LIMIT\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM users \n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'data_engineering_db')\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "example-where",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'data_engineering_db'...\n",
      "Query: \n",
      "SELECT id, name, created_at\n",
      "FROM users\n",
      "WHERE name LIKE 'Alice%'\n",
      "\n",
      "Query ID: 872b276b-fcf7-4dc0-95ef-e4003f7724eb\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.57 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 1\n"
     ]
    }
   ],
   "source": [
    "# SELECT with WHERE clause\n",
    "query = \"\"\"\n",
    "SELECT id, name, created_at\n",
    "FROM users\n",
    "WHERE name LIKE 'Alice%'\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'data_engineering_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregation-queries",
   "metadata": {},
   "source": [
    "### Aggregation Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "example-aggregation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'data_engineering_db'...\n",
      "Query: \n",
      "SELECT \n",
      "    DATE(created_at) as date,\n",
      "    COUNT(*) as user_count\n",
      "FROM users\n",
      "GROUP BY DATE(created_a...\n",
      "Query ID: 08373576-9231-4d36-ba4e-36129fe6d43a\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.79 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 10\n"
     ]
    }
   ],
   "source": [
    "# COUNT and GROUP BY\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    DATE(created_at) as date,\n",
    "    COUNT(*) as user_count\n",
    "FROM users\n",
    "GROUP BY DATE(created_at)\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'data_engineering_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "example-having",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'data_engineering_db'...\n",
      "Query: \n",
      "SELECT \n",
      "    region,\n",
      "    COUNT(*) as num_sales,\n",
      "    SUM(total) as total_revenue,\n",
      "    ROUND(AVG(total...\n",
      "Query ID: fa1774ef-2382-4bf9-9051-c1b807170e08\n",
      "  Status: QUEUED\n",
      "Query FAILED: TABLE_NOT_FOUND: line 6:6: Table 'awsdatacatalog.data_engineering_db.raw_sales' does not exist\n"
     ]
    }
   ],
   "source": [
    "# GROUP BY with HAVING\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    region,\n",
    "    COUNT(*) as num_sales,\n",
    "    SUM(total) as total_revenue,\n",
    "    ROUND(AVG(total), 2) as avg_sale\n",
    "FROM raw_sales\n",
    "GROUP BY region\n",
    "HAVING SUM(total) > 10000\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'data_engineering_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "join-queries",
   "metadata": {},
   "source": [
    "### JOIN Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "example-join",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'data_engineering_db'...\n",
      "Query: \n",
      "SELECT \n",
      "    c.name,\n",
      "    c.country,\n",
      "    COUNT(s.sale_id) as num_purchases,\n",
      "    SUM(s.total) as total...\n",
      "Query ID: cc0769de-1f2d-4893-b5d8-57f248f0b370\n",
      "  Status: QUEUED\n",
      "Query FAILED: TABLE_NOT_FOUND: line 6:6: Table 'awsdatacatalog.data_engineering_db.customers' does not exist\n"
     ]
    }
   ],
   "source": [
    "# JOIN tables\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.country,\n",
    "    COUNT(s.sale_id) as num_purchases,\n",
    "    SUM(s.total) as total_spent\n",
    "FROM customers c\n",
    "JOIN sales s ON c.customer_id = s.customer_id\n",
    "GROUP BY c.name, c.country\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'data_engineering_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "date-queries",
   "metadata": {},
   "source": [
    "### Date Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "example-date",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query on database 'data_engineering_db'...\n",
      "Query: \n",
      "SELECT \n",
      "    DATE_FORMAT(CAST(sale_date AS DATE), '%Y-%m') as month,\n",
      "    SUM(total) as monthly_reven...\n",
      "Query ID: 7aae9378-e38d-43d1-afeb-e16b883b70c1\n",
      "  Status: QUEUED\n",
      "Query FAILED: TABLE_NOT_FOUND: line 5:6: Table 'awsdatacatalog.data_engineering_db.raw_sales' does not exist\n"
     ]
    }
   ],
   "source": [
    "# Monthly trend analysis\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    DATE_FORMAT(CAST(sale_date AS DATE), '%Y-%m') as month,\n",
    "    SUM(total) as monthly_revenue,\n",
    "    COUNT(*) as num_sales\n",
    "FROM raw_sales\n",
    "GROUP BY DATE_FORMAT(CAST(sale_date AS DATE), '%Y-%m')\n",
    "ORDER BY month\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(query, 'data_engineering_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase5-tables",
   "metadata": {},
   "source": [
    "## PHASE 5: TABLE MANAGEMENT\n",
    "\n",
    "### Create External Table (Without Crawler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "func-create-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table 'users_manual'...\n",
      "Executing query on database 'data_engineering_db'...\n",
      "Query: \n",
      "CREATE EXTERNAL TABLE IF NOT EXISTS users_manual (\n",
      "    `id` INT,\n",
      "    `name` STRING,\n",
      "    `created_at...\n",
      "Query ID: 25bdbe92-2d6d-4c01-8179-52211bc7e36f\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.44 seconds\n",
      "Estimated cost: $0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_external_table(database, table_name, columns, s3_location, \n",
    "                          file_format='csv', serde=None):\n",
    "    \"\"\"\n",
    "    Create an external table in Athena\n",
    "    \n",
    "    Args:\n",
    "        database (str): Database name\n",
    "        table_name (str): Table name\n",
    "        columns (list): List of (name, type) tuples\n",
    "        s3_location (str): S3 path to data\n",
    "        file_format (str): 'csv', 'json', or 'parquet'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build column definitions\n",
    "    col_defs = ',\\n    '.join([f\"`{name}` {dtype}\" for name, dtype in columns])\n",
    "    \n",
    "    # Format-specific settings\n",
    "    if file_format == 'csv':\n",
    "        row_format = \"\"\"ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES ('field.delim' = ',')\"\"\"\n",
    "    elif file_format == 'json':\n",
    "        row_format = \"ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\"\n",
    "    elif file_format == 'parquet':\n",
    "        row_format = \"STORED AS PARQUET\"\n",
    "    else:\n",
    "        print(f\"ERROR: Unsupported format '{file_format}'\")\n",
    "        return None\n",
    "    \n",
    "    query = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} (\n",
    "    {col_defs}\n",
    ")\n",
    "{row_format}\n",
    "LOCATION '{s3_location}'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Creating table '{table_name}'...\")\n",
    "    result = run_athena_query(query, database)\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "columns = [\n",
    "    ('id', 'INT'),\n",
    "    ('name', 'STRING'),\n",
    "    ('created_at', 'TIMESTAMP')\n",
    "]\n",
    "create_external_table('data_engineering_db', 'users_manual', columns, f's3://{BUCKET_NAME}/data/users/', 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partitioned-table",
   "metadata": {},
   "source": [
    "### Create Partitioned Table\n",
    "\n",
    "Partitioning improves query performance and reduces costs by scanning only relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "func-create-partitioned",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating partitioned table 'sales_partitioned'...\n",
      "Executing query on database 'my_db'...\n",
      "Query: \n",
      "CREATE EXTERNAL TABLE IF NOT EXISTS sales_partitioned (\n",
      "    `sale_id` INT,\n",
      "    `product` STRING,\n",
      "  ...\n",
      "Query ID: f5b5b676-1d75-4fa8-a9a3-b5cd24fd2730\n",
      "  Status: QUEUED\n",
      "Query FAILED: FAILED: SemanticException [Error 10072]: Database does not exist: my_db\n"
     ]
    }
   ],
   "source": [
    "def create_partitioned_table(database, table_name, columns, partition_keys, s3_location):\n",
    "    \"\"\"\n",
    "    Create a partitioned external table\n",
    "    \n",
    "    Args:\n",
    "        database (str): Database name\n",
    "        table_name (str): Table name\n",
    "        columns (list): List of (name, type) tuples for data columns\n",
    "        partition_keys (list): List of (name, type) tuples for partition columns\n",
    "        s3_location (str): S3 path to data\n",
    "    \"\"\"\n",
    "    \n",
    "    col_defs = ',\\n    '.join([f\"`{name}` {dtype}\" for name, dtype in columns])\n",
    "    partition_defs = ', '.join([f\"`{name}` {dtype}\" for name, dtype in partition_keys])\n",
    "    \n",
    "    query = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} (\n",
    "    {col_defs}\n",
    ")\n",
    "PARTITIONED BY ({partition_defs})\n",
    "STORED AS PARQUET\n",
    "LOCATION '{s3_location}'\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Creating partitioned table '{table_name}'...\")\n",
    "    result = run_athena_query(query, database)\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "columns = [\n",
    "    ('sale_id', 'INT'),\n",
    "    ('product', 'STRING'),\n",
    "    ('total', 'DOUBLE')\n",
    "]\n",
    "partitions = [('region', 'STRING'), ('year', 'INT')]\n",
    "create_partitioned_table('my_db', 'sales_partitioned', columns, partitions, 's3://bucket/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "repair-partitions",
   "metadata": {},
   "source": [
    "### Load Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "func-repair-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repairing table 'sales_partitioned' (loading partitions)...\n",
      "Executing query on database 'data_engineering_db'...\n",
      "Query: MSCK REPAIR TABLE sales_partitioned\n",
      "Query ID: 60064c74-ed9a-48e3-860e-972eefe94b2d\n",
      "  Status: QUEUED\n",
      "  Status: RUNNING\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 1.64 seconds\n",
      "Estimated cost: $0.000000\n",
      "Rows returned: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repair_table(database, table_name):\n",
    "    \"\"\"\n",
    "    Discover and load all partitions for a partitioned table\n",
    "    \n",
    "    This scans S3 and adds any partitions found to the table metadata.\n",
    "    \"\"\"\n",
    "    query = f\"MSCK REPAIR TABLE {table_name}\"\n",
    "    print(f\"Repairing table '{table_name}' (loading partitions)...\")\n",
    "    return run_athena_query(query, database)\n",
    "\n",
    "# Example\n",
    "repair_table('data_engineering_db', 'sales_partitioned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drop-table",
   "metadata": {},
   "source": [
    "### Drop Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "func-drop-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping table 'temp_table'...\n",
      "Executing query on database 'data_engineering_db'...\n",
      "Query: DROP TABLE IF EXISTS temp_table\n",
      "Query ID: 1199c746-17c2-43a2-a3c9-b2d494ca87c9\n",
      "  Status: QUEUED\n",
      "\n",
      "Query SUCCEEDED\n",
      "Data scanned: 0.00 MB\n",
      "Execution time: 0.25 seconds\n",
      "Estimated cost: $0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_table(database, table_name):\n",
    "    \"\"\"\n",
    "    Drop an external table (does not delete S3 data)\n",
    "    \"\"\"\n",
    "    query = f\"DROP TABLE IF EXISTS {table_name}\"\n",
    "    print(f\"Dropping table '{table_name}'...\")\n",
    "    return run_athena_query(query, database)\n",
    "\n",
    "# Example\n",
    "drop_table('data_engineering_db', 'temp_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase6-best-practices",
   "metadata": {},
   "source": [
    "## PHASE 6: BEST PRACTICES\n",
    "\n",
    "### Cost Optimization\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|                    COST OPTIMIZATION                             |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|  1. USE COLUMNAR FORMATS (Parquet, ORC)                          |\n",
    "|     - Scan only needed columns                                   |\n",
    "|     - 5-10x cheaper than CSV/JSON                                |\n",
    "|     - Better compression                                         |\n",
    "|                                                                  |\n",
    "|  2. PARTITION YOUR DATA                                          |\n",
    "|     - By date, region, category, etc.                            |\n",
    "|     - Scan only relevant partitions                              |\n",
    "|     - Use WHERE on partition columns                             |\n",
    "|                                                                  |\n",
    "|  3. COMPRESS YOUR DATA                                           |\n",
    "|     - Gzip, Snappy, ZSTD for Parquet                             |\n",
    "|     - Reduces data scanned                                       |\n",
    "|                                                                  |\n",
    "|  4. USE LIMIT FOR TESTING                                        |\n",
    "|     - Test queries with LIMIT first                              |\n",
    "|     - Avoid scanning full tables during development              |\n",
    "|                                                                  |\n",
    "|  5. SELECT SPECIFIC COLUMNS                                      |\n",
    "|     - Avoid SELECT *                                             |\n",
    "|     - Specify only needed columns                                |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|                    PERFORMANCE TIPS                              |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|  1. OPTIMIZE JOINs                                               |\n",
    "|     - Put smaller table on the left side of JOIN                 |\n",
    "|     - Filter data before joining                                 |\n",
    "|     - Use appropriate JOIN types                                 |\n",
    "|                                                                  |\n",
    "|  2. USE APPROPRIATE DATA TYPES                                   |\n",
    "|     - INT instead of STRING for numbers                          |\n",
    "|     - DATE/TIMESTAMP for dates                                   |\n",
    "|     - Avoid VARCHAR(MAX)                                         |\n",
    "|                                                                  |\n",
    "|  3. FILTER EARLY                                                 |\n",
    "|     - Put WHERE clauses as early as possible                     |\n",
    "|     - Filter on partition columns first                          |\n",
    "|                                                                  |\n",
    "|  4. OPTIMIZE FILE SIZES                                          |\n",
    "|     - Target 128 MB - 512 MB per file                            |\n",
    "|     - Avoid many small files                                     |\n",
    "|     - Avoid very large files (>1 GB)                             |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase7-workflow",
   "metadata": {},
   "source": [
    "## PHASE 7: COMPLETE DATA LAKE WORKFLOW\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                        COMPLETE DATA LAKE PIPELINE                                |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "|                                                                                   |\n",
    "|  1. INGEST                                                                        |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Upload to S3      |  <-- Manual, Lambda, Kinesis Firehose                   |\n",
    "|     | (raw/ folder)     |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  2. CATALOG                                                                       |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Glue Crawler      |  --> Discovers schema, creates tables                   |\n",
    "|     | (Data Catalog)    |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  3. TRANSFORM                                                                     |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Glue ETL Job      |  --> Clean, transform, convert to Parquet               |\n",
    "|     | (processed/)      |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  4. CATALOG PROCESSED                                                             |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Another Crawler   |  --> Update catalog with processed tables               |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  5. ANALYZE                                                                       |\n",
    "|     +-------------------+                                                         |\n",
    "|     | Athena SQL        |  --> Fast, serverless analytics                         |\n",
    "|     | Queries           |                                                         |\n",
    "|     +-------------------+                                                         |\n",
    "|              |                                                                    |\n",
    "|              v                                                                    |\n",
    "|  6. VISUALIZE (Optional)                                                          |\n",
    "|     +-------------------+                                                         |\n",
    "|     | QuickSight        |  --> Dashboards, reports                                |\n",
    "|     +-------------------+                                                         |\n",
    "|                                                                                   |\n",
    "|  All serverless, managed, pay-per-use!                                            |\n",
    "+-----------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
